# RCA-BSc-thesis-project
Abstract: 
In todayâ€™s computer systems, logs which are generated by these systems as important records, offering
insights into the system's functionality and problem explanation. By increasing computer systems
usage, logs are generated more and more. Logs containing errors are particularly significant, as they
offer opportunities for system developers and maintainers to pinpoint and address issues to solve
problems. Automated solutions in this realm of technology, falling under the umbrella of Artificial
Intelligence for IT Operations (AIOps), prove valuable in tackling challenges posed by latent
problems or those not generated by the individual system sections that caused the error. Root cause
analysis (RCA), a subdomain of AIOps, is dedicated to analyzing data and identifying core issues.
This project introduces a machine learning-driven solution, fine-tuning the BERT language model,
for root cause detection tasks in logs collected from the Hadoop framework. Despite encountering
challenges, the developed model showcases promising results due to the volume of the log's content
and our dataset's imbalance distribution of samples through classes. Our result is achieving an
accuracy of 70% on the test set and 76% on the validation set.

## Description

This project presents a machine learning-based solution for performing Root Cause Analysis (RCA) on logs collected from the Hadoop framework. In any computer system, registered events, or "logs," play a crucial role in understanding system behavior and explaining problems that arise. The analysis of these logs is of great importance and significantly helps in system development and management. With the increasing use of computer systems, the volume of generated logs has grown dramatically, making manual analysis a difficult, and sometimes impossible, task.

This project leverages a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) language model to classify logs and identify the root cause of errors. The model was trained and evaluated on a dataset of Hadoop logs, tackling challenges such as large log volume and imbalanced class distribution. The final model achieves an accuracy of 70% on the test set and 76% on the validation set.

## Dataset

The project utilizes a publicly available dataset from **LogHub**, which contains logs collected from a 46-core Hadoop cluster running WordCount and PageRank applications under various conditions. The logs are categorized into four classes, three of which represent error states and are used in this project:
* **Machine Down**: Logs generated when a machine is down.
* **Network Disconnection**: Logs generated during a network disconnection.
* **Disk Full**: Logs generated when the disk is full.

The original "Normal" class was excluded as the focus of this project is on error analysis.

You can find the dataset [here](https://github.com/logpai/loghub/tree/master/Hadoop).

## Methodology

The core of this project is a `bert-base-uncased` model fine-tuned for multi-class text classification. The process involved several key steps:

1.  **Data Preprocessing**:
    * Collected and parsed log files into a structured format.
    * Cleaned the text data by removing timestamps and other irrelevant information.
    * Due to BERT's 512-token limit, logs were split into smaller, overlapping chunks to ensure no loss of contextual information.

2.  **Handling Class Imbalance**: The dataset exhibited a significant imbalance among the classes. To address this, several techniques were explored:
    * **Class Weighting**: Assigning higher weights to minority classes in the loss function.
    * **Random Oversampling**: Duplicating samples from the minority classes.
    * **SMOTE (Synthetic Minority Over-sampling Technique)**: Generating synthetic samples for the minority classes to create a balanced dataset.

3.  **Model Training**:
    * The model was trained using the PyTorch library.
    * The `AdamW` optimizer and `CrossEntropyLoss` function were used for training.
    * The model was trained for multiple epochs with a batch size of 16.

## Results

The model's performance was evaluated using multiple metrics, including accuracy, precision, recall, and F1-score. The final model achieved an accuracy of **70%** on the test set and **76%** on the validation set. The notebook also contains detailed classification reports from various experimental setups, with some approaches, like using SMOTE, yielding accuracy as high as 82% on the validation set.

## Installation

To set up the project environment, follow these steps:

1.  Clone the repository to your local machine:
    ```bash
    git clone <your-repository-link>
    ```
2.  Navigate to the project directory:
    ```bash
    cd <your-project-directory>
    ```
3.  Install the required dependencies using pip:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The main logic for this project is contained within the `FinalBScProject.ipynb` Jupyter Notebook. To run the project:

1.  Ensure all the required CSV files (`chunked_new.csv`, `dataset.csv`, etc.) are in the same directory as the notebook.
2.  Open the notebook using Jupyter Notebook or JupyterLab:
    ```bash
    jupyter notebook FinalBScProject.ipynb
    ```
3.  You can then run the cells sequentially to reproduce the results. The notebook includes sections for data analysis, preprocessing, model training (with various techniques for handling imbalance), and evaluation.

## License

This project is licensed under the MIT License. See the `LICENSE` file for more details.
