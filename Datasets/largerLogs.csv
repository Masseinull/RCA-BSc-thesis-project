,LogContent,RootCause,num_words
10,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0022, Ident: (mapreduce.security.token.JobTokenIdentifier@3acd9e86)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0022
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@e7d6ad
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:536870912+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48250246; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305444(69221776); length = 8908953/6553600
   mapred.MapTask: (EQUATOR) 57318998 kvi 14329744(57318976)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318998 kv 14329744(57318976) kvi 12130124(48520496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318998; bufend = 707922; bufvoid = 104857599
   mapred.MapTask: kvstart = 14329744(57318976); kvend = 5419856(21679424); length = 8909889/6553600
   mapred.MapTask: (EQUATOR) 9776658 kvi 2444160(9776640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9776658 kv 2444160(9776640) kvi 247856(991424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9776658; bufend = 57994455; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444160(9776640); kvend = 19741496(78965984); length = 8917065/6553600
   mapred.MapTask: (EQUATOR) 67063207 kvi 16765796(67063184)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67063207 kv 16765796(67063184) kvi 14570840(58283360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67063207; bufend = 10480387; bufvoid = 104857600
   mapred.MapTask: kvstart = 16765796(67063184); kvend = 7862980(31451920); length = 8902817/6553600
   mapred.MapTask: (EQUATOR) 19549139 kvi 4887280(19549120)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19549139 kv 4887280(19549120) kvi 2679652(10718608)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19549139; bufend = 67751785; bufvoid = 104857600
   mapred.MapTask: kvstart = 4887280(19549120); kvend = 22180828(88723312); length = 8920853/6553600
   mapred.MapTask: (EQUATOR) 76820537 kvi 19205128(76820512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76820537 kv 19205128(76820512) kvi 16995388(67981552)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/127.0.0.1""; destination host is: ""10.190.173.170"":29630; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] mapred.Task: Process Thread Dump: Communication exception
12 active threads
Thread 21 (SpillThread):
  State: WAITING
  Blocked count: 0
  Waited count: 6
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2e498b1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1521)
Thread 20 (hdfs.PeerCache@60ec1f20):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 74
  Stack:
    java.lang.Thread.sleep(Native Method)
    hdfs.PeerCache.run(PeerCache.java:244)
    hdfs.PeerCache.access$000(PeerCache.java:41)
    hdfs.PeerCache$1.run(PeerCache.java:119)
    java.lang.Thread.run(Thread.java:724)
Thread 16 (communication thread):
  State: RUNNABLE
  Blocked count: 58
  Waited count: 179
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:165)
    util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:219)
    mapred.Task$TaskReporter.run(Task.java:760)
    java.lang.Thread.run(Thread.java:724)
Thread 15 (Thread for syncLogs):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 56
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 13 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 54
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 11 (Timer for 'MapTask' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 23
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 10 (Thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
    sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:142)
Thread 5 (Attach Listener):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 120
  Waited count: 20
  Waiting on java.lang.ref.ReferenceQueue$Lock@3c1473d0
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 21
  Waited count: 22
  Waiting on java.lang.ref.Reference$Lock@62bf7b80
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 4
  Waited count: 12
  Stack:
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
    sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
    net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
    net.SocketInputStream.read(SocketInputStream.java:161)
    hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
    hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
    hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
    hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
    hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:186)
    hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:146)
    hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:693)
    hdfs.DFSInputStream.readBuffer(DFSInputStream.java:749)
    hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
    hdfs.DFSInputStream.read(DFSInputStream.java:847)
    java.io.DataInputStream.read(DataInputStream.java:100)

 WARN [communication thread] mapred.Task: Last retry, killing attempt_1445144423722_0022_m_000004_0
",NetworkDisconnection,1212
11,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0022, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0022
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 4 new map-outputs
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000003_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445144423722_0022_m_000003_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 823ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000002_0,attempt_1445144423722_0022_m_000000_0,attempt_1445144423722_0022_m_000001_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445144423722_0022_m_000002_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445144423722_0022_m_000000_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445144423722_0022_m_000001_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 1840ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000009_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445144423722_0022_m_000009_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 17251ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000007_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000007_1: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000007_1 decomp: 60517368 len: 60517372 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445144423722_0022_m_000007_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 333ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000005_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445144423722_0022_m_000005_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 718ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000008_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445144423722_0022_m_000008_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 1003ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000004_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000004_1: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000004_1 decomp: 60513765 len: 60513769 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0022_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445144423722_0022_m_000004_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 13721ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000006_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0022_m_000006_1: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0022_m_000006_1 decomp: 60515100 len: 60515104 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445144423722_0022_m_000006_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 9032ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [Thread-125] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [Thread-125] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743517_2738
  [Thread-125] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445144423722_0022_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445144423722_0022_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445144423722_0022_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out3/_temporary/1/task_1445144423722_0022_r_000000
   mapred.Task: Task 'attempt_1445144423722_0022_r_000000_0' done.
",NetworkDisconnection,960
16,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0023, Ident: (mapreduce.security.token.JobTokenIdentifier@304cc139)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0023
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@78aa82b
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@24a20a63
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 3 new map-outputs
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000009_1000 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000001_1000,attempt_1445144423722_0023_m_000000_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000009_1000: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0023_m_000009_1000 decomp: 56695786 len: 56695790 to DISK
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000001_1000: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000001_1000 decomp: 60515836 len: 60515840 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445144423722_0023_m_000001_1000
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445144423722_0023_m_000009_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 534ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445144423722_0023_m_000000_1000
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 791ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000002_1000 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000002_1000: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000002_1000 decomp: 60514392 len: 60514396 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445144423722_0023_m_000002_1000
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 395ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000003_1000 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000003_1000: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000003_1000 decomp: 60515787 len: 60515791 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445144423722_0023_m_000003_1000
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 545ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000005_1001 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000005_1001: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000005_1001 decomp: 60514806 len: 60514810 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445144423722_0023_m_000005_1001
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 723ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000006_1001 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000006_1001: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000006_1001 decomp: 60515100 len: 60515104 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445144423722_0023_m_000006_1001
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 729ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000004_1001 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000004_1001: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000004_1001 decomp: 60513765 len: 60513769 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445144423722_0023_m_000004_1001
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 604ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000007_1001 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000007_1001: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000007_1001 decomp: 60517368 len: 60517372 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445144423722_0023_m_000007_1001
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 706ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0023_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0023&reduce=0&map=attempt_1445144423722_0023_m_000008_1001 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0023_m_000008_1001: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445144423722_0023_m_000008_1001 decomp: 60516677 len: 60516681 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445144423722_0023_m_000008_1001
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 678ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445144423722_0023_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445144423722_0023_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445144423722_0023_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/task_1445144423722_0023_r_000000
   mapred.Task: Task 'attempt_1445144423722_0023_r_000000_1000' done.
",NetworkDisconnection,979
28,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@6b7a5e49)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@7cedd9bc
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:805306368+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48215795; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17296824(69187296); length = 8917573/6553600
   mapred.MapTask: (EQUATOR) 57284531 kvi 14321128(57284512)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57284531 kv 14321128(57284512) kvi 12112692(48450768)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57284531; bufend = 630553; bufvoid = 104857600
   mapred.MapTask: kvstart = 14321128(57284512); kvend = 5400516(21602064); length = 8920613/6553600
   mapred.MapTask: (EQUATOR) 9699305 kvi 2424820(9699280)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9699305 kv 2424820(9699280) kvi 222764(891056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9699305; bufend = 57911793; bufvoid = 104857600
   mapred.MapTask: kvstart = 2424820(9699280); kvend = 19720828(78883312); length = 8918393/6553600
   mapred.MapTask: (EQUATOR) 66980545 kvi 16745132(66980528)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 66980545 kv 16745132(66980528) kvi 14546624(58186496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 66980545; bufend = 10374147; bufvoid = 104857600
   mapred.MapTask: kvstart = 16745132(66980528); kvend = 7836420(31345680); length = 8908713/6553600
   mapred.MapTask: (EQUATOR) 19442899 kvi 4860720(19442880)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19442899 kv 4860720(19442880) kvi 2660780(10643120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19442899; bufend = 67657921; bufvoid = 104857600
   mapred.MapTask: kvstart = 4860720(19442880); kvend = 22157356(88629424); length = 8917765/6553600
   mapred.MapTask: (EQUATOR) 76726657 kvi 19181660(76726640)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76726657 kv 19181660(76726640) kvi 16980352(67921408)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76726657; bufend = 20115617; bufvoid = 104857600
   mapred.MapTask: kvstart = 19181660(76726640); kvend = 10271780(41087120); length = 8909881/6553600
   mapred.MapTask: (EQUATOR) 29184353 kvi 7296084(29184336)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29184353 kv 7296084(29184336) kvi 5097788(20391152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29184353; bufend = 77442473; bufvoid = 104857600
   mapred.MapTask: kvstart = 7296084(29184336); kvend = 24603496(98413984); length = 8906989/6553600
   mapred.MapTask: (EQUATOR) 86511225 kvi 21627800(86511200)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86511225 kv 21627800(86511200) kvi 19431636(77726544)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 34 time(s); maxRetries=45
",NetworkDisconnection,945
29,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@56b87a95)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@24207655
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:939524096+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48252774; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306072(69224288); length = 8908325/6553600
   mapred.MapTask: (EQUATOR) 57321526 kvi 14330376(57321504)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57321526 kv 14330376(57321504) kvi 12128560(48514240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57321526; bufend = 695920; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330376(57321504); kvend = 5416856(21667424); length = 8913521/6553600
   mapred.MapTask: (EQUATOR) 9764656 kvi 2441160(9764640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9764656 kv 2441160(9764640) kvi 236684(946736)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9764656; bufend = 58004947; bufvoid = 104857600
   mapred.MapTask: kvstart = 2441160(9764640); kvend = 19744112(78976448); length = 8911449/6553600
   mapred.MapTask: (EQUATOR) 67073683 kvi 16768416(67073664)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67073683 kv 16768416(67073664) kvi 14561752(58247008)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67073683; bufend = 10426966; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768416(67073664); kvend = 7849620(31398480); length = 8918797/6553600
   mapred.MapTask: (EQUATOR) 19495718 kvi 4873924(19495696)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19495718 kv 4873924(19495696) kvi 2677448(10709792)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19495718; bufend = 67755457; bufvoid = 104857600
   mapred.MapTask: kvstart = 4873924(19495696); kvend = 22181748(88726992); length = 8906577/6553600
   mapred.MapTask: (EQUATOR) 76824209 kvi 19206048(76824192)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76824209 kv 19206048(76824192) kvi 16996444(67985776)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76824209; bufend = 20191510; bufvoid = 104857600
   mapred.MapTask: kvstart = 19206048(76824192); kvend = 10290756(41163024); length = 8915293/6553600
   mapred.MapTask: (EQUATOR) 29260262 kvi 7315060(29260240)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29260262 kv 7315060(29260240) kvi 5114312(20457248)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29260262; bufend = 77519736; bufvoid = 104857600
   mapred.MapTask: kvstart = 7315060(29260240); kvend = 24622816(98491264); length = 8906645/6553600
   mapred.MapTask: (EQUATOR) 86588488 kvi 21647116(86588464)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86588488 kv 21647116(86588464) kvi 19453336(77813344)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 34 time(s); maxRetries=45
",NetworkDisconnection,945
37,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@271ff531)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2eedd32f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b904247
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000003_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445144423722_0020_m_000003_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 712ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 2 new map-outputs
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000004_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000004_1000: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000004_1000 decomp: 60513765 len: 60513769 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445144423722_0020_m_000004_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 742ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000000_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445144423722_0020_m_000000_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 534ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000001_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000001_1000: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000001_1000 decomp: 60515836 len: 60515840 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445144423722_0020_m_000001_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 836ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000002_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000002_1000: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000002_1000 decomp: 60514392 len: 60514396 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445144423722_0020_m_000002_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 824ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000008_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000008_1000: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000008_1000 decomp: 60516677 len: 60516681 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445144423722_0020_m_000008_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 689ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000005_1001 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000005_1001: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000005_1001 decomp: 60514806 len: 60514810 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445144423722_0020_m_000005_1001
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 667ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000006_1001 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000006_1001: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000006_1001 decomp: 60515100 len: 60515104 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445144423722_0020_m_000006_1001
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 791ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000009_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000009_1000: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000009_1000 decomp: 56695786 len: 56695790 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445144423722_0020_m_000009_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 8340ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445144423722_0020_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445144423722_0020&reduce=0&map=attempt_1445144423722_0020_m_000007_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445144423722_0020_m_000007_1000: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445144423722_0020_m_000007_1000 decomp: 60517368 len: 60517372 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445144423722_0020_m_000007_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 11028ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445144423722_0020_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445144423722_0020_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445144423722_0020_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out1/_temporary/2/task_1445144423722_0020_r_000000
   mapred.Task: Task 'attempt_1445144423722_0020_r_000000_1000' done.
",NetworkDisconnection,1026
51,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0: Got 6 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0003_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445175094696_0003_m_000001_0
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445175094696_0003_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 29825ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000004_0,attempt_1445175094696_0003_m_000000_0,attempt_1445175094696_0003_m_000003_0,attempt_1445175094696_0003_m_000002_0,attempt_1445175094696_0003_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 28065ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445175094696_0003_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445175094696_0003_m_000000_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445175094696_0003_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445175094696_0003_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445175094696_0003_m_000005_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 40729ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000008_1: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000008_1 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445175094696_0003_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3597ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000006_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0003_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445175094696_0003_m_000006_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 14802ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445175094696_0003_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 39489ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52155; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 7 time(s); maxRetries=45
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445175094696_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445175094696_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743677_2902
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445175094696_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 17 time(s); maxRetries=45
   mapred.Task: Task:attempt_1445175094696_0003_r_000000_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52155. Already tried 34 time(s); maxRetries=45
",NetworkDisconnection,1405
61,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4b8a4c9d
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:671088640+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 34177286; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 13787204(55148816); length = 12427193/6553600
   mapred.MapTask: (EQUATOR) 44663043 kvi 11165756(44663024)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 44663043 kv 11165756(44663024) kvi 8544328(34177312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 44663043; bufend = 78834546; bufvoid = 104857600
   mapred.MapTask: kvstart = 11165756(44663024); kvend = 24951520(99806080); length = 12428637/6553600
   mapred.MapTask: (EQUATOR) 89320305 kvi 22330072(89320288)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 89320305 kv 22330072(89320288) kvi 19708644(78834576)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 89320305; bufend = 18636739; bufvoid = 104857592
   mapred.MapTask: kvstart = 22330072(89320288); kvend = 9902068(39608272); length = 12428005/6553600
   mapred.MapTask: (EQUATOR) 29122497 kvi 7280620(29122480)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 29122497 kv 7280620(29122480) kvi 4659192(18636768)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/127.0.0.1""; destination host is: ""10.190.173.170"":25859; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] mapred.Task: Process Thread Dump: Communication exception
12 active threads
Thread 21 (SpillThread):
  State: WAITING
  Blocked count: 0
  Waited count: 4
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@220336f0
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1521)
Thread 20 (hdfs.PeerCache@891c3e3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 45
  Stack:
    java.lang.Thread.sleep(Native Method)
    hdfs.PeerCache.run(PeerCache.java:244)
    hdfs.PeerCache.access$000(PeerCache.java:41)
    hdfs.PeerCache$1.run(PeerCache.java:119)
    java.lang.Thread.run(Thread.java:724)
Thread 16 (communication thread):
  State: RUNNABLE
  Blocked count: 31
  Waited count: 104
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:165)
    util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:219)
    mapred.Task$TaskReporter.run(Task.java:760)
    java.lang.Thread.run(Thread.java:724)
Thread 15 (Thread for syncLogs):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 34
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 13 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 31
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 11 (Timer for 'MapTask' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 10 (Thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
    sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:142)
Thread 5 (Attach Listener):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 33
  Waited count: 17
  Waiting on java.lang.ref.ReferenceQueue$Lock@2eb4ada3
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 18
  Waited count: 18
  Waiting on java.lang.ref.Reference$Lock@57f3c049
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 4
  Waited count: 11
  Stack:
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
    sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
    net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
    net.SocketInputStream.read(SocketInputStream.java:161)
    hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
    hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
    hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
    hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
    hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:186)
    hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:146)
    hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:693)
    hdfs.DFSInputStream.readBuffer(DFSInputStream.java:749)
    hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
    hdfs.DFSInputStream.read(DFSInputStream.java:847)
    java.io.DataInputStream.read(DataInputStream.java:100)

 WARN [communication thread] mapred.Task: Last retry, killing attempt_1445175094696_0004_m_000005_0
",NetworkDisconnection,1128
63,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 2 new map-outputs
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000003_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445175094696_0004_m_000003_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 30308ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000001_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445175094696_0004_m_000001_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2773ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000004_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445175094696_0004_m_000004_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1895ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000002_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445175094696_0004_m_000002_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2020ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000007_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445175094696_0004_m_000007_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1896ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000008_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445175094696_0004_m_000008_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2470ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000009_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000009_1: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000009_1 decomp: 172334804 len: 172334808 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445175094696_0004_m_000009_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1706ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000005_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000005_1: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000005_1 decomp: 216990140 len: 216990144 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445175094696_0004_m_000005_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 4004ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000006_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445175094696_0004_m_000006_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2367ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0004&reduce=0&map=attempt_1445175094696_0004_m_000000_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0004_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445175094696_0004_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445175094696_0004_m_000000_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 24283ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445175094696_0004_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445175094696_0004_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743688_2913
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445175094696_0004_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445175094696_0004_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445175094696_0004_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445175094696_0004_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445175094696_0004_r_000000
   mapred.Task: Task 'attempt_1445175094696_0004_r_000000_0' done.
",NetworkDisconnection,1063
69,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@3acd9e86)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@51038c46
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 34175940; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 13786864(55147456); length = 12427533/6553600
   mapred.MapTask: (EQUATOR) 44661690 kvi 11165416(44661664)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 44661690 kv 11165416(44661664) kvi 8543992(34175968)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 44661690; bufend = 78835380; bufvoid = 104857600
   mapred.MapTask: kvstart = 11165416(44661664); kvend = 24951728(99806912); length = 12428089/6553600
   mapred.MapTask: (EQUATOR) 89321138 kvi 22330280(89321120)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 89321138 kv 22330280(89321120) kvi 19708852(78835408)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 89321138; bufend = 18639482; bufvoid = 104857600
   mapred.MapTask: kvstart = 22330280(89321120); kvend = 9902752(39611008); length = 12427529/6553600
   mapred.MapTask: (EQUATOR) 29125237 kvi 7281304(29125216)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 29125237 kv 7281304(29125216) kvi 4659876(18639504)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29125237; bufend = 63299431; bufvoid = 104857600
   mapred.MapTask: kvstart = 7281304(29125216); kvend = 21067736(84270944); length = 12427969/6553600
   mapred.MapTask: (EQUATOR) 73785179 kvi 18446288(73785152)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 73785179 kv 18446288(73785152) kvi 15824864(63299456)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 73785179; bufend = 3101447; bufvoid = 104857600
   mapred.MapTask: kvstart = 18446288(73785152); kvend = 6018244(24072976); length = 12428045/6553600
   mapred.MapTask: (EQUATOR) 13587203 kvi 3396796(13587184)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 13587203 kv 3396796(13587184) kvi 775368(3101472)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 13587203; bufend = 47759762; bufvoid = 104857600
   mapred.MapTask: kvstart = 3396796(13587184); kvend = 17182820(68731280); length = 12428377/6553600
   mapred.MapTask: (EQUATOR) 58245513 kvi 14561372(58245488)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1857.517062515084 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010 10.86.169.121:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1716.6852518353096 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.DFSClient: DFS Read
java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)
	at hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)
	at hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)
	at hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)
	at hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:917)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:568)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 36 more
   mapred.MapTask: Starting flush of map output
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/127.0.0.1""; destination host is: ""10.190.173.170"":25859; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 58245513 kv 14561372(58245488) kvi 12513928(50055712)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 58245513; bufend = 63878406; bufvoid = 104857600
   mapred.MapTask: kvstart = 14561372(58245488); kvend = 12513932(50055728); length = 2047441/6553600
   mapred.MapTask: Finished spill 6
   mapred.Merger: Merging 7 sorted segments
   mapred.Merger: Down to the last merge-pass, with 7 segments left of total size: 228403967 bytes
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] mapred.Task: Process Thread Dump: Communication exception
15 active threads
Thread 55 (Readahead Thread #3):
  State: WAITING
  Blocked count: 0
  Waited count: 24
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@632e7cf3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 54 (Readahead Thread #2):
  State: WAITING
  Blocked count: 0
  Waited count: 25
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@632e7cf3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 53 (Readahead Thread #1):
  State: WAITING
  Blocked count: 0
  Waited count: 25
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@632e7cf3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 52 (Readahead Thread #0):
  State: WAITING
  Blocked count: 0
  Waited count: 26
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@632e7cf3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 20 (hdfs.PeerCache@396828e9):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 50
  Stack:
    java.lang.Thread.sleep(Native Method)
    hdfs.PeerCache.run(PeerCache.java:244)
    hdfs.PeerCache.access$000(PeerCache.java:41)
    hdfs.PeerCache$1.run(PeerCache.java:119)
    java.lang.Thread.run(Thread.java:724)
Thread 16 (communication thread):
  State: RUNNABLE
  Blocked count: 36
  Waited count: 119
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:165)
    util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:219)
    mapred.Task$TaskReporter.run(Task.java:760)
    java.lang.Thread.run(Thread.java:724)
Thread 15 (Thread for syncLogs):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 38
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 13 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 35
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 11 (Timer for 'MapTask' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 17
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 10 (Thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
    sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:142)
Thread 5 (Attach Listener):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 53
  Waited count: 20
  Waiting on java.lang.ref.ReferenceQueue$Lock@59246ad9
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 22
  Waited count: 21
  Waiting on java.lang.ref.Reference$Lock@65e2fa47
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 4
  Waited count: 17
  Stack:
    java.util.zip.CRC32.updateBytes(Native Method)
    java.util.zip.CRC32.update(CRC32.java:65)
    util.DataChecksum.update(DataChecksum.java:265)
    mapred.IFileOutputStream.write(IFileOutputStream.java:87)
    mapred.IFileOutputStream.write(IFileOutputStream.java:94)
    fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)
    java.io.DataOutputStream.writeByte(DataOutputStream.java:153)
    io.WritableUtils.writeVLong(WritableUtils.java:273)
    io.WritableUtils.writeVInt(WritableUtils.java:253)
    mapred.IFile$Writer.append(IFile.java:214)
    mapred.Task$CombineOutputCollector.collect(Task.java:1313)
    mapred.Task$NewCombinerRunner$OutputConverter.write(Task.java:1630)
    mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
    examples.WordCount$IntSumReducer.reduce(WordCount.java:64)
    examples.WordCount$IntSumReducer.reduce(WordCount.java:52)
    mapreduce.Reducer.run(Reducer.java:171)
    mapred.Task$NewCombinerRunner.combine(Task.java:1651)
    mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1911)
    mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1504)

 WARN [communication thread] mapred.Task: Last retry, killing attempt_1445175094696_0004_m_000000_0
",NetworkDisconnection,2319
75,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@78093f60)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@7fc3caba
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5bef7247
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0003_r_000000_1000: Got 10 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0003_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000009_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445175094696_0003_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445175094696_0003_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1318ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000001_0,attempt_1445175094696_0003_m_000002_0,attempt_1445175094696_0003_m_000003_0,attempt_1445175094696_0003_m_000004_0,attempt_1445175094696_0003_m_000005_0,attempt_1445175094696_0003_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445175094696_0003_m_000006_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1974ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0003&reduce=0&map=attempt_1445175094696_0003_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0003_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445175094696_0003_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445175094696_0003_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445175094696_0003_m_000007_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1957ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445175094696_0003_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445175094696_0003_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445175094696_0003_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0003_m_000008_1: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0003_m_000008_1 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445175094696_0003_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 7273ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445175094696_0003_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 26105ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445175094696_0003_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445175094696_0003_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445175094696_0003_r_000000_1000' to hdfs://msra-sa-41:9000/out/out2/_temporary/2/task_1445175094696_0003_r_000000
   mapred.Task: Task 'attempt_1445175094696_0003_r_000000_1000' done.
",NetworkDisconnection,755
78,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@67b65d47)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4d220536
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:805306368+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 34172179; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 13785924(55143696); length = 12428473/6553600
   mapred.MapTask: (EQUATOR) 44657929 kvi 11164476(44657904)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 44657929 kv 11164476(44657904) kvi 8543052(34172208)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 44657929; bufend = 78836928; bufvoid = 104857600
   mapred.MapTask: kvstart = 11164476(44657904); kvend = 24952116(99808464); length = 12426761/6553600
   mapred.MapTask: (EQUATOR) 89322688 kvi 22330668(89322672)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 89322688 kv 22330668(89322672) kvi 19709236(78836944)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/127.0.0.1""; destination host is: ""10.190.173.170"":25859; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:25859. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:25859 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] mapred.Task: Process Thread Dump: Communication exception
12 active threads
Thread 21 (SpillThread):
  State: WAITING
  Blocked count: 0
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@4166d6d3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1521)
Thread 20 (hdfs.PeerCache@7316a5eb):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 46
  Stack:
    java.lang.Thread.sleep(Native Method)
    hdfs.PeerCache.run(PeerCache.java:244)
    hdfs.PeerCache.access$000(PeerCache.java:41)
    hdfs.PeerCache$1.run(PeerCache.java:119)
    java.lang.Thread.run(Thread.java:724)
Thread 16 (communication thread):
  State: RUNNABLE
  Blocked count: 29
  Waited count: 102
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:165)
    util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:219)
    mapred.Task$TaskReporter.run(Task.java:760)
    java.lang.Thread.run(Thread.java:724)
Thread 15 (Thread for syncLogs):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 33
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 13 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 30
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 11 (Timer for 'MapTask' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 10 (Thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
    sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:142)
Thread 5 (Attach Listener):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 51
  Waited count: 18
  Waiting on java.lang.ref.ReferenceQueue$Lock@7a766ca0
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 18
  Waited count: 19
  Waiting on java.lang.ref.Reference$Lock@74d5bb2f
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 4
  Waited count: 11
  Stack:
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
    sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
    net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
    net.SocketInputStream.read(SocketInputStream.java:161)
    hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
    hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
    hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
    hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
    hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:186)
    hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:146)
    hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:693)
    hdfs.DFSInputStream.readBuffer(DFSInputStream.java:749)
    hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
    hdfs.DFSInputStream.read(DFSInputStream.java:847)
    java.io.DataInputStream.read(DataInputStream.java:100)

 WARN [communication thread] mapred.Task: Last retry, killing attempt_1445175094696_0004_m_000006_0
",NetworkDisconnection,1086
80,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445175094696_0001_m_000003_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 50327ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000001_0,attempt_1445175094696_0001_m_000005_0,attempt_1445175094696_0001_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 2 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000009_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445175094696_0001_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445175094696_0001_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445175094696_0001_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 3077ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445175094696_0001_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445175094696_0001_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5814ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445175094696_0001_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1322ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000002_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445175094696_0001_m_000002_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4838ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000004_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000004_1: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0001_m_000004_1 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445175094696_0001_m_000004_1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 29784ms
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000006_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445175094696_0001_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445175094696_0001_m_000006_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 10851ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0001&reduce=0&map=attempt_1445175094696_0001_m_000000_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0001_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445175094696_0001_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445175094696_0001_m_000000_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 6401ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [DataStreamer for file /out/out1/_temporary/1/_temporary/attempt_1445175094696_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out1/_temporary/1/_temporary/attempt_1445175094696_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743674_2899
  [DataStreamer for file /out/out1/_temporary/1/_temporary/attempt_1445175094696_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445175094696_0001_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445175094696_0001_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445175094696_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out1/_temporary/1/task_1445175094696_0001_r_000000
   mapred.Task: Task 'attempt_1445175094696_0001_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",NetworkDisconnection,986
105,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445175094696_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@1ebe6739)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445175094696_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4112054e
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@309134f7
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 5 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445175094696_0002_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1215ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 4 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000005_0,attempt_1445175094696_0002_m_000008_0,attempt_1445175094696_0002_m_000003_0,attempt_1445175094696_0002_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445175094696_0002_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445175094696_0002_m_000008_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445175094696_0002_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445175094696_0002_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5325ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000000_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445175094696_0002_m_000000_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1364ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000001_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000001_1: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000001_1 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445175094696_0002_m_000001_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1201ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000002_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445175094696_0002_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000006_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0002_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445175094696_0002_m_000002_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1529ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445175094696_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445175094696_0002_m_000006_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 23884ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445175094696_0002&reduce=0&map=attempt_1445175094696_0002_m_000004_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445175094696_0002_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445175094696_0002_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445175094696_0002_m_000004_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 41134ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445175094696_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445175094696_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743665_2890
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445175094696_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445175094696_0002_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445175094696_0002_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445175094696_0002_r_000000_0' to hdfs://msra-sa-41:9000/out/out3/_temporary/1/task_1445175094696_0002_r_000000
   mapred.Task: Task 'attempt_1445175094696_0002_r_000000_0' done.
",NetworkDisconnection,922
108,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0023, Ident: (mapreduce.security.token.JobTokenIdentifier@2ebb9a37)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0023
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@7e2c946e
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.15""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62304; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 28 time(s); maxRetries=45
",NetworkDisconnection,867
109,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@7f219df4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@5d463027
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:671088640+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48241717; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17303312(69213248); length = 8911085/6553600
   mapred.MapTask: (EQUATOR) 57310469 kvi 14327612(57310448)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57310469 kv 14327612(57310448) kvi 12124056(48496224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57310469; bufend = 677160; bufvoid = 104857600
   mapred.MapTask: kvstart = 14327612(57310448); kvend = 5412172(21648688); length = 8915441/6553600
   mapred.MapTask: (EQUATOR) 9745912 kvi 2436472(9745888)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9745912 kv 2436472(9745888) kvi 243380(973520)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9745912; bufend = 58001423; bufvoid = 104857600
   mapred.MapTask: kvstart = 2436472(9745888); kvend = 19743236(78972944); length = 8907637/6553600
   mapred.MapTask: (EQUATOR) 67070175 kvi 16767536(67070144)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67070175 kv 16767536(67070144) kvi 14572400(58289600)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67070175; bufend = 10445315; bufvoid = 104857600
   mapred.MapTask: kvstart = 16767536(67070144); kvend = 7854204(31416816); length = 8913333/6553600
   mapred.MapTask: (EQUATOR) 19514051 kvi 4878508(19514032)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19514051 kv 4878508(19514032) kvi 2676468(10705872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19514051; bufend = 67765795; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878508(19514032); kvend = 22184324(88737296); length = 8908585/6553600
   mapred.MapTask: (EQUATOR) 76834531 kvi 19208628(76834512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76834531 kv 19208628(76834512) kvi 17011572(68046288)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76834531; bufend = 20216843; bufvoid = 104857600
   mapred.MapTask: kvstart = 19208628(76834512); kvend = 10297092(41188368); length = 8911537/6553600
   mapred.MapTask: (EQUATOR) 29285595 kvi 7321392(29285568)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285595 kv 7321392(29285568) kvi 5123212(20492848)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285595; bufend = 77486083; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321392(29285568); kvend = 24614400(98457600); length = 8921393/6553600
   mapred.MapTask: (EQUATOR) 86554835 kvi 21638704(86554816)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86554835 kv 21638704(86554816) kvi 19443444(77773776)
 WARN  mapred.Task: Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task.statusUpdate(Task.java:1063)
	at mapred.MapTask.runNewMapper(MapTask.java:787)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
",NetworkDisconnection,962
110,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@6977f8ad)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@57b092d6
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:536870912+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48250246; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305444(69221776); length = 8908953/6553600
   mapred.MapTask: (EQUATOR) 57318998 kvi 14329744(57318976)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318998 kv 14329744(57318976) kvi 12130124(48520496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318998; bufend = 707922; bufvoid = 104857599
   mapred.MapTask: kvstart = 14329744(57318976); kvend = 5419856(21679424); length = 8909889/6553600
   mapred.MapTask: (EQUATOR) 9776658 kvi 2444160(9776640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9776658 kv 2444160(9776640) kvi 247856(991424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9776658; bufend = 57994455; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444160(9776640); kvend = 19741496(78965984); length = 8917065/6553600
   mapred.MapTask: (EQUATOR) 67063207 kvi 16765796(67063184)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67063207 kv 16765796(67063184) kvi 14570840(58283360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67063207; bufend = 10480387; bufvoid = 104857600
   mapred.MapTask: kvstart = 16765796(67063184); kvend = 7862980(31451920); length = 8902817/6553600
   mapred.MapTask: (EQUATOR) 19549139 kvi 4887280(19549120)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19549139 kv 4887280(19549120) kvi 2679652(10718608)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19549139; bufend = 67751785; bufvoid = 104857600
   mapred.MapTask: kvstart = 4887280(19549120); kvend = 22180828(88723312); length = 8920853/6553600
   mapred.MapTask: (EQUATOR) 76820537 kvi 19205128(76820512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76820537 kv 19205128(76820512) kvi 16995388(67981552)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76820537; bufend = 20214918; bufvoid = 104857600
   mapred.MapTask: kvstart = 19205128(76820512); kvend = 10296608(41186432); length = 8908521/6553600
   mapred.MapTask: (EQUATOR) 29283670 kvi 7320912(29283648)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29283670 kv 7320912(29283648) kvi 5125060(20500240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29283670; bufend = 77538589; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320912(29283648); kvend = 24627528(98510112); length = 8907785/6553600
   mapred.MapTask: (EQUATOR) 86607341 kvi 21651828(86607312)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86607341 kv 21651828(86607312) kvi 19456628(77826512)
 WARN  mapred.Task: Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task.statusUpdate(Task.java:1063)
	at mapred.MapTask.runNewMapper(MapTask.java:787)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
",NetworkDisconnection,962
111,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0023, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0023
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:134217728+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48254386; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306472(69225888); length = 8907925/6553600
   mapred.MapTask: (EQUATOR) 57323122 kvi 14330776(57323104)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57323122 kv 14330776(57323104) kvi 12127800(48511200)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57323122; bufend = 699496; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330776(57323104); kvend = 5417752(21671008); length = 8913025/6553600
   mapred.MapTask: (EQUATOR) 9768248 kvi 2442056(9768224)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9768248 kv 2442056(9768224) kvi 241544(966176)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9768248; bufend = 57981481; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442056(9768224); kvend = 19738252(78953008); length = 8918205/6553600
   mapred.MapTask: (EQUATOR) 67050233 kvi 16762552(67050208)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67050233 kv 16762552(67050208) kvi 14554320(58217280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67050233; bufend = 10441503; bufvoid = 104857600
   mapred.MapTask: kvstart = 16762552(67050208); kvend = 7853256(31413024); length = 8909297/6553600
   mapred.MapTask: (EQUATOR) 19510255 kvi 4877556(19510224)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19510255 kv 4877556(19510224) kvi 2674180(10696720)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19510255; bufend = 67733940; bufvoid = 104857600
   mapred.MapTask: kvstart = 4877556(19510224); kvend = 22176360(88705440); length = 8915597/6553600
   mapred.MapTask: (EQUATOR) 76802676 kvi 19200664(76802656)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76802676 kv 19200664(76802656) kvi 17001428(68005712)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62304; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76802676; bufend = 20189504; bufvoid = 104857600
   mapred.MapTask: kvstart = 19200664(76802656); kvend = 10290256(41161024); length = 8910409/6553600
   mapred.MapTask: (EQUATOR) 29258256 kvi 7314560(29258240)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29258256 kv 7314560(29258240) kvi 5109580(20438320)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29258256; bufend = 77515779; bufvoid = 104857600
   mapred.MapTask: kvstart = 7314560(29258240); kvend = 24621820(98487280); length = 8907141/6553600
   mapred.MapTask: (EQUATOR) 86584515 kvi 21646124(86584496)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86584515 kv 21646124(86584496) kvi 19440748(77762992)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 33 time(s); maxRetries=45
",NetworkDisconnection,932
113,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@11628d93)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4e621c4
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48249276; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600
   mapred.MapTask: (EQUATOR) 57318028 kvi 14329500(57318000)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318028; bufend = 686843; bufvoid = 104857600
   mapred.MapTask: kvstart = 14329500(57318000); kvend = 5414592(21658368); length = 8914909/6553600
   mapred.MapTask: (EQUATOR) 9755595 kvi 2438892(9755568)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9755595 kv 2438892(9755568) kvi 240952(963808)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9755595; bufend = 58006021; bufvoid = 104857600
   mapred.MapTask: kvstart = 2438892(9755568); kvend = 19744380(78977520); length = 8908913/6553600
   mapred.MapTask: (EQUATOR) 67074757 kvi 16768684(67074736)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67074757 kv 16768684(67074736) kvi 14558340(58233360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67074757; bufend = 10447270; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768684(67074736); kvend = 7854692(31418768); length = 8913993/6553600
   mapred.MapTask: (EQUATOR) 19516006 kvi 4878996(19515984)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19516006 kv 4878996(19515984) kvi 2677056(10708224)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 723.7852941897946 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 154.4690359206521 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.DFSClient: DFS Read
java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)
	at hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)
	at hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)
	at hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)
	at hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:917)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:568)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 36 more
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19516006; bufend = 41690327; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878996(19515984); kvend = 779716(3118864); length = 4099281/6553600
   mapred.MapTask: Finished spill 4
   mapred.Merger: Merging 5 sorted segments
   mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 167801399 bytes
 WARN  mapred.YarnChild: Exception running child : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)
	at hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)
	at hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)
	at hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)
	at hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:917)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:568)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 36 more

   mapred.Task: Runnning cleanup for the task
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
   mapred.YarnChild: Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.delete(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:521)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.delete(Unknown Source)
	at hdfs.DFSClient.delete(DFSClient.java:1929)
	at hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:638)
	at hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:634)
	at fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:634)
	at mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:466)
	at mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:455)
	at mapred.Task.taskCleanup(Task.java:1199)
	at mapred.YarnChild$3.run(YarnChild.java:185)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:182)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 24 more

   metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...
   metrics2.impl.MetricsSystemImpl: MapTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: MapTask metrics system shutdown complete.
",NetworkDisconnection,1428
114,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:1073741824+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48246341; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17304468(69217872); length = 8909929/6553600
   mapred.MapTask: (EQUATOR) 57315093 kvi 14328768(57315072)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57315093 kv 14328768(57315072) kvi 12122788(48491152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57315093; bufend = 701411; bufvoid = 104857600
   mapred.MapTask: kvstart = 14328768(57315072); kvend = 5418228(21672912); length = 8910541/6553600
   mapred.MapTask: (EQUATOR) 9770147 kvi 2442532(9770128)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9770147 kv 2442532(9770128) kvi 233240(932960)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9770147; bufend = 58023267; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442532(9770128); kvend = 19748700(78994800); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 67092019 kvi 16773000(67092000)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67092019 kv 16773000(67092000) kvi 14575980(58303920)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67092019; bufend = 10489855; bufvoid = 104857600
   mapred.MapTask: kvstart = 16773000(67092000); kvend = 7865344(31461376); length = 8907657/6553600
   mapred.MapTask: (EQUATOR) 19558607 kvi 4889644(19558576)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19558607 kv 4889644(19558576) kvi 2691852(10767408)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19558607; bufend = 67814201; bufvoid = 104857600
   mapred.MapTask: kvstart = 4889644(19558576); kvend = 22196432(88785728); length = 8907613/6553600
   mapred.MapTask: (EQUATOR) 76882953 kvi 19220732(76882928)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76882953 kv 19220732(76882928) kvi 17013156(68052624)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76882953; bufend = 20214328; bufvoid = 104857600
   mapred.MapTask: kvstart = 19220732(76882928); kvend = 10296460(41185840); length = 8924273/6553600
   mapred.MapTask: (EQUATOR) 29283080 kvi 7320764(29283056)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29283080 kv 7320764(29283056) kvi 5121912(20487648)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29283080; bufend = 77555951; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320764(29283056); kvend = 24631868(98527472); length = 8903297/6553600
   mapred.MapTask: (EQUATOR) 86624703 kvi 21656168(86624672)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86624703 kv 21656168(86624672) kvi 19462536(77850144)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 34 time(s); maxRetries=45
",NetworkDisconnection,945
115,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:1207959552+48562176
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48193401; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17291232(69164928); length = 8923165/6553600
   mapred.MapTask: (EQUATOR) 57262153 kvi 14315532(57262128)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57262153 kv 14315532(57262128) kvi 12120076(48480304)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57262153; bufend = 658166; bufvoid = 104857600
   mapred.MapTask: kvstart = 14315532(57262128); kvend = 5407424(21629696); length = 8908109/6553600
   mapred.MapTask: (EQUATOR) 9726918 kvi 2431724(9726896)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9726918 kv 2431724(9726896) kvi 228516(914064)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9726918; bufend = 49084664; bufvoid = 104857600
   mapred.MapTask: kvstart = 2431724(9726896); kvend = 21376588(85506352); length = 7269537/6553600
   mapred.MapTask: Finished spill 2
   mapred.Merger: Merging 3 sorted segments
   mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 105653983 bytes
   mapred.Task: Task:attempt_1445144423722_0020_m_000009_0 is done. And is in the process of committing
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 33 time(s); maxRetries=45
",NetworkDisconnection,719
117,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0023, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0023
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48249276; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600
   mapred.MapTask: (EQUATOR) 57318028 kvi 14329500(57318000)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318028; bufend = 686843; bufvoid = 104857600
   mapred.MapTask: kvstart = 14329500(57318000); kvend = 5414592(21658368); length = 8914909/6553600
   mapred.MapTask: (EQUATOR) 9755595 kvi 2438892(9755568)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9755595 kv 2438892(9755568) kvi 240952(963808)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9755595; bufend = 58006021; bufvoid = 104857600
   mapred.MapTask: kvstart = 2438892(9755568); kvend = 19744380(78977520); length = 8908913/6553600
   mapred.MapTask: (EQUATOR) 67074757 kvi 16768684(67074736)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67074757 kv 16768684(67074736) kvi 14558340(58233360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67074757; bufend = 10447270; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768684(67074736); kvend = 7854692(31418768); length = 8913993/6553600
   mapred.MapTask: (EQUATOR) 19516006 kvi 4878996(19515984)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19516006 kv 4878996(19515984) kvi 2677056(10708224)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62304; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19516006; bufend = 67756598; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878996(19515984); kvend = 22182024(88728096); length = 8911373/6553600
   mapred.MapTask: (EQUATOR) 76825334 kvi 19206328(76825312)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76825334 kv 19206328(76825312) kvi 17012764(68051056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76825334; bufend = 20217188; bufvoid = 104857598
   mapred.MapTask: kvstart = 19206328(76825312); kvend = 10297172(41188688); length = 8909157/6553600
   mapred.MapTask: (EQUATOR) 29285924 kvi 7321476(29285904)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285924 kv 7321476(29285904) kvi 5114320(20457280)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 0 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285924; bufend = 77503154; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321476(29285904); kvend = 24618672(98474688); length = 8917205/6553600
   mapred.MapTask: (EQUATOR) 86571906 kvi 21642972(86571888)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86571906 kv 21642972(86571888) kvi 19445800(77783200)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 34 time(s); maxRetries=45
",NetworkDisconnection,945
118,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0023, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0023
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:402653184+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48271024; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17310640(69242560); length = 8903757/6553600
   mapred.MapTask: (EQUATOR) 57339776 kvi 14334940(57339760)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57339776 kv 14334940(57339760) kvi 12140764(48563056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57339776; bufend = 743078; bufvoid = 104857600
   mapred.MapTask: kvstart = 14334940(57339760); kvend = 5428644(21714576); length = 8906297/6553600
   mapred.MapTask: (EQUATOR) 9811814 kvi 2452948(9811792)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9811814 kv 2452948(9811792) kvi 244148(976592)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9811814; bufend = 58036090; bufvoid = 104857600
   mapred.MapTask: kvstart = 2452948(9811792); kvend = 19751904(79007616); length = 8915445/6553600
   mapred.MapTask: (EQUATOR) 67104842 kvi 16776204(67104816)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67104842 kv 16776204(67104816) kvi 14566280(58265120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67104842; bufend = 10444035; bufvoid = 104857600
   mapred.MapTask: kvstart = 16776204(67104816); kvend = 7853884(31415536); length = 8922321/6553600
   mapred.MapTask: (EQUATOR) 19512771 kvi 4878188(19512752)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19512771 kv 4878188(19512752) kvi 2672040(10688160)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19512771; bufend = 67736095; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878188(19512752); kvend = 22176904(88707616); length = 8915685/6553600
   mapred.MapTask: (EQUATOR) 76804847 kvi 19201204(76804816)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76804847 kv 19201204(76804816) kvi 17013340(68053360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76804847; bufend = 20212744; bufvoid = 104857600
   mapred.MapTask: kvstart = 19201204(76804816); kvend = 10296064(41184256); length = 8905141/6553600
   mapred.MapTask: (EQUATOR) 29281496 kvi 7320368(29281472)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29281496 kv 7320368(29281472) kvi 5113468(20453872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29281496; bufend = 77550660; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320368(29281472); kvend = 24630548(98522192); length = 8904221/6553600
   mapred.MapTask: (EQUATOR) 86619412 kvi 21654848(86619392)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62304; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86619412 kv 21654848(86619392) kvi 19451324(77805296)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62304. Already tried 32 time(s); maxRetries=45
",NetworkDisconnection,919
122,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@c5f5deb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@630a7131
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.15""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":62270; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:62270. Already tried 32 time(s); maxRetries=45
",NetworkDisconnection,919
123,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@37b1886a
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:134217728+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48254386; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306472(69225888); length = 8907925/6553600
   mapred.MapTask: (EQUATOR) 57323122 kvi 14330776(57323104)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57323122 kv 14330776(57323104) kvi 12127800(48511200)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57323122; bufend = 699496; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330776(57323104); kvend = 5417752(21671008); length = 8913025/6553600
   mapred.MapTask: (EQUATOR) 9768248 kvi 2442056(9768224)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9768248 kv 2442056(9768224) kvi 241544(966176)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9768248; bufend = 57981481; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442056(9768224); kvend = 19738252(78953008); length = 8918205/6553600
   mapred.MapTask: (EQUATOR) 67050233 kvi 16762552(67050208)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67050233 kv 16762552(67050208) kvi 14554320(58217280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67050233; bufend = 10441503; bufvoid = 104857600
   mapred.MapTask: kvstart = 16762552(67050208); kvend = 7853256(31413024); length = 8909297/6553600
   mapred.MapTask: (EQUATOR) 19510255 kvi 4877556(19510224)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19510255 kv 4877556(19510224) kvi 2674180(10696720)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1460)
	at hdfs.DFSInputStream.readBuffer(DFSInputStream.java:773)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2558.365341350332 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.190.173.170:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information
java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...
 WARN  hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 398.80007962755826 msec.
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
 WARN  hdfs.DFSClient: DFS Read
java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)
	at hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)
	at hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)
	at hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)
	at hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:917)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:568)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 36 more
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19510255; bufend = 38350397; bufvoid = 104857600
   mapred.MapTask: kvstart = 4877556(19510224); kvend = 1391044(5564176); length = 3486513/6553600
   mapred.MapTask: Finished spill 4
   mapred.Merger: Merging 5 sorted segments
   mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 165262931 bytes
 WARN  mapred.YarnChild: Exception running child : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)
	at hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)
	at hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)
	at hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)
	at hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)
	at hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:917)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:568)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at util.LineReader.fillBuffer(LineReader.java:180)
	at util.LineReader.readDefaultLine(LineReader.java:216)
	at util.LineReader.readLine(LineReader.java:174)
	at mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:553)
	at mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
	at mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
	at mapreduce.Mapper.run(Mapper.java:144)
	at mapred.MapTask.runNewMapper(MapTask.java:784)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 36 more

   mapred.Task: Runnning cleanup for the task
 WARN  ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
   mapred.YarnChild: Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy15.delete(Unknown Source)
	at hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:521)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy16.delete(Unknown Source)
	at hdfs.DFSClient.delete(DFSClient.java:1929)
	at hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:638)
	at hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:634)
	at fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:634)
	at mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:466)
	at mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:455)
	at mapred.Task.taskCleanup(Task.java:1199)
	at mapred.YarnChild$3.run(YarnChild.java:185)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:182)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 24 more

   metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...
   metrics2.impl.MetricsSystemImpl: MapTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: MapTask metrics system shutdown complete.
",NetworkDisconnection,1428
126,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445144423722_0022, Ident: (mapreduce.security.token.JobTokenIdentifier@3d05ffdb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445144423722_0022
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@31adfcea
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:805306368+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48215795; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17296824(69187296); length = 8917573/6553600
   mapred.MapTask: (EQUATOR) 57284531 kvi 14321128(57284512)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57284531 kv 14321128(57284512) kvi 12112692(48450768)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57284531; bufend = 630553; bufvoid = 104857600
   mapred.MapTask: kvstart = 14321128(57284512); kvend = 5400516(21602064); length = 8920613/6553600
   mapred.MapTask: (EQUATOR) 9699305 kvi 2424820(9699280)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9699305 kv 2424820(9699280) kvi 222764(891056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9699305; bufend = 57911793; bufvoid = 104857600
   mapred.MapTask: kvstart = 2424820(9699280); kvend = 19720828(78883312); length = 8918393/6553600
   mapred.MapTask: (EQUATOR) 66980545 kvi 16745132(66980528)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 66980545 kv 16745132(66980528) kvi 14546624(58186496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 66980545; bufend = 10374147; bufvoid = 104857600
   mapred.MapTask: kvstart = 16745132(66980528); kvend = 7836420(31345680); length = 8908713/6553600
   mapred.MapTask: (EQUATOR) 19442899 kvi 4860720(19442880)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19442899 kv 4860720(19442880) kvi 2660780(10643120)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/127.0.0.1""; destination host is: ""10.190.173.170"":29630; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:757)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.NoRouteToHostException: No route to host: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

  [communication thread] mapred.Task: Process Thread Dump: Communication exception
12 active threads
Thread 21 (SpillThread):
  State: WAITING
  Blocked count: 0
  Waited count: 5
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3542d6bb
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1521)
Thread 20 (hdfs.PeerCache@385ec662):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 73
  Stack:
    java.lang.Thread.sleep(Native Method)
    hdfs.PeerCache.run(PeerCache.java:244)
    hdfs.PeerCache.access$000(PeerCache.java:41)
    hdfs.PeerCache$1.run(PeerCache.java:119)
    java.lang.Thread.run(Thread.java:724)
Thread 16 (communication thread):
  State: RUNNABLE
  Blocked count: 52
  Waited count: 169
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:165)
    util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:219)
    mapred.Task$TaskReporter.run(Task.java:760)
    java.lang.Thread.run(Thread.java:724)
Thread 15 (Thread for syncLogs):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 55
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 13 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 53
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
Thread 11 (Timer for 'MapTask' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 23
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 10 (Thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
    sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:142)
Thread 5 (Attach Listener):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 33
  Waited count: 19
  Waiting on java.lang.ref.ReferenceQueue$Lock@7ffc6a12
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 21
  Waited count: 21
  Waiting on java.lang.ref.Reference$Lock@14980563
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 6
  Waited count: 13
  Stack:
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
    sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
    sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
    net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
    net.SocketInputStream.read(SocketInputStream.java:161)
    hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
    hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
    hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
    hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
    hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:186)
    hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:146)
    hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:693)
    hdfs.DFSInputStream.readBuffer(DFSInputStream.java:749)
    hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:806)
    hdfs.DFSInputStream.read(DFSInputStream.java:847)
    java.io.DataInputStream.read(DataInputStream.java:100)

 WARN [communication thread] mapred.Task: Last retry, killing attempt_1445144423722_0022_m_000006_0
",NetworkDisconnection,1170
142,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0011, Ident: (mapreduce.security.token.JobTokenIdentifier@304cc139)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0011
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3c33282e
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@e55c068
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0011_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0011_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1065ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0011_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0011_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 878ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0011_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0011_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1043ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000007_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000007_1: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0011_m_000007_1 decomp: 60517368 len: 60517372 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000005_1 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0011_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0011_m_000007_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1987ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0011_m_000005_1
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 2383ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000002_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0011_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0011_m_000002_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 12227ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000006_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0011_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0011_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0011_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1227ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0011_m_000006_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 21918ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000004_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0011_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0011_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0011&reduce=0&map=attempt_1445182159119_0011_m_000003_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0011_m_000003_1: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0011_m_000003_1 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0011_m_000003_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 720ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0011_m_000004_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 14173ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0011_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0011_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0011_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out2/_temporary/1/task_1445182159119_0011_r_000000
   mapred.Task: Task 'attempt_1445182159119_0011_r_000000_0' done.
",DiskFull,1026
157,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@38b9d4f7)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@f32bade
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4537c278
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0002_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1266ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0002_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1344ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000003_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000003_1: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000003_1 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0002_m_000003_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4415ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000005_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000005_1: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000005_1 decomp: 216990140 len: 216990144 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0002_m_000005_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 15242ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000002_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0002_m_000002_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4207ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000008_1: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000008_1 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0002_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5203ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000004_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000004_1: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000004_1 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0002_m_000004_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2931ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0002_m_000006_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3079ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0002_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000007_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000007_1: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000007_1 decomp: 216976206 len: 216976210 to DISK
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000009_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000009_1: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0002_m_000009_1 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0002_m_000007_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5421ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0002_m_000009_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 4444ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
",DiskFull,1021
161,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0004_m_000000_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0004_m_000004_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445182159119_0004_m_000004_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0004_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2048ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0004_m_000005_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2390ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0004_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2760ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0004_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2951ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0004_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3779ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000000_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0004_m_000000_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1924ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0004_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1768ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000004_2 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000004_2: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000004_2 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0004_m_000004_2
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2233ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0004_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 36889ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0004&reduce=0&map=attempt_1445182159119_0004_m_000006_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0004_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0004_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0004_m_000006_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 87912ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0004_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0004_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0004_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445182159119_0004_r_000000
   mapred.Task: Task 'attempt_1445182159119_0004_r_000000_0' done.
",DiskFull,1080
165,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0005, Ident: (mapreduce.security.token.JobTokenIdentifier@2ec441bf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0005
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2eedd32f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b904247
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0005_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0005_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8745ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000002_0,attempt_1445182159119_0005_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0005_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000004_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0005_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000009_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0005_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0005_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0005_m_000004_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 12128ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0005_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0005_m_000000_0
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000006_0,attempt_1445182159119_0005_m_000005_0,attempt_1445182159119_0005_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0005_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 15076ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0005_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0005_m_000006_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0005_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0005_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 6685ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0005_m_000005_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0005_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0005_m_000007_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 27480ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0005&reduce=0&map=attempt_1445182159119_0005_m_000008_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0005_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0005_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0005_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 37975ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0005_m_000008_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 8479ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0005_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0005_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0005_r_000000_0' to hdfs://msra-sa-41:9000/out/out5/_temporary/1/task_1445182159119_0005_r_000000
   mapred.Task: Task 'attempt_1445182159119_0005_r_000000_0' done.
",DiskFull,921
178,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@1c667739)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@56f8d74c
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b895852
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 4 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0001_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0001_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1385ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000008_0,attempt_1445182159119_0001_m_000006_0,attempt_1445182159119_0001_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0001_m_000008_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0001_m_000006_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0001_m_000007_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 3816ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000003_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0001_m_000003_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 9084ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0001_m_000000_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 8160ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000002_0,attempt_1445182159119_0001_m_000004_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0001_r_000000_0: Got 2 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0001_m_000002_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0001_m_000004_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 10646ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0001&reduce=0&map=attempt_1445182159119_0001_m_000005_0,attempt_1445182159119_0001_m_000001_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0001_m_000005_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0001_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0001_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0001_m_000001_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 9279ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0001_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0001_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out1/_temporary/1/task_1445182159119_0001_r_000000
   mapred.Task: Task 'attempt_1445182159119_0001_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",DiskFull,866
179,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0014, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0014
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1000: Got 9 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000001_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000001_1: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000001_1 decomp: 60515836 len: 60515840 to DISK
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0014_m_000001_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 991ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 4 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000002_0,attempt_1445182159119_0014_m_000005_0,attempt_1445182159119_0014_m_000006_0,attempt_1445182159119_0014_m_000007_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0014_m_000003_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1324ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000004_0,attempt_1445182159119_0014_m_000008_0,attempt_1445182159119_0014_m_000009_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0014_m_000002_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0014_m_000004_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0014_m_000005_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0014_m_000008_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000009_1: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000009_1 decomp: 56695786 len: 56695790 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0014_m_000006_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0014_m_000009_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2033ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0014_m_000007_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 2609ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1000: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000000_1000 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0014_m_000000_1000
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 676ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0014_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0014_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0014_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/task_1445182159119_0014_r_000000
   mapred.Task: Task 'attempt_1445182159119_0014_r_000000_1000' done.
",DiskFull,767
193,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0015, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0015
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0015_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0015_m_000005_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0015_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 822ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0015_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 372ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0015_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 355ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0015_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 371ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0015_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 543ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445182159119_0015_m_000003_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000006_1: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000006_1 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0015_m_000006_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 820ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000004_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000004_1: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000004_1 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0015_m_000004_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 704ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000005_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0015_m_000005_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 647ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0015_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 771ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0015&reduce=0&map=attempt_1445182159119_0015_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0015_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0015_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0015_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 16299ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0015_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0015_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0015_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/task_1445182159119_0015_r_000000
   mapred.Task: Task 'attempt_1445182159119_0015_r_000000_0' done.
",DiskFull,1080
201,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000009_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0013_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 803ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000003_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0013_m_000003_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 747ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000006_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0013_m_000006_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 721ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000005_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0013_m_000005_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 732ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000008_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0013_m_000008_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 758ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000007_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0013_m_000007_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 684ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000004_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0013_m_000004_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 745ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000002_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000002_1: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000002_1 decomp: 60514392 len: 60514396 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0013_m_000002_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 550ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000000_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0013_m_000000_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 29601ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0013_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0013&reduce=0&map=attempt_1445182159119_0013_m_000001_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0013_m_000001_1: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0013_m_000001_1 decomp: 60515836 len: 60515840 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0013_m_000001_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 583ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0013_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0013_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0013_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out3/_temporary/1/task_1445182159119_0013_r_000000
   mapred.Task: Task 'attempt_1445182159119_0013_r_000000_0' done.
",DiskFull,1038
208,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0003_r_000000_1 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0003_m_000000_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0003_r_000000_1: Got 8 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000003_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0003_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0003_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1291ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 4 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000006_0,attempt_1445182159119_0003_m_000007_0,attempt_1445182159119_0003_m_000008_0,attempt_1445182159119_0003_m_000001_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0003_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2289ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000004_0,attempt_1445182159119_0003_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0003_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0003_m_000006_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445182159119_0003_m_000000_2'
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0003_m_000004_0
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0003_m_000007_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0003_m_000008_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000001_1: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000001_1 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0003_m_000001_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 10482ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0003_r_000000_1: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0003_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000002_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0003_m_000005_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 59534ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0003_m_000002_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2594ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0003_r_000000_1: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0003&reduce=0&map=attempt_1445182159119_0003_m_000000_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0003_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0003_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0003_m_000000_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 144586ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0003_r_000000_1 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0003_r_000000_1 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0003_r_000000_1' to hdfs://msra-sa-41:9000/out/out3/_temporary/1/task_1445182159119_0003_r_000000
   mapred.Task: Task 'attempt_1445182159119_0003_r_000000_1' done.
",DiskFull,842
214,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0002_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0002_r_000000_1000: Got 10 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000002_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0002_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445182159119_0002_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2435ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000001_0,attempt_1445182159119_0002_m_000003_1,attempt_1445182159119_0002_m_000004_1,attempt_1445182159119_0002_m_000006_1,attempt_1445182159119_0002_m_000007_1,attempt_1445182159119_0002_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445182159119_0002_m_000002_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2518ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0002&reduce=0&map=attempt_1445182159119_0002_m_000005_1,attempt_1445182159119_0002_m_000009_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000005_1: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0002_m_000005_1 decomp: 216990140 len: 216990144 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445182159119_0002_m_000005_1
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000009_1: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0002_m_000009_1 decomp: 172334804 len: 172334808 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445182159119_0002_m_000009_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2628ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445182159119_0002_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000003_1: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000003_1 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445182159119_0002_m_000003_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000004_1: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000004_1 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445182159119_0002_m_000004_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445182159119_0002_m_000006_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000007_1: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000007_1 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445182159119_0002_m_000007_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0002_m_000008_1: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0002_m_000008_1 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445182159119_0002_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 16313ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0002_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0002_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0002_r_000000_1000' to hdfs://msra-sa-41:9000/out/out2/_temporary/2/task_1445182159119_0002_r_000000
   mapred.Task: Task 'attempt_1445182159119_0002_r_000000_1000' done.
",DiskFull,734
215,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0014, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0014
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1: Got 8 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000002_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0014_m_000002_0
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0014_m_000003_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 919ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000009_1,attempt_1445182159119_0014_m_000004_0,attempt_1445182159119_0014_m_000008_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000009_1: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000009_1 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 958ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000007_0,attempt_1445182159119_0014_m_000005_0,attempt_1445182159119_0014_m_000006_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0014_m_000009_1
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0014_m_000007_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0014_m_000004_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0014_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0014_m_000008_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2034ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0014_m_000006_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2193ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_1: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000001_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000001_1: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0014_m_000001_1 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0014_m_000001_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 668ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: Exception in getting events
java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":51086; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.getMapCompletionEvents(Unknown Source)
	at mapreduce.task.reduce.EventFetcher.getMapCompletionEvents(EventFetcher.java:120)
	at mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:66)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)
",DiskFull,732
256,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0014, Ident: (mapreduce.security.token.JobTokenIdentifier@67b65d47)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0014
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@c16af82
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5bec0560
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000002_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0014_m_000002_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 39595ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000003_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0014_m_000007_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 23360ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000005_0,attempt_1445182159119_0014_m_000006_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0014_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0014_m_000003_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 25522ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000009_1,attempt_1445182159119_0014_m_000004_0,attempt_1445182159119_0014_m_000008_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000009_1: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0014_m_000009_1 decomp: 56695786 len: 56695790 to DISK
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 20147ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
 WARN  mapred.YarnChild: Exception running child : mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1
	at mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
	at mapred.ReduceTask.run(ReduceTask.java:376)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: fs.FSError: java.io.IOException: There is not enough space on the disk
	at fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:414)
	at fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)
	at fs.FSOutputSummer.write1(FSOutputSummer.java:124)
	at fs.FSOutputSummer.write(FSOutputSummer.java:110)
	at fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at mapreduce.task.reduce.OnDiskMapOutput.shuffle(OnDiskMapOutput.java:103)
	at mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:534)
	at mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:329)
	at mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)
Caused by: java.io.IOException: There is not enough space on the disk
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:345)
	at fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)
	... 14 more

   mapred.Task: Runnning cleanup for the task
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 12937ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000005_0,attempt_1445182159119_0014_m_000006_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
 ERROR [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Shuffle failed : local error on this node: 04DN8IQ/10.86.164.138
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 69ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000008_0,attempt_1445182159119_0014_m_000009_1,attempt_1445182159119_0014_m_000004_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0014_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
 ERROR [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Shuffle failed : local error on this node: 04DN8IQ/10.86.164.138
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 60ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0014&reduce=0&map=attempt_1445182159119_0014_m_000005_0,attempt_1445182159119_0014_m_000006_0 sent hash and received reply
 WARN  mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out4/_temporary/1/_temporary/attempt_1445182159119_0014_r_000000_0
",DiskFull,800
279,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@6ace4625)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@276e2428
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:402653184+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48271024; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17310640(69242560); length = 8903757/6553600
   mapred.MapTask: (EQUATOR) 57339776 kvi 14334940(57339760)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57339776 kv 14334940(57339760) kvi 12140764(48563056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57339776; bufend = 743078; bufvoid = 104857600
   mapred.MapTask: kvstart = 14334940(57339760); kvend = 5428644(21714576); length = 8906297/6553600
   mapred.MapTask: (EQUATOR) 9811814 kvi 2452948(9811792)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9811814 kv 2452948(9811792) kvi 244148(976592)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9811814; bufend = 58036090; bufvoid = 104857600
   mapred.MapTask: kvstart = 2452948(9811792); kvend = 19751904(79007616); length = 8915445/6553600
   mapred.MapTask: (EQUATOR) 67104842 kvi 16776204(67104816)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67104842 kv 16776204(67104816) kvi 14566280(58265120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67104842; bufend = 10444035; bufvoid = 104857600
   mapred.MapTask: kvstart = 16776204(67104816); kvend = 7853884(31415536); length = 8922321/6553600
   mapred.MapTask: (EQUATOR) 19512771 kvi 4878188(19512752)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19512771 kv 4878188(19512752) kvi 2988224(11952896)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19512771; bufend = 67736095; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878188(19512752); kvend = 22176904(88707616); length = 8915685/6553600
   mapred.MapTask: (EQUATOR) 76804847 kvi 19201204(76804816)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.9""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76804847 kv 19201204(76804816) kvi 17013340(68053360)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76804847; bufend = 20212744; bufvoid = 104857600
   mapred.MapTask: kvstart = 19201204(76804816); kvend = 10296064(41184256); length = 8905141/6553600
   mapred.MapTask: (EQUATOR) 29281496 kvi 7320368(29281472)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29281496 kv 7320368(29281472) kvi 5113468(20453872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29281496; bufend = 77550660; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320368(29281472); kvend = 24630548(98522192); length = 8904221/6553600
   mapred.MapTask: (EQUATOR) 86619412 kvi 21654848(86619392)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86619412 kv 21654848(86619392) kvi 19451324(77805296)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
288,"   mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445062781478_0020_000001
   mapreduce.v2.app.MRAppMaster: Executing with tokens:
   mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445062781478 } attemptId: 1 } keyId: 471522253)
   mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.
   mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null
   mapreduce.v2.app.MRAppMaster: OutputCommitter is mapreduce.lib.output.FileOutputCommitter
   yarn.event.AsyncDispatcher: Registering class mapreduce.jobhistory.EventType for class mapreduce.jobhistory.JobHistoryEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobEventType for class mapreduce.v2.app.MRAppMaster$JobEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskEventType for class mapreduce.v2.app.MRAppMaster$TaskEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskAttemptEventType for class mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.commit.CommitterEventType for class mapreduce.v2.app.commit.CommitterEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.speculate.Speculator$EventType for class mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.rm.ContainerAllocator$EventType for class mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.launcher.ContainerLauncher$EventType for class mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobFinishEvent$Type for class mapreduce.v2.app.MRAppMaster$JobFinishEventHandler
   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started
   mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1445062781478_0020 to jobTokenSecretManager
   mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1445062781478_0020 because: not enabled; too many maps; too much input;
   mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1445062781478_0020 = 1256521728. Number of splits = 10
   mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1445062781478_0020 = 1
   mapreduce.v2.app.job.impl.JobImpl: job_1445062781478_0020Job Transitioned from NEW to INITED
   mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1445062781478_0020.
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 20049] ipc.Server: Starting Socket Reader #1 for port 20049
   yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol mapreduce.v2.api.MRClientProtocolPB to the server
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 20049] ipc.Server: IPC Server listener on 20049: starting
   mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at MSRA-SA-41.fareast.corp.microsoft.com/10.190.173.170:20049
   org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
   http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined
   http.HttpServer2: Added global filter 'safety' (class=http.HttpServer2$QuotingInputFilter)
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context static
   http.HttpServer2: adding path spec: /mapreduce/*
   http.HttpServer2: adding path spec: /ws/*
   http.HttpServer2: Jetty bound to port 20056
   org.mortbay.log: jetty-6.1.26
   org.mortbay.log: Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\2\Jetty_0_0_0_0_20056_mapreduce____cv7jk6\webapp
   org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:20056
   yarn.webapp.WebApps: Web app /mapreduce started at 20056
   yarn.webapp.WebApps: Registered webapp guice modules
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1445062781478_0020
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 20059] ipc.Server: Starting Socket Reader #1 for port 20059
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 20059] ipc.Server: IPC Server listener on 20059: starting
   mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true
   mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3
   mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33
   yarn.client.RMProxy: Connecting to ResourceManager at MSRA-SA-41/10.190.173.170:8030
   mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:8192, vCores:32>
   mapreduce.v2.app.rm.RMContainerAllocator: queue: default
   mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
   yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445062781478_0020Job Transitioned from INITED to SETUP
  [CommitterEvent Processor #0] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445062781478_0020Job Transitioned from SETUP to RUNNING
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000001 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000002 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000003 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000004 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000005 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000006 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000007 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000008 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000009 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_r_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000004_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000005_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000006_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000007_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000008_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000009_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1>
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:1024, vCores:1>
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1445062781478_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020/job_1445062781478_0020_1.jhist
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:10 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:23552, vCores:-4> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:23552, vCores:-4>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 10
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000002 to attempt_1445062781478_0020_m_000000_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000003 to attempt_1445062781478_0020_m_000001_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000004 to attempt_1445062781478_0020_m_000002_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000005 to attempt_1445062781478_0020_m_000003_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000006 to attempt_1445062781478_0020_m_000004_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000007 to attempt_1445062781478_0020_m_000005_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000008 to attempt_1445062781478_0020_m_000006_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000009 to attempt_1445062781478_0020_m_000007_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000010 to attempt_1445062781478_0020_m_000008_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000011 to attempt_1445062781478_0020_m_000009_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:13312, vCores:-14>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020/job.jar
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020/job.xml
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000004_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000005_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000006_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000007_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000008_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000009_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000002 taskAttempt attempt_1445062781478_0020_m_000000_0
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000003 taskAttempt attempt_1445062781478_0020_m_000001_0
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000004 taskAttempt attempt_1445062781478_0020_m_000002_0
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000005 taskAttempt attempt_1445062781478_0020_m_000003_0
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000006 taskAttempt attempt_1445062781478_0020_m_000004_0
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000007 taskAttempt attempt_1445062781478_0020_m_000005_0
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000008 taskAttempt attempt_1445062781478_0020_m_000006_0
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000009 taskAttempt attempt_1445062781478_0020_m_000007_0
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000010 taskAttempt attempt_1445062781478_0020_m_000008_0
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000011 taskAttempt attempt_1445062781478_0020_m_000009_0
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000005_0
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000007_0
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000002_0
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000003_0
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000006_0
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000004_0
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000009_0
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000008_0
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000001_0
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000000_0
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [ContainerLauncher #9] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [ContainerLauncher #4] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000003_0 : 13562
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000004_0 : 13562
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000007_0 : 13562
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000002_0 : 13562
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000006_0 : 13562
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000005_0 : 13562
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000009_0 : 13562
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000001_0 : 13562
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000000_0 : 13562
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000008_0 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000009_0] using containerId: [container_1445062781478_0020_01_000011 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:42313]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000009_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000003_0] using containerId: [container_1445062781478_0020_01_000005 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000005_0] using containerId: [container_1445062781478_0020_01_000007 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000005_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000001_0] using containerId: [container_1445062781478_0020_01_000003 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000000_0] using containerId: [container_1445062781478_0020_01_000002 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000006_0] using containerId: [container_1445062781478_0020_01_000008 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:42313]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000006_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000002_0] using containerId: [container_1445062781478_0020_01_000004 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000004_0] using containerId: [container_1445062781478_0020_01_000006 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000004_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000007_0] using containerId: [container_1445062781478_0020_01_000009 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:42313]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000007_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000008_0] using containerId: [container_1445062781478_0020_01_000010 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:42313]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000008_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000009
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000009 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000003
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000003 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000005
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000005 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000001 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000000 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000006
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000006 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000002
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000002 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000004
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000004 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000007
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000007 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000008
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000008 Task Transitioned from SCHEDULED to RUNNING
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:13312, vCores:-14> knownNMs=4
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000011 asked for a task
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000008 asked for a task
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000008 given task: attempt_1445062781478_0020_m_000006_0
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000011 given task: attempt_1445062781478_0020_m_000009_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000010 asked for a task
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000010 given task: attempt_1445062781478_0020_m_000008_0
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000009 asked for a task
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000009 given task: attempt_1445062781478_0020_m_000007_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000005 asked for a task
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000005 given task: attempt_1445062781478_0020_m_000003_0
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000002 asked for a task
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000002 given task: attempt_1445062781478_0020_m_000000_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000004 asked for a task
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000004 given task: attempt_1445062781478_0020_m_000002_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000006 asked for a task
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000006 given task: attempt_1445062781478_0020_m_000004_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000003 asked for a task
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000003 given task: attempt_1445062781478_0020_m_000001_0
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000007 asked for a task
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000007 given task: attempt_1445062781478_0020_m_000005_0
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.295472
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.106964506
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.10681946
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.106881365
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.10635664
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.106493875
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.10660437
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.10680563
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.1066108
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.10685723
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.295472
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.106964506
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.10681946
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.106881365
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.10635664
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.106493875
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.10660437
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.10680563
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.1066108
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.10685723
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.324392
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.118691765
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.10681946
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.106881365
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.10635664
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.106493875
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.10660437
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.10680563
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.1066108
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.10685723
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.19068728
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.5222586
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.17581606
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.17251138
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.10635664
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.10660437
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.106493875
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.10859275
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.10865837
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.111328624
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.5323719
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.19266446
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.19255035
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.19258286
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.14796747
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.14760028
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.15814745
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.18022124
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.18001968
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.18213248
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.5323719
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.19266446
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.19255035
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.19258286
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.19158794
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.19212553
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.19209063
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.19242907
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.19211523
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.19247705
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.62294626
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.23040459
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.19255035
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.19258286
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.62294626
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.19158794
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.19212553
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.19209063
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.19242907
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.19211523
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.19247705
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.667
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.2783809
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.22767149
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.22737078
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.19209063
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.19158794
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.19212553
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.19242907
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.19211523
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.19247705
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.667
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.2783809
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.27825075
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.27811313
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.22709332
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.23562789
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.23281455
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.23688921
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.2376328
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.24027815
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.6673934
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.2783809
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.27825075
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.27811313
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.27765483
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.27772525
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.27696857
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.2781602
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.27776006
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.27813601
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.73986816
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.31827462
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.27825075
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.27811313
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.27765483
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.27772525
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.27696857
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.2781602
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.27813601
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.27776006
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.831289
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.36404583
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.35459784
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.3176316
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.27765483
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.27772525
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.27696857
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.2781602
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.27776006
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.27813601
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.9200961
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.36404583
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.3638923
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.3637686
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.29395512
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.30158108
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.3119641
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.33034268
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.328011
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.3285037
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.36404583
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 0.9999122
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000009_0 is : 1.0
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000009_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000009_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000011 taskAttempt attempt_1445062781478_0020_m_000009_0
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000009_0
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000009_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000009_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000009 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.3637686
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.3638923
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445062781478_0020_m_000003
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445062781478_0020_m_000003
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from NEW to UNASSIGNED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:13312, vCores:-14> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:13312, vCores:-14>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold reached. Scheduling reduces.
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: completedMapPercent 0.1 totalResourceLimit:<memory:23552, vCores:-4> finalMapResourceLimit:<memory:11264, vCores:11> finalReduceResourceLimit:<memory:12288, vCores:-15> netScheduledMapResource:<memory:11264, vCores:11> netScheduledReduceResource:<memory:0, vCores:0>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Ramping up 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=1 release= 0 newContainers=1 finishedContainers=1 resourcelimit=<memory:13312, vCores:-14> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000011
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000009_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000012 to attempt_1445062781478_0020_m_000003_1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:0 HostLocal:10 RackLocal:1
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000012 taskAttempt attempt_1445062781478_0020_m_000003_1
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000003_1
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000003_1 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000003_1] using containerId: [container_1445062781478_0020_01_000012 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:64642]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000003
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.36323506
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.3624012
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.36317363
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.36388028
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.36319977
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.36390656
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=1 finishedContainers=0 resourcelimit=<memory:12288, vCores:-15> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000013 to attempt_1445062781478_0020_r_000000_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:12 ContRel:0 HostLocal:10 RackLocal:1
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000013 taskAttempt attempt_1445062781478_0020_r_000000_0
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_r_000000_0
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_r_000000_0 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_r_000000_0] using containerId: [container_1445062781478_0020_01_000013 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:64642]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_r_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_r_000000 Task Transitioned from SCHEDULED to RUNNING
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.4399499
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.3637686
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.4214182
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:12288, vCores:-15> knownNMs=4
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.36323506
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.36317363
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.3624012
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.36388028
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.36319977
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.36390656
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.44980705
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.44950172
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.44964966
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.36323506
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.3624012
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.36317363
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.36388028
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.36319977
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.36390656
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_r_000013 asked for a task
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_r_000013 given task: attempt_1445062781478_0020_r_000000_0
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000012 asked for a task
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000012 given task: attempt_1445062781478_0020_m_000003_1
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.44980705
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.44950172
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.44964966
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.3649571
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.3885376
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.37019986
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.4256608
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.4020901
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.41387808
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 0 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.48276442
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.492068
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.44950172
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.44091162
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.44789755
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.44414717
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.44968578
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.448704
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.44950968
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.53543663
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.5352825
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.53521925
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445062781478_0020_m_000002
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445062781478_0020_m_000002
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from NEW to UNASSIGNED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:12 ContRel:0 HostLocal:10 RackLocal:1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:12288, vCores:-15> knownNMs=4
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000014 to attempt_1445062781478_0020_m_000002_1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:13 ContRel:0 HostLocal:10 RackLocal:2
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000014 taskAttempt attempt_1445062781478_0020_m_000002_1
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000002_1
  [ContainerLauncher #9] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.4486067
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.44789755
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.44859612
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.44968578
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000002_1 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000002_1] using containerId: [container_1445062781478_0020_01_000014 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:64642]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000002
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.44950968
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.448704
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:11264, vCores:-16> knownNMs=4
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.0
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.53543663
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.5352825
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.53521925
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.06546045
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.4486067
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.44789755
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.44859612
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.45195487
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.44950968
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.448704
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.53543663
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.5352825
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.53521925
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.0
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.100270376
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.46088505
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.48774862
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.46827805
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.52721363
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.5229624
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000014 asked for a task
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000014 given task: attempt_1445062781478_0020_m_000002_1
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.51590294
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6210422
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.6104692
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.5541956
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.0
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.5343203
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.53341997
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.5342037
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.5352028
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.5352021
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.53425497
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6210422
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.620844
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.6207798
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.5343203
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.53341997
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.5352028
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.5342037
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.5352021
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.53425497
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6210422
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445062781478_0020_m_000001
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445062781478_0020_m_000001
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from NEW to UNASSIGNED
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.620844
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.6207798
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:13 ContRel:0 HostLocal:10 RackLocal:2
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:11264, vCores:-16> knownNMs=4
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000015 to attempt_1445062781478_0020_m_000001_1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000015 taskAttempt attempt_1445062781478_0020_m_000001_1
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000001_1
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.53341997
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.5343203
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.5352028
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.5342037
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.53425497
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000001_1 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000001_1] using containerId: [container_1445062781478_0020_01_000015 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:64642]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000001
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.5352021
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:10240, vCores:-17> knownNMs=4
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6252609
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.6395945
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.6207798
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.065619476
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6252609
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.6395945
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.5547179
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.55340356
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.61354107
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.56033367
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.6013707
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.6109639
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.667
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.6207798
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.667
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.667
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.099842764
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.6199081
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.61898744
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.6208445
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.6196791
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.6209487
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.6197233
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.667
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.667
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.667
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.6199081
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.61898744
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.6208445
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.6196791
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.6197233
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.6209487
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000015 asked for a task
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000015 given task: attempt_1445062781478_0020_m_000001_1
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.667
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.667
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.667
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.6199081
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.61898744
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.6298147
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.6196791
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.6197233
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.6292532
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445062781478_0020_m_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445062781478_0020_m_000000
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from NEW to UNASSIGNED
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.6926627
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.6298147
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.67778707
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.6292532
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.69040716
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:10240, vCores:-17> knownNMs=4
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.6197233
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445062781478_0020_01_000016 to attempt_1445062781478_0020_m_000000_1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:13 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445062781478_0020_01_000016 taskAttempt attempt_1445062781478_0020_m_000000_1
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445062781478_0020_m_000000_1
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445062781478_0020_m_000000_1 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445062781478_0020_m_000000_1] using containerId: [container_1445062781478_0020_01_000016 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:42313]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445062781478_0020_m_000000
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.642735
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.63439417
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.667
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.6492648
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.667
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.667
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445062781478_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:9216, vCores:-18> knownNMs=4
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.6492648
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.642735
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.63439417
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.72653395
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.69718146
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.71025634
  [Socket Reader #1 for port 20059] SecurityLogger.ipc.Server: Auth successful for job_1445062781478_0020 (auth:SIMPLE)
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445062781478_0020_m_000016 asked for a task
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445062781478_0020_m_000016 given task: attempt_1445062781478_0020_m_000000_1
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.667
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.667
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.667
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.667
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.667
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.667
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.74964625
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.72925836
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.74426454
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.667
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.667
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.667
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.667
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.667
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.667
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.106493875
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.7772497
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.7590936
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.7761037
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.038316555
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.667
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.6847919
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.667
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.667
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.673317
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.68015534
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.13548234
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.10635664
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.79608333
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.8008257
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.8172877
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.08962054
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.72243494
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.68729776
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.6896647
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.69483376
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.71307385
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.7218813
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.10635664
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.17886485
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.8151878
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.84266984
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.8609328
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.72854125
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.73011017
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.7617643
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.73784685
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.75421786
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.76227516
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.10635664
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.8380622
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.8721353
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.89160633
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.76939386
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.7645177
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.7947242
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.77493405
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.7883416
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.7964324
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.19158794
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.85705614
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.90842927
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.92748094
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.80689293
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.8311372
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.79926544
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.81321025
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.82810265
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.8367236
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.10660437
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.19158794
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.88423383
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.95810694
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.93819666
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.86848557
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.8394517
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.8292054
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.84569126
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.8664425
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.8738835
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.1577693
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.19158794
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.90345484
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 0.9670727
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 0.98843527
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000007_0 is : 1.0
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000007_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000007_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000009 taskAttempt attempt_1445062781478_0020_m_000007_0
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000007_0
  [ContainerLauncher #4] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000007_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000007_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000007 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 2
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.87683386
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.92119133
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.8632102
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.8830246
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.921389
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.9288331
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:13 AssignedReds:1 CompletedMaps:2 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.19158794
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.19212553
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000008_0 is : 1.0
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000008_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000008_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000010 taskAttempt attempt_1445062781478_0020_m_000008_0
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000008_0
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000008_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000008_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000008 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 3
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 2 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:13 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000009
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000007_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.92723
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 3 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000010
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000008_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.9075622
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.96188486
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.89152
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.9147134
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 0.9632846
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 0.97013986
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 3 maxEvents 10000
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.26724863
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.19212553
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 0.9586107
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 3 maxEvents 10000
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_1 is : 0.1066108
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000005_0 is : 1.0
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000005_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000005_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000007 taskAttempt attempt_1445062781478_0020_m_000005_0
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000005_0
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000005_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000005_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000005 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:4 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 3 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.9449853
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 0.9982183
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.92587435
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.9528737
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 1.0
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000001_0 is : 1.0
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000001_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000003 taskAttempt attempt_1445062781478_0020_m_000001_0
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000001_0
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000001_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445062781478_0020_m_000001_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000001 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 5
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000015 taskAttempt attempt_1445062781478_0020_m_000001_1
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000001_1
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445062781478_0020_m_000001
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000004_0 is : 1.0
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000004_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000004_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000006 taskAttempt attempt_1445062781478_0020_m_000004_0
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000004_0
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000004_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000004_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000004 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 6
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #1] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #1] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000001_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000001_1 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:6 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000007
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000003
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000005_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:9 AssignedReds:1 CompletedMaps:6 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000001_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 4 maxEvents 10000
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.27696857
  [Socket Reader #1 for port 20059] ipc.Server: Socket Reader #1 for port 20059: readAndProcess from client 10.86.169.121 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000006_0 is : 1.0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000006
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:8 AssignedReds:1 CompletedMaps:6 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000004_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000006_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000006_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000008 taskAttempt attempt_1445062781478_0020_m_000006_0
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000006_0
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000006_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000006_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000006 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 7
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_1 is : 0.19212553
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 6 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:8 AssignedReds:1 CompletedMaps:7 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000015
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:7 AssignedReds:1 CompletedMaps:7 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000001_1: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 0.9881778
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.9660619
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 0.99586475
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 7 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000002_0 is : 1.0
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000002_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000004 taskAttempt attempt_1445062781478_0020_m_000002_0
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000002_0
  [ContainerLauncher #9] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000002_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445062781478_0020_m_000002_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000002 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 8
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000014 taskAttempt attempt_1445062781478_0020_m_000002_1
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000002_1
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:7 AssignedReds:1 CompletedMaps:8 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000008
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:6 AssignedReds:1 CompletedMaps:8 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000006_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #2] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #2] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000002_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000002_1 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_1 is : 0.27696857
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 7 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000000_0 is : 1.0
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000002 taskAttempt attempt_1445062781478_0020_m_000000_0
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000000_0
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445062781478_0020_m_000000_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000000 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 9
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000016 taskAttempt attempt_1445062781478_0020_m_000000_1
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000000_1
  [ContainerLauncher #4] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:42313
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #3] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #3] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000000_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000000_1 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [Socket Reader #1 for port 20059] ipc.Server: Socket Reader #1 for port 20059: readAndProcess from client 10.190.173.170 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [Socket Reader #1 for port 20059] ipc.Server: Socket Reader #1 for port 20059: readAndProcess from client 10.86.169.121 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:6 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000004
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:5 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000002_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 8 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000014
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000016
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000002_1: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000002
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000000_1: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:2 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000000_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 0.9941143
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 9 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_0 is : 1.0
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_m_000003_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000005 taskAttempt attempt_1445062781478_0020_m_000003_0
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000003_0
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_m_000003_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445062781478_0020_m_000003_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_m_000003 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 10
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000012 taskAttempt attempt_1445062781478_0020_m_000003_1
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_m_000003_1
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_m_000003_1 is : 0.19209063
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #4] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #4] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000003_1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_m_000003_1 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [Socket Reader #1 for port 20059] ipc.Server: Socket Reader #1 for port 20059: readAndProcess from client 10.86.169.121 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 9 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:2 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000012
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445062781478_0020_01_000005
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000003_1: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445062781478_0020_m_000003_0: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.033333335
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.06666667
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.10000001
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.10000001
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.10000001
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.10000001
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.10000001
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.13333334
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.13333334
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.13333334
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.13333334
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.16666667
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.16666667
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.16666667
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.20000002
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.23333333
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.23333333
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.23333333
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.23333333
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 21 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.26666668
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.26666668
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.26666668
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.3
  [IPC Server handler 15 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.3
  [IPC Server handler 26 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.3
  [IPC Server handler 3 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445062781478_0020_r_000000_0. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.3
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.3
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.6667707
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.67623824
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.6921079
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.7082201
  [IPC Server handler 11 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.72405237
  [IPC Server handler 16 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.7400066
  [IPC Server handler 24 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.7562506
  [IPC Server handler 12 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.7726015
  [IPC Server handler 8 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.7866131
  [IPC Server handler 0 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.8028904
  [IPC Server handler 1 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.8189403
  [IPC Server handler 22 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.83548474
  [IPC Server handler 20 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.85192525
  [IPC Server handler 10 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.86731374
  [IPC Server handler 28 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.8838906
  [IPC Server handler 2 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.9003594
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.9160572
  [IPC Server handler 19 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.93260217
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.9489118
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.96544206
  [IPC Server handler 14 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.98081887
  [IPC Server handler 5 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 0.9972495
  [IPC Server handler 13 on 20059] mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1445062781478_0020_r_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: attempt_1445062781478_0020_r_000000_0 given a go for committing the task output.
  [IPC Server handler 18 on 20059] mapred.TaskAttemptListenerImpl: Commit go/no-go request from attempt_1445062781478_0020_r_000000_0
  [IPC Server handler 18 on 20059] mapreduce.v2.app.job.impl.TaskImpl: Result of canCommit for attempt_1445062781478_0020_r_000000_0:true
  [IPC Server handler 25 on 20059] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445062781478_0020_r_000000_0 is : 1.0
  [IPC Server handler 4 on 20059] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445062781478_0020_r_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445062781478_0020_01_000013 taskAttempt attempt_1445062781478_0020_r_000000_0
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445062781478_0020_r_000000_0
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:64642
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445062781478_0020_r_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445062781478_0020_r_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445062781478_0020_r_000000 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 11
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445062781478_0020Job Transitioned from RUNNING to COMMITTING
  [CommitterEvent Processor #0] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_COMMIT
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Calling handler for JobFinishedEvent 
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445062781478_0020Job Transitioned from COMMITTING to SUCCEEDED
  [Thread-104] mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry
  [Thread-104] mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true
  [Thread-104] mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true
  [Thread-104] mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true
  [Thread-104] mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true
  [Thread-104] mapreduce.v2.app.MRAppMaster: Calling stop for all the services
  [Thread-104] mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020/job_1445062781478_0020_1.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020-1445073079681-msrabi-pagerank-1445073609167-10-1-SUCCEEDED-default-1445073321328.jhist_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020-1445073079681-msrabi-pagerank-1445073609167-10-1-SUCCEEDED-default-1445073321328.jhist_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020/job_1445062781478_0020_1_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020.summary_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020.summary
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020_conf.xml_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020_conf.xml
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020-1445073079681-msrabi-pagerank-1445073609167-10-1-SUCCEEDED-default-1445073321328.jhist_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445062781478_0020-1445073079681-msrabi-pagerank-1445073609167-10-1-SUCCEEDED-default-1445073321328.jhist
  [Thread-104] mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop()
  [Thread-104] mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to 
  [Thread-104] mapreduce.v2.app.rm.RMContainerAllocator: History url is http://MSRA-SA-41.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445062781478_0020
  [Thread-104] mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.
  [Thread-104] mapreduce.v2.app.rm.RMContainerAllocator: Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:15 ContRel:0 HostLocal:11 RackLocal:3
  [Thread-104] mapreduce.v2.app.MRAppMaster: Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445062781478_0020
  [Thread-104] ipc.Server: Stopping server on 20059
  [IPC Server listener on 20059] ipc.Server: Stopping IPC Server listener on 20059
  [TaskHeartbeatHandler PingChecker] mapreduce.v2.app.TaskHeartbeatHandler: TaskHeartbeatHandler thread interrupted
  [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
",MachineDown,15986
298,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@c5f5deb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@17a80e30
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48249276; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600
   mapred.MapTask: (EQUATOR) 57318028 kvi 14329500(57318000)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318028; bufend = 686843; bufvoid = 104857600
   mapred.MapTask: kvstart = 14329500(57318000); kvend = 5414592(21658368); length = 8914909/6553600
   mapred.MapTask: (EQUATOR) 9755595 kvi 2438892(9755568)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9755595 kv 2438892(9755568) kvi 240952(963808)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9755595; bufend = 58006021; bufvoid = 104857600
   mapred.MapTask: kvstart = 2438892(9755568); kvend = 19744380(78977520); length = 8908913/6553600
   mapred.MapTask: (EQUATOR) 67074757 kvi 16768684(67074736)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67074757 kv 16768684(67074736) kvi 14558340(58233360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67074757; bufend = 10447270; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768684(67074736); kvend = 7854692(31418768); length = 8913993/6553600
   mapred.MapTask: (EQUATOR) 19516006 kvi 4878996(19515984)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19516006 kv 4878996(19515984) kvi 2677056(10708224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19516006; bufend = 67756598; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878996(19515984); kvend = 22182024(88728096); length = 8911373/6553600
   mapred.MapTask: (EQUATOR) 76825334 kvi 19206328(76825312)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76825334 kv 19206328(76825312) kvi 17012764(68051056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76825334; bufend = 20217188; bufvoid = 104857598
   mapred.MapTask: kvstart = 19206328(76825312); kvend = 10297172(41188688); length = 8909157/6553600
   mapred.MapTask: (EQUATOR) 29285924 kvi 7321476(29285904)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285924 kv 7321476(29285904) kvi 5114320(20457280)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285924; bufend = 77503154; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321476(29285904); kvend = 24618672(98474688); length = 8917205/6553600
   mapred.MapTask: (EQUATOR) 86571906 kvi 21642972(86571888)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86571906 kv 21642972(86571888) kvi 19445800(77783200)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
305,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@3db9b677)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3aa191be
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@1cbd8f1
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0001_r_000000_0: Got 7 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000002_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0001_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0001_r_000000_0: Got 2 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0001_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5943ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000001_0,attempt_1445094324383_0001_m_000008_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0001_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0001_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8239ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000003_0,attempt_1445094324383_0001_m_000006_0,attempt_1445094324383_0001_m_000005_0,attempt_1445094324383_0001_m_000004_0,attempt_1445094324383_0001_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0001_m_000001_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0001_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0001_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0001_m_000008_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 8388ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0001_m_000006_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0001_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0001_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0001_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 22560ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000000_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0001_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0001_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0001_m_000000_1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4978ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445094324383_0001_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445094324383_0001_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445094324383_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445094324383_0001_r_000000
   mapred.Task: Task 'attempt_1445094324383_0001_r_000000_0' done.
",MachineDown,779
312,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0018, Ident: (mapreduce.security.token.JobTokenIdentifier@75a61582)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0018
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@57c2f8b0
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@61ea2c55
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_1000: Got 9 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000009_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000003_1 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0018_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000003_1: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000003_1 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0018_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 857ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0018_m_000003_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 23907ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000004_1,attempt_1445062781478_0018_m_000008_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000004_1: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000004_1 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0018_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 29789ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 4 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000001_0,attempt_1445062781478_0018_m_000002_1,attempt_1445062781478_0018_m_000005_1,attempt_1445062781478_0018_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0018_m_000004_1
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0018_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000002_1: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000002_1 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0018_m_000008_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 39042ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0018_m_000002_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0018_m_000005_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000006_1: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000006_1 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0018_m_000006_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 69703ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000007_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000007_1000: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000007_1000 decomp: 60517368 len: 60517372 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0018_m_000007_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 603ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445062781478_0018_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0018_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0018_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out3/_temporary/2/task_1445062781478_0018_r_000000
   mapred.Task: Task 'attempt_1445062781478_0018_r_000000_1000' done.
",MachineDown,802
319,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@490ef5a5)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@5bff8187
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:536870912+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48250246; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305444(69221776); length = 8908953/6553600
   mapred.MapTask: (EQUATOR) 57318998 kvi 14329744(57318976)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318998 kv 14329744(57318976) kvi 12130124(48520496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318998; bufend = 707922; bufvoid = 104857599
   mapred.MapTask: kvstart = 14329744(57318976); kvend = 5419856(21679424); length = 8909889/6553600
   mapred.MapTask: (EQUATOR) 9776658 kvi 2444160(9776640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9776658 kv 2444160(9776640) kvi 247856(991424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9776658; bufend = 57994455; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444160(9776640); kvend = 19741496(78965984); length = 8917065/6553600
   mapred.MapTask: (EQUATOR) 67063207 kvi 16765796(67063184)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67063207 kv 16765796(67063184) kvi 14570840(58283360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67063207; bufend = 10480387; bufvoid = 104857600
   mapred.MapTask: kvstart = 16765796(67063184); kvend = 7862980(31451920); length = 8902817/6553600
   mapred.MapTask: (EQUATOR) 19549139 kvi 4887280(19549120)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19549139 kv 4887280(19549120) kvi 2679652(10718608)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19549139; bufend = 67751785; bufvoid = 104857600
   mapred.MapTask: kvstart = 4887280(19549120); kvend = 22180828(88723312); length = 8920853/6553600
   mapred.MapTask: (EQUATOR) 76820537 kvi 19205128(76820512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76820537 kv 19205128(76820512) kvi 16995388(67981552)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76820537; bufend = 20214918; bufvoid = 104857600
   mapred.MapTask: kvstart = 19205128(76820512); kvend = 10296608(41186432); length = 8908521/6553600
   mapred.MapTask: (EQUATOR) 29283670 kvi 7320912(29283648)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29283670 kv 7320912(29283648) kvi 5125060(20500240)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29283670; bufend = 77538589; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320912(29283648); kvend = 24627528(98510112); length = 8907785/6553600
   mapred.MapTask: (EQUATOR) 86607341 kvi 21651828(86607312)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86607341 kv 21651828(86607312) kvi 19456628(77826512)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
322,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@2ebb9a37)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2945fc29
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:805306368+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48215795; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17296824(69187296); length = 8917573/6553600
   mapred.MapTask: (EQUATOR) 57284531 kvi 14321128(57284512)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57284531 kv 14321128(57284512) kvi 12112692(48450768)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57284531; bufend = 630553; bufvoid = 104857600
   mapred.MapTask: kvstart = 14321128(57284512); kvend = 5400516(21602064); length = 8920613/6553600
   mapred.MapTask: (EQUATOR) 9699305 kvi 2424820(9699280)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9699305 kv 2424820(9699280) kvi 222764(891056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9699305; bufend = 57911793; bufvoid = 104857600
   mapred.MapTask: kvstart = 2424820(9699280); kvend = 19720828(78883312); length = 8918393/6553600
   mapred.MapTask: (EQUATOR) 66980545 kvi 16745132(66980528)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 66980545 kv 16745132(66980528) kvi 14546624(58186496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 66980545; bufend = 10374147; bufvoid = 104857600
   mapred.MapTask: kvstart = 16745132(66980528); kvend = 7836420(31345680); length = 8908713/6553600
   mapred.MapTask: (EQUATOR) 19442899 kvi 4860720(19442880)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19442899 kv 4860720(19442880) kvi 2660780(10643120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19442899; bufend = 67657921; bufvoid = 104857600
   mapred.MapTask: kvstart = 4860720(19442880); kvend = 22157356(88629424); length = 8917765/6553600
   mapred.MapTask: (EQUATOR) 76726657 kvi 19181660(76726640)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76726657 kv 19181660(76726640) kvi 16980352(67921408)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76726657; bufend = 20115617; bufvoid = 104857600
   mapred.MapTask: kvstart = 19181660(76726640); kvend = 10271780(41087120); length = 8909881/6553600
   mapred.MapTask: (EQUATOR) 29184353 kvi 7296084(29184336)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.9""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29184353 kv 7296084(29184336) kvi 5097788(20391152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29184353; bufend = 77442473; bufvoid = 104857600
   mapred.MapTask: kvstart = 7296084(29184336); kvend = 24603496(98413984); length = 8906989/6553600
   mapred.MapTask: (EQUATOR) 86511225 kvi 21627800(86511200)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86511225 kv 21627800(86511200) kvi 19431636(77726544)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
",MachineDown,893
335,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@271ff531)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@12dfba11
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4c2d5b57
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0001&reduce=0&map=attempt_1445076437777_0001_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0001_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0001_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445076437777_0001_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 15708ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: Exception in getting events
java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.getMapCompletionEvents(Unknown Source)
	at mapreduce.task.reduce.EventFetcher.getMapCompletionEvents(EventFetcher.java:120)
	at mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:66)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
",MachineDown,788
340,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@c5f5deb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3ff0750a
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:671088640+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48241717; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17303312(69213248); length = 8911085/6553600
   mapred.MapTask: (EQUATOR) 57310469 kvi 14327612(57310448)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57310469 kv 14327612(57310448) kvi 12124056(48496224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57310469; bufend = 677160; bufvoid = 104857600
   mapred.MapTask: kvstart = 14327612(57310448); kvend = 5412172(21648688); length = 8915441/6553600
   mapred.MapTask: (EQUATOR) 9745912 kvi 2436472(9745888)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9745912 kv 2436472(9745888) kvi 243380(973520)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9745912; bufend = 58001423; bufvoid = 104857600
   mapred.MapTask: kvstart = 2436472(9745888); kvend = 19743236(78972944); length = 8907637/6553600
   mapred.MapTask: (EQUATOR) 67070175 kvi 16767536(67070144)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67070175 kv 16767536(67070144) kvi 14572400(58289600)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67070175; bufend = 10445315; bufvoid = 104857600
   mapred.MapTask: kvstart = 16767536(67070144); kvend = 7854204(31416816); length = 8913333/6553600
   mapred.MapTask: (EQUATOR) 19514051 kvi 4878508(19514032)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.9""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19514051 kv 4878508(19514032) kvi 2676468(10705872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19514051; bufend = 67765795; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878508(19514032); kvend = 22184324(88737296); length = 8908585/6553600
   mapred.MapTask: (EQUATOR) 76834531 kvi 19208628(76834512)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76834531 kv 19208628(76834512) kvi 17011572(68046288)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76834531; bufend = 20216843; bufvoid = 104857600
   mapred.MapTask: kvstart = 19208628(76834512); kvend = 10297092(41188368); length = 8911537/6553600
   mapred.MapTask: (EQUATOR) 29285595 kvi 7321392(29285568)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285595 kv 7321392(29285568) kvi 5123212(20492848)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285595; bufend = 77486083; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321392(29285568); kvend = 24614400(98457600); length = 8921393/6553600
   mapred.MapTask: (EQUATOR) 86554835 kvi 21638704(86554816)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86554835 kv 21638704(86554816) kvi 19443444(77773776)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
346,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_2 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_2: Got 10 new map-outputs
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000009_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000000_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0002_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0002_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2080ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000005_0,attempt_1445094324383_0002_m_000008_0,attempt_1445094324383_0002_m_000001_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0002_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0002_m_000000_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 2772ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000004_0,attempt_1445094324383_0002_m_000003_0,attempt_1445094324383_0002_m_000006_0,attempt_1445094324383_0002_m_000002_0,attempt_1445094324383_0002_m_000007_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0002_m_000004_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0002_m_000003_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0002_m_000005_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0002_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0002_m_000006_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0002_m_000008_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0002_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0002_m_000002_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0002_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0002_m_000001_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 9707ms
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0002_m_000007_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 10352ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445094324383_0002_r_000000_2 is done. And is in the process of committing
   mapred.Task: Task attempt_1445094324383_0002_r_000000_2 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445094324383_0002_r_000000_2' to hdfs://msra-sa-41:9000/out/out1/_temporary/1/task_1445094324383_0002_r_000000
   mapred.Task: Task 'attempt_1445094324383_0002_r_000000_2' done.
",MachineDown,720
356,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@75a61582)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6270c836
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:134217728+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48254386; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306472(69225888); length = 8907925/6553600
   mapred.MapTask: (EQUATOR) 57323122 kvi 14330776(57323104)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57323122 kv 14330776(57323104) kvi 12127800(48511200)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57323122; bufend = 699496; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330776(57323104); kvend = 5417752(21671008); length = 8913025/6553600
   mapred.MapTask: (EQUATOR) 9768248 kvi 2442056(9768224)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9768248 kv 2442056(9768224) kvi 241544(966176)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9768248; bufend = 57981481; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442056(9768224); kvend = 19738252(78953008); length = 8918205/6553600
   mapred.MapTask: (EQUATOR) 67050233 kvi 16762552(67050208)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67050233 kv 16762552(67050208) kvi 14554320(58217280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67050233; bufend = 10441503; bufvoid = 104857600
   mapred.MapTask: kvstart = 16762552(67050208); kvend = 7853256(31413024); length = 8909297/6553600
   mapred.MapTask: (EQUATOR) 19510255 kvi 4877556(19510224)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19510255 kv 4877556(19510224) kvi 2674180(10696720)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19510255; bufend = 67733940; bufvoid = 104857600
   mapred.MapTask: kvstart = 4877556(19510224); kvend = 22176360(88705440); length = 8915597/6553600
   mapred.MapTask: (EQUATOR) 76802676 kvi 19200664(76802656)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76802676 kv 19200664(76802656) kvi 17001428(68005712)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76802676; bufend = 20189504; bufvoid = 104857600
   mapred.MapTask: kvstart = 19200664(76802656); kvend = 10290256(41161024); length = 8910409/6553600
   mapred.MapTask: (EQUATOR) 29258256 kvi 7314560(29258240)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29258256 kv 7314560(29258240) kvi 5109580(20438320)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29258256; bufend = 77515779; bufvoid = 104857600
   mapred.MapTask: kvstart = 7314560(29258240); kvend = 24621820(98487280); length = 8907141/6553600
   mapred.MapTask: (EQUATOR) 86584515 kvi 21646124(86584496)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86584515 kv 21646124(86584496) kvi 19440748(77762992)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
366,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0002_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 9202ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000010_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0002_m_000010_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 17994ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000003_0'
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000002_0: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000002_0 decomp: 216986711 len: 216986715 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0002_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2021ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000004_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000004_0: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000004_0 decomp: 216992138 len: 216992142 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0002_m_000004_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2247ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000005_0: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000005_0 decomp: 216996859 len: 216996863 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0002_m_000005_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1941ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000000_0: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000000_0 decomp: 227948846 len: 227948850 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0002_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2589ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000008_0: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000008_0 decomp: 216989049 len: 216989053 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0002_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2031ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000009_0: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000009_0 decomp: 216983391 len: 216983395 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0002_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2352ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000011_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0002_m_000011_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2082ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000007_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0002_m_000007_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1603ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000003_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000003_1: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000003_1 decomp: 216980068 len: 216980072 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0002_m_000003_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2070ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000012_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0002_m_000012_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1678ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000006_1: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0002_m_000006_1 decomp: 217023144 len: 217023148 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0002_m_000006_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 65056ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":49594; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 8 time(s); maxRetries=45
  [DataStreamer for file /out/out5/_temporary/1/_temporary/attempt_1445087491445_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out5/_temporary/1/_temporary/attempt_1445087491445_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742903_2102
  [DataStreamer for file /out/out5/_temporary/1/_temporary/attempt_1445087491445_0002_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   mapred.Task: Task:attempt_1445087491445_0002_r_000000_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] mapred.Task: Communication exception: java.net.ConnectException: Call From MSRA-SA-39/172.22.149.145 to minint-fnanli5.fareast.corp.microsoft.com:49594 failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at net.NetUtils.wrapException(NetUtils.java:731)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at net.NetUtils.connect(NetUtils.java:494)
	at ipc.Client$Connection.setupConnection(Client.java:607)
	at ipc.Client$Connection.setupIOstreams(Client.java:705)
	at ipc.Client$Connection.access$2800(Client.java:368)
	at ipc.Client.getConnection(Client.java:1521)
	at ipc.Client.call(Client.java:1438)
	... 5 more

   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:49594. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
",MachineDown,1978
369,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@37fade2f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@3b9af1f7
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0020&reduce=0&map=attempt_1445062781478_0020_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0020_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0020_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 14293ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0020&reduce=0&map=attempt_1445062781478_0020_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0020_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0020&reduce=0&map=attempt_1445062781478_0020_m_000005_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 2 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0020_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 21770ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0020&reduce=0&map=attempt_1445062781478_0020_m_000008_0,attempt_1445062781478_0020_m_000006_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0020_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0020_m_000005_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 17048ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 5 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0020&reduce=0&map=attempt_1445062781478_0020_m_000001_0,attempt_1445062781478_0020_m_000004_0,attempt_1445062781478_0020_m_000002_0,attempt_1445062781478_0020_m_000000_0,attempt_1445062781478_0020_m_000003_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0020_m_000008_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0020_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0020_m_000001_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0020_m_000006_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 38545ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0020_m_000004_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0020_m_000002_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0020_m_000000_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0020_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0020_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0020_m_000003_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 73102ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445062781478_0020_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0020_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0020_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/task_1445062781478_0020_r_000000
   mapred.Task: Task 'attempt_1445062781478_0020_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,867
376,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@7253580c)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@15b78021
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
381,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0018, Ident: (mapreduce.security.token.JobTokenIdentifier@6ace4625)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0018
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@9a99335
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:939524096+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48252774; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306072(69224288); length = 8908325/6553600
   mapred.MapTask: (EQUATOR) 57321526 kvi 14330376(57321504)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57321526 kv 14330376(57321504) kvi 12128560(48514240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57321526; bufend = 695920; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330376(57321504); kvend = 5416856(21667424); length = 8913521/6553600
   mapred.MapTask: (EQUATOR) 9764656 kvi 2441160(9764640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9764656 kv 2441160(9764640) kvi 236684(946736)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9764656; bufend = 58004947; bufvoid = 104857600
   mapred.MapTask: kvstart = 2441160(9764640); kvend = 19744112(78976448); length = 8911449/6553600
   mapred.MapTask: (EQUATOR) 67073683 kvi 16768416(67073664)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67073683 kv 16768416(67073664) kvi 14561752(58247008)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67073683; bufend = 10426966; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768416(67073664); kvend = 7849620(31398480); length = 8918797/6553600
   mapred.MapTask: (EQUATOR) 19495718 kvi 4873924(19495696)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19495718 kv 4873924(19495696) kvi 2677448(10709792)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19495718; bufend = 67755457; bufvoid = 104857600
   mapred.MapTask: kvstart = 4873924(19495696); kvend = 22181748(88726992); length = 8906577/6553600
   mapred.MapTask: (EQUATOR) 76824209 kvi 19206048(76824192)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76824209 kv 19206048(76824192) kvi 16996444(67985776)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76824209; bufend = 20191510; bufvoid = 104857600
   mapred.MapTask: kvstart = 19206048(76824192); kvend = 10290756(41163024); length = 8915293/6553600
   mapred.MapTask: (EQUATOR) 29260262 kvi 7315060(29260240)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29260262 kv 7315060(29260240) kvi 5114312(20457248)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29260262; bufend = 77519736; bufvoid = 104857600
   mapred.MapTask: kvstart = 7315060(29260240); kvend = 24622816(98491264); length = 8906645/6553600
   mapred.MapTask: (EQUATOR) 86588488 kvi 21647116(86588464)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.9""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":53419; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86588488 kv 21647116(86588464) kvi 19453336(77813344)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 35 time(s); maxRetries=45
",MachineDown,958
391,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@b1d4dc0)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2eedd32f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b904247
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000002_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000003_0'
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000004_0'
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000008_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000009_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000012_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000002_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000004_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000009_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1000: Got 10 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000000_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000003_2 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000000_1: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000003_2: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000000_1 decomp: 227948846 len: 227948850 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000003_2 decomp: 216980068 len: 216980072 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0004_m_000000_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5387ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 6 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000001_0,attempt_1445087491445_0004_m_000006_1,attempt_1445087491445_0004_m_000007_1,attempt_1445087491445_0004_m_000008_1,attempt_1445087491445_0004_m_000010_0,attempt_1445087491445_0004_m_000011_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0004_m_000003_2
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5951ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000012_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0004_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000006_1: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000006_1 decomp: 217023144 len: 217023148 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0004_m_000012_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 4478ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0004_m_000006_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0004_m_000007_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000008_1: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000008_1 decomp: 216989049 len: 216989053 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0004_m_000008_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0004_m_000010_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0004_m_000011_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 18068ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000004_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000004_1000: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000004_1000 decomp: 216992138 len: 216992142 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0004_m_000004_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3921ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000009_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000009_1000: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000009_1000 decomp: 216983391 len: 216983395 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0004_m_000009_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2863ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000002_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000002_1000: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000002_1000 decomp: 216986711 len: 216986715 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0004_m_000002_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 6859ms
 ERROR [fetcher#3] mapreduce.task.reduce.Fetcher: Connection retry failed with 4 attempts in 180 seconds
 WARN [fetcher#3] mapreduce.task.reduce.Fetcher: Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 map outputs
java.net.ConnectException: Connection timed out: connect
	at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at mapreduce.task.reduce.Fetcher.connect(Fetcher.java:689)
	at mapreduce.task.reduce.Fetcher.setupConnectionsWithRetry(Fetcher.java:386)
	at mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:292)
	at mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Reporting fetch failure for attempt_1445087491445_0004_m_000005_0 to jobtracker.
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 180042ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000005_0'
",MachineDown,1130
395,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:671088640+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48241717; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17303312(69213248); length = 8911085/6553600
   mapred.MapTask: (EQUATOR) 57310469 kvi 14327612(57310448)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57310469 kv 14327612(57310448) kvi 12124056(48496224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57310469; bufend = 677160; bufvoid = 104857600
   mapred.MapTask: kvstart = 14327612(57310448); kvend = 5412172(21648688); length = 8915441/6553600
   mapred.MapTask: (EQUATOR) 9745912 kvi 2436472(9745888)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9745912 kv 2436472(9745888) kvi 243380(973520)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9745912; bufend = 58001423; bufvoid = 104857600
   mapred.MapTask: kvstart = 2436472(9745888); kvend = 19743236(78972944); length = 8907637/6553600
   mapred.MapTask: (EQUATOR) 67070175 kvi 16767536(67070144)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67070175 kv 16767536(67070144) kvi 14572400(58289600)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67070175; bufend = 10445315; bufvoid = 104857600
   mapred.MapTask: kvstart = 16767536(67070144); kvend = 7854204(31416816); length = 8913333/6553600
   mapred.MapTask: (EQUATOR) 19514051 kvi 4878508(19514032)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19514051 kv 4878508(19514032) kvi 2676468(10705872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19514051; bufend = 67765795; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878508(19514032); kvend = 22184324(88737296); length = 8908585/6553600
   mapred.MapTask: (EQUATOR) 76834531 kvi 19208628(76834512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76834531 kv 19208628(76834512) kvi 17011572(68046288)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76834531; bufend = 20216843; bufvoid = 104857600
   mapred.MapTask: kvstart = 19208628(76834512); kvend = 10297092(41188368); length = 8911537/6553600
   mapred.MapTask: (EQUATOR) 29285595 kvi 7321392(29285568)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285595 kv 7321392(29285568) kvi 5123212(20492848)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285595; bufend = 77486083; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321392(29285568); kvend = 24614400(98457600); length = 8921393/6553600
   mapred.MapTask: (EQUATOR) 86554835 kvi 21638704(86554816)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86554835 kv 21638704(86554816) kvi 19443444(77773776)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
413,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0005, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0005
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2945fc29
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@283561b9
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000008_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1: Got 6 new map-outputs
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000000_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000009_1 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000009_1: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445094324383_0005_m_000009_1 decomp: 172334804 len: 172334808 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000007_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0005_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_1: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0005_m_000009_1
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 148132ms
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0005_m_000000_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 213954ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000005_0,attempt_1445094324383_0005_m_000003_0,attempt_1445094324383_0005_m_000001_0,attempt_1445094324383_0005_m_000004_0,attempt_1445094324383_0005_m_000002_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0005_m_000005_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0005_m_000007_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 498845ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 2 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000008_0,attempt_1445094324383_0005_m_000006_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0005_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0005_m_000003_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0005_m_000001_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0005_m_000004_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0005_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0005_m_000002_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 643623ms
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0005_m_000008_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445094324383_0005_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0005_m_000006_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 502237ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
",MachineDown,800
419,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@7c31963c)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4279112f
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:402653184+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48271024; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17310640(69242560); length = 8903757/6553600
   mapred.MapTask: (EQUATOR) 57339776 kvi 14334940(57339760)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57339776 kv 14334940(57339760) kvi 12140764(48563056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57339776; bufend = 743078; bufvoid = 104857600
   mapred.MapTask: kvstart = 14334940(57339760); kvend = 5428644(21714576); length = 8906297/6553600
   mapred.MapTask: (EQUATOR) 9811814 kvi 2452948(9811792)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9811814 kv 2452948(9811792) kvi 244148(976592)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9811814; bufend = 58036090; bufvoid = 104857600
   mapred.MapTask: kvstart = 2452948(9811792); kvend = 19751904(79007616); length = 8915445/6553600
   mapred.MapTask: (EQUATOR) 67104842 kvi 16776204(67104816)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67104842 kv 16776204(67104816) kvi 14566280(58265120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67104842; bufend = 10444035; bufvoid = 104857600
   mapred.MapTask: kvstart = 16776204(67104816); kvend = 7853884(31415536); length = 8922321/6553600
   mapred.MapTask: (EQUATOR) 19512771 kvi 4878188(19512752)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19512771 kv 4878188(19512752) kvi 2672040(10688160)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19512771; bufend = 67736095; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878188(19512752); kvend = 22176904(88707616); length = 8915685/6553600
   mapred.MapTask: (EQUATOR) 76804847 kvi 19201204(76804816)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76804847 kv 19201204(76804816) kvi 17013340(68053360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76804847; bufend = 20212744; bufvoid = 104857600
   mapred.MapTask: kvstart = 19201204(76804816); kvend = 10296064(41184256); length = 8905141/6553600
   mapred.MapTask: (EQUATOR) 29281496 kvi 7320368(29281472)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29281496 kv 7320368(29281472) kvi 5113468(20453872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29281496; bufend = 77550660; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320368(29281472); kvend = 24630548(98522192); length = 8904221/6553600
   mapred.MapTask: (EQUATOR) 86619412 kvi 21654848(86619392)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86619412 kv 21654848(86619392) kvi 19451324(77805296)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
421,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@231a7808)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2c93d898
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@fa12c68
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 6 new map-outputs
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000004_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000004_0: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000004_0 decomp: 216992138 len: 216992142 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0001_m_000004_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 6214ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000007_0,attempt_1445087491445_0001_m_000003_0,attempt_1445087491445_0001_m_000002_0,attempt_1445087491445_0001_m_000001_0,attempt_1445087491445_0001_m_000006_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000007_0: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000007_0 decomp: 216987422 len: 216987426 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0001_m_000007_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000003_0: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000003_0 decomp: 216980068 len: 216980072 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0001_m_000003_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000002_0: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000002_0 decomp: 216986711 len: 216986715 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0001_m_000002_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0001_m_000001_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000006_0: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000006_0 decomp: 217023144 len: 217023148 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0001_m_000006_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 14577ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000000_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000000_1: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000000_1 decomp: 227948846 len: 227948850 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0001_m_000000_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 3738ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000012_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0001_m_000012_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2853ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000008_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000008_1: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000008_1 decomp: 216989049 len: 216989053 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0001_m_000008_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2970ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000010_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000010_1: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000010_1 decomp: 216998520 len: 216998524 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0001_m_000010_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 3333ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000005_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000005_1: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000005_1 decomp: 216996859 len: 216996863 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0001_m_000005_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 3748ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000009_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000009_0: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000009_0 decomp: 216983391 len: 216983395 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0001_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0001_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 136300ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0001&reduce=0&map=attempt_1445087491445_0001_m_000011_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0001_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0001_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0001_m_000011_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 44417ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445087491445_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445087491445_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742898_2097
  [DataStreamer for file /out/out3/_temporary/1/_temporary/attempt_1445087491445_0001_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445087491445_0001_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0001_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out3/_temporary/1/task_1445087491445_0001_r_000000
   mapred.Task: Task 'attempt_1445087491445_0001_r_000000_0' done.
",MachineDown,1141
422,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0006, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0006
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_0: Got 6 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445087491445_0006_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2012ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000003_0,attempt_1445087491445_0006_m_000007_0,attempt_1445087491445_0006_m_000002_0,attempt_1445087491445_0006_m_000006_0,attempt_1445087491445_0006_m_000004_0,attempt_1445087491445_0006_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445087491445_0006_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_0: Got 2 new map-outputs
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0006_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445087491445_0006_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 1735ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445087491445_0006_m_000007_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445087491445_0006_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445087491445_0006_m_000006_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445087491445_0006_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000001_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445087491445_0006_m_000005_0
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0006_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 19842ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445087491445_0006_m_000001_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 25305ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445087491445_0006_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 33221ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
 WARN [ResponseProcessor for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244
java.io.IOException: Bad response ERROR for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244 from datanode 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:898)
 WARN [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445087491445_0006_r_000000_0/part-r-00000 block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244] hdfs.DFSClient: Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244 in pipeline 172.22.149.145:50010, 10.86.169.121:50010: bad datanode 10.86.169.121:50010
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":55219; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 5 time(s); maxRetries=45
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445087491445_0006_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445087491445_0006_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743047_2263
  [DataStreamer for file /out/out2/_temporary/1/_temporary/attempt_1445087491445_0006_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 16 time(s); maxRetries=45
   mapred.Task: Task:attempt_1445087491445_0006_r_000000_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55219. Already tried 31 time(s); maxRetries=45
",MachineDown,1381
428,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@2ae02324)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3fee7137
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@748443c9
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 2 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0002_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 3 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0002_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0002_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 49702ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000005_0,attempt_1445094324383_0002_m_000008_0,attempt_1445094324383_0002_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0002_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0002_m_000000_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 81154ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0002&reduce=0&map=attempt_1445094324383_0002_m_000004_0,attempt_1445094324383_0002_m_000003_0,attempt_1445094324383_0002_m_000006_0,attempt_1445094324383_0002_m_000002_0,attempt_1445094324383_0002_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0002_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0002_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0002_m_000004_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0002_m_000008_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0002_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0002_m_000003_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0002_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 279654ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0002_m_000006_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0002_m_000002_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0002_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0002_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0002_m_000007_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 365338ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
",MachineDown,761
442,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48249276; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600
   mapred.MapTask: (EQUATOR) 57318028 kvi 14329500(57318000)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318028; bufend = 686843; bufvoid = 104857600
   mapred.MapTask: kvstart = 14329500(57318000); kvend = 5414592(21658368); length = 8914909/6553600
   mapred.MapTask: (EQUATOR) 9755595 kvi 2438892(9755568)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9755595 kv 2438892(9755568) kvi 240952(963808)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9755595; bufend = 58006021; bufvoid = 104857600
   mapred.MapTask: kvstart = 2438892(9755568); kvend = 19744380(78977520); length = 8908913/6553600
   mapred.MapTask: (EQUATOR) 67074757 kvi 16768684(67074736)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67074757 kv 16768684(67074736) kvi 14558340(58233360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67074757; bufend = 10447270; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768684(67074736); kvend = 7854692(31418768); length = 8913993/6553600
   mapred.MapTask: (EQUATOR) 19516006 kvi 4878996(19515984)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19516006 kv 4878996(19515984) kvi 2677056(10708224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19516006; bufend = 67756598; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878996(19515984); kvend = 22182024(88728096); length = 8911373/6553600
   mapred.MapTask: (EQUATOR) 76825334 kvi 19206328(76825312)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76825334 kv 19206328(76825312) kvi 17012764(68051056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76825334; bufend = 20217188; bufvoid = 104857598
   mapred.MapTask: kvstart = 19206328(76825312); kvend = 10297172(41188688); length = 8909157/6553600
   mapred.MapTask: (EQUATOR) 29285924 kvi 7321476(29285904)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285924 kv 7321476(29285904) kvi 5114320(20457280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285924; bufend = 77503154; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321476(29285904); kvend = 24618672(98474688); length = 8917205/6553600
   mapred.MapTask: (EQUATOR) 86571906 kvi 21642972(86571888)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86571906 kv 21642972(86571888) kvi 19445800(77783200)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86571906; bufend = 20324632; bufvoid = 104857600
   mapred.MapTask: kvstart = 21642972(86571888); kvend = 14513800(58055200); length = 7129173/6553600
   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288688442 bytes
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
   mapred.Task: Task:attempt_1445062781478_0013_m_000002_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 35 time(s); maxRetries=45
",MachineDown,1023
443,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:402653184+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48271024; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17310640(69242560); length = 8903757/6553600
   mapred.MapTask: (EQUATOR) 57339776 kvi 14334940(57339760)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57339776 kv 14334940(57339760) kvi 12140764(48563056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57339776; bufend = 743078; bufvoid = 104857600
   mapred.MapTask: kvstart = 14334940(57339760); kvend = 5428644(21714576); length = 8906297/6553600
   mapred.MapTask: (EQUATOR) 9811814 kvi 2452948(9811792)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9811814 kv 2452948(9811792) kvi 244148(976592)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9811814; bufend = 58036090; bufvoid = 104857600
   mapred.MapTask: kvstart = 2452948(9811792); kvend = 19751904(79007616); length = 8915445/6553600
   mapred.MapTask: (EQUATOR) 67104842 kvi 16776204(67104816)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67104842 kv 16776204(67104816) kvi 14566280(58265120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67104842; bufend = 10444035; bufvoid = 104857600
   mapred.MapTask: kvstart = 16776204(67104816); kvend = 7853884(31415536); length = 8922321/6553600
   mapred.MapTask: (EQUATOR) 19512771 kvi 4878188(19512752)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19512771 kv 4878188(19512752) kvi 2672040(10688160)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19512771; bufend = 67736095; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878188(19512752); kvend = 22176904(88707616); length = 8915685/6553600
   mapred.MapTask: (EQUATOR) 76804847 kvi 19201204(76804816)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76804847 kv 19201204(76804816) kvi 17013340(68053360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76804847; bufend = 20212744; bufvoid = 104857600
   mapred.MapTask: kvstart = 19201204(76804816); kvend = 10296064(41184256); length = 8905141/6553600
   mapred.MapTask: (EQUATOR) 29281496 kvi 7320368(29281472)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29281496 kv 7320368(29281472) kvi 5113468(20453872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29281496; bufend = 77550660; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320368(29281472); kvend = 24630548(98522192); length = 8904221/6553600
   mapred.MapTask: (EQUATOR) 86619412 kvi 21654848(86619392)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86619412 kv 21654848(86619392) kvi 19451324(77805296)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86619412; bufend = 20122746; bufvoid = 104857600
   mapred.MapTask: kvstart = 21654848(86619392); kvend = 14553008(58212032); length = 7101841/6553600
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288694349 bytes
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
   mapred.Task: Task:attempt_1445062781478_0013_m_000003_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 34 time(s); maxRetries=45
",MachineDown,1028
444,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@7521491b)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@5181d903
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:1207959552+48562176
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48193401; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17291232(69164928); length = 8923165/6553600
   mapred.MapTask: (EQUATOR) 57262153 kvi 14315532(57262128)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57262153 kv 14315532(57262128) kvi 12264080(49056320)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57262153; bufend = 658166; bufvoid = 104857600
   mapred.MapTask: kvstart = 14315532(57262128); kvend = 5407424(21629696); length = 8908109/6553600
   mapred.MapTask: (EQUATOR) 9726918 kvi 2431724(9726896)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9726918 kv 2431724(9726896) kvi 228516(914064)
 WARN  mapred.Task: Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task.statusUpdate(Task.java:1063)
	at mapred.MapTask.runNewMapper(MapTask.java:787)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
",MachineDown,741
450,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0008, Ident: (mapreduce.security.token.JobTokenIdentifier@7f219df4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0008
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@f810fd9
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@181586a5
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 5 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445087491445_0008_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 6810ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000002_0,attempt_1445087491445_0008_m_000000_0,attempt_1445087491445_0008_m_000004_0,attempt_1445087491445_0008_m_000006_0,attempt_1445087491445_0008_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445087491445_0008_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445087491445_0008_m_000000_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445087491445_0008_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000005_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445087491445_0008_m_000005_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 2356ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445087491445_0008_m_000006_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445087491445_0008_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 14890ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445087491445_0008_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2374ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445087491445_0008_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1798ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445087491445_0008_m_000001_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 9318ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
 WARN [ResponseProcessor for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246
java.io.IOException: Bad response ERROR for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246 from datanode 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:898)
 WARN [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0008_r_000000_0/part-r-00000 block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246] hdfs.DFSClient: Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246 in pipeline 10.190.173.170:50010, 10.86.169.121:50010: bad datanode 10.86.169.121:50010
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":55226; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 0 time(s); maxRetries=45
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0008_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0008_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743037_2253
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0008_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 16 time(s); maxRetries=45
   mapred.Task: Task:attempt_1445087491445_0008_r_000000_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:55226. Already tried 32 time(s); maxRetries=45
",MachineDown,1435
456,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0017, Ident: (mapreduce.security.token.JobTokenIdentifier@3d05ffdb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0017
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4ae51802
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@38fe9315
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000009_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0017_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 71395ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000000_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0017_m_000000_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1255ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000006_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0017_m_000006_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1188ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000003_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000003_1: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000003_1 decomp: 60515787 len: 60515791 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000007_0 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445062781478_0017_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0017_m_000007_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 742ms
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0017_m_000003_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 13032ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000001_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0017_m_000001_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 14806ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0017&reduce=0&map=attempt_1445062781478_0017_m_000005_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0017_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0017_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0017_m_000005_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 10708ms
",MachineDown,685
463,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@490ef5a5)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@20b2cd5f
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:671088640+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48241717; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17303312(69213248); length = 8911085/6553600
   mapred.MapTask: (EQUATOR) 57310469 kvi 14327612(57310448)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57310469 kv 14327612(57310448) kvi 12124056(48496224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57310469; bufend = 677160; bufvoid = 104857600
   mapred.MapTask: kvstart = 14327612(57310448); kvend = 5412172(21648688); length = 8915441/6553600
   mapred.MapTask: (EQUATOR) 9745912 kvi 2436472(9745888)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9745912 kv 2436472(9745888) kvi 243380(973520)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9745912; bufend = 58001423; bufvoid = 104857600
   mapred.MapTask: kvstart = 2436472(9745888); kvend = 19743236(78972944); length = 8907637/6553600
   mapred.MapTask: (EQUATOR) 67070175 kvi 16767536(67070144)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-75DGDAM1/10.86.165.66""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67070175 kv 16767536(67070144) kvi 14572400(58289600)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67070175; bufend = 10445315; bufvoid = 104857600
   mapred.MapTask: kvstart = 16767536(67070144); kvend = 7854204(31416816); length = 8913333/6553600
   mapred.MapTask: (EQUATOR) 19514051 kvi 4878508(19514032)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19514051 kv 4878508(19514032) kvi 2676468(10705872)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19514051; bufend = 67765795; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878508(19514032); kvend = 22184324(88737296); length = 8908585/6553600
   mapred.MapTask: (EQUATOR) 76834531 kvi 19208628(76834512)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76834531 kv 19208628(76834512) kvi 17011572(68046288)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76834531; bufend = 20216843; bufvoid = 104857600
   mapred.MapTask: kvstart = 19208628(76834512); kvend = 10297092(41188368); length = 8911537/6553600
   mapred.MapTask: (EQUATOR) 29285595 kvi 7321392(29285568)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285595 kv 7321392(29285568) kvi 5123212(20492848)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285595; bufend = 77486083; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321392(29285568); kvend = 24614400(98457600); length = 8921393/6553600
   mapred.MapTask: (EQUATOR) 86554835 kvi 21638704(86554816)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86554835 kv 21638704(86554816) kvi 19443444(77773776)
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 34 time(s); maxRetries=45
",MachineDown,945
467,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0019, Ident: (mapreduce.security.token.JobTokenIdentifier@7521491b)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0019
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@b2e6de0
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@539bd2fe
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0019&reduce=0&map=attempt_1445182159119_0019_m_000009_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000009_1000: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000009_1000 decomp: 56695786 len: 56695790 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0019_m_000009_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 18199ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0019&reduce=0&map=attempt_1445182159119_0019_m_000006_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000006_1000: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000006_1000 decomp: 60515100 len: 60515104 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 2 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 2 new map-outputs
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0019&reduce=0&map=attempt_1445182159119_0019_m_000001_1000 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000001_1000: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0019_m_000001_1000 decomp: 60515836 len: 60515840 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0019_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0019_m_000006_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 37289ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0019&reduce=0&map=attempt_1445182159119_0019_m_000008_1000,attempt_1445182159119_0019_m_000003_1000,attempt_1445182159119_0019_m_000005_1000,attempt_1445182159119_0019_m_000004_1000,attempt_1445182159119_0019_m_000007_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000008_1000: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000008_1000 decomp: 60516677 len: 60516681 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0019_m_000001_1000
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 34427ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0019&reduce=0&map=attempt_1445182159119_0019_m_000000_1000,attempt_1445182159119_0019_m_000002_1000 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0019_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0019_m_000000_1000
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000002_1000: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445182159119_0019_m_000002_1000 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0019_m_000008_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000003_1000: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000003_1000 decomp: 60515787 len: 60515791 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0019_m_000002_1000
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 62601ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0019_m_000003_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000005_1000: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000005_1000 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0019_m_000005_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000004_1000: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000004_1000 decomp: 60513765 len: 60513769 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0019_m_000004_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0019_m_000007_1000: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0019_m_000007_1000 decomp: 60517368 len: 60517372 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0019_m_000007_1000
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 121505ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0019_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0019_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0019_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/task_1445182159119_0019_r_000000
   mapred.Task: Task 'attempt_1445182159119_0019_r_000000_1000' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,855
482,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0018, Ident: (mapreduce.security.token.JobTokenIdentifier@3d05ffdb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0018
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@428d7ac3
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":58950; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 4 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 6 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 9 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 12 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 14 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58950. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
",MachineDown,969
486,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86640743; bufend = 20832247; bufvoid = 104857600
   mapred.MapTask: kvstart = 21660180(86640720); kvend = 14457636(57830544); length = 7202545/6553600
   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288922501 bytes
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.Task: Task:attempt_1445062781478_0013_m_000000_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 35 time(s); maxRetries=45
",MachineDown,1023
487,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:134217728+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48254386; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306472(69225888); length = 8907925/6553600
   mapred.MapTask: (EQUATOR) 57323122 kvi 14330776(57323104)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57323122 kv 14330776(57323104) kvi 12127800(48511200)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57323122; bufend = 699496; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330776(57323104); kvend = 5417752(21671008); length = 8913025/6553600
   mapred.MapTask: (EQUATOR) 9768248 kvi 2442056(9768224)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9768248 kv 2442056(9768224) kvi 241544(966176)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9768248; bufend = 57981481; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442056(9768224); kvend = 19738252(78953008); length = 8918205/6553600
   mapred.MapTask: (EQUATOR) 67050233 kvi 16762552(67050208)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67050233 kv 16762552(67050208) kvi 14554320(58217280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67050233; bufend = 10441503; bufvoid = 104857600
   mapred.MapTask: kvstart = 16762552(67050208); kvend = 7853256(31413024); length = 8909297/6553600
   mapred.MapTask: (EQUATOR) 19510255 kvi 4877556(19510224)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19510255 kv 4877556(19510224) kvi 2674180(10696720)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19510255; bufend = 67733940; bufvoid = 104857600
   mapred.MapTask: kvstart = 4877556(19510224); kvend = 22176360(88705440); length = 8915597/6553600
   mapred.MapTask: (EQUATOR) 76802676 kvi 19200664(76802656)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76802676 kv 19200664(76802656) kvi 17001428(68005712)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76802676; bufend = 20189504; bufvoid = 104857600
   mapred.MapTask: kvstart = 19200664(76802656); kvend = 10290256(41161024); length = 8910409/6553600
   mapred.MapTask: (EQUATOR) 29258256 kvi 7314560(29258240)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29258256 kv 7314560(29258240) kvi 5109580(20438320)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29258256; bufend = 77515779; bufvoid = 104857600
   mapred.MapTask: kvstart = 7314560(29258240); kvend = 24621820(98487280); length = 8907141/6553600
   mapred.MapTask: (EQUATOR) 86584515 kvi 21646124(86584496)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86584515 kv 21646124(86584496) kvi 19440748(77762992)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86584515; bufend = 20318567; bufvoid = 104857600
   mapred.MapTask: kvstart = 21646124(86584496); kvend = 14513012(58052048); length = 7133113/6553600
   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288817234 bytes
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-39/172.22.149.145""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy8.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.Task: Task:attempt_1445062781478_0013_m_000001_0 is done. And is in the process of committing
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 35 time(s); maxRetries=45
",MachineDown,1023
495,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0014, Ident: (mapreduce.security.token.JobTokenIdentifier@666adef3)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0014
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@5a6e2b66
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@28c26936
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0014_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0014_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1183ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000006_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0014_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0014_m_000006_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1474ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0014_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000002_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0014_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1057ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0014_m_000002_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1002ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 3 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000004_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0014_m_000004_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1003ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000003_0,attempt_1445062781478_0014_m_000005_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0014_m_000003_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0014_m_000005_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 3319ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000008_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0014_m_000008_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 805ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000000_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000000_1: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000000_1 decomp: 60515385 len: 60515389 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0014_m_000000_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 683ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0014_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0014&reduce=0&map=attempt_1445062781478_0014_m_000001_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0014_m_000001_1: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0014_m_000001_1 decomp: 60515836 len: 60515840 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0014_m_000001_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1872ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445062781478_0014_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0014_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0014_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out4/_temporary/1/task_1445062781478_0014_r_000000
   mapred.Task: Task 'attempt_1445062781478_0014_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,995
501,"   mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445182159119_0018_000001
   mapreduce.v2.app.MRAppMaster: Executing with tokens:
   mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 18 cluster_timestamp: 1445182159119 } attemptId: 1 } keyId: 1694045684)
   mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.
   mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null
   mapreduce.v2.app.MRAppMaster: OutputCommitter is mapreduce.lib.output.FileOutputCommitter
   yarn.event.AsyncDispatcher: Registering class mapreduce.jobhistory.EventType for class mapreduce.jobhistory.JobHistoryEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobEventType for class mapreduce.v2.app.MRAppMaster$JobEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskEventType for class mapreduce.v2.app.MRAppMaster$TaskEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskAttemptEventType for class mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.commit.CommitterEventType for class mapreduce.v2.app.commit.CommitterEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.speculate.Speculator$EventType for class mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.rm.ContainerAllocator$EventType for class mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.launcher.ContainerLauncher$EventType for class mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobFinishEvent$Type for class mapreduce.v2.app.MRAppMaster$JobFinishEventHandler
   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started
   mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1445182159119_0018 to jobTokenSecretManager
   mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1445182159119_0018 because: not enabled; too many maps; too much input;
   mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1445182159119_0018 = 1256521728. Number of splits = 10
   mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1445182159119_0018 = 1
   mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0018Job Transitioned from NEW to INITED
   mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1445182159119_0018.
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 58929] ipc.Server: Starting Socket Reader #1 for port 58929
   yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol mapreduce.v2.api.MRClientProtocolPB to the server
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 58929] ipc.Server: IPC Server listener on 58929: starting
   mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58929
   org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
   http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined
   http.HttpServer2: Added global filter 'safety' (class=http.HttpServer2$QuotingInputFilter)
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context static
   http.HttpServer2: adding path spec: /mapreduce/*
   http.HttpServer2: adding path spec: /ws/*
   http.HttpServer2: Jetty bound to port 58943
   org.mortbay.log: jetty-6.1.26
   org.mortbay.log: Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\Jetty_0_0_0_0_58943_mapreduce____.4f6srg\webapp
   org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:58943
   yarn.webapp.WebApps: Web app /mapreduce started at 58943
   yarn.webapp.WebApps: Registered webapp guice modules
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1445182159119_0018
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 58950] ipc.Server: Starting Socket Reader #1 for port 58950
  [IPC Server listener on 58950] ipc.Server: IPC Server listener on 58950: starting
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
   mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true
   mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3
   mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33
   yarn.client.RMProxy: Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030
   mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:8192, vCores:32>
   mapreduce.v2.app.rm.RMContainerAllocator: queue: default
   mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
   yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0018Job Transitioned from INITED to SETUP
  [CommitterEvent Processor #0] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0018Job Transitioned from SETUP to RUNNING
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000001 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000002 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000003 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000004 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000005 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000006 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000007 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000008 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000009 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_r_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000001_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000002_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000003_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000004_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000005_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000006_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000007_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000008_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000009_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_r_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1>
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:1024, vCores:1>
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1445182159119_0018, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0018/job_1445182159119_0018_1.jhist
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:10 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0018: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:7168, vCores:-20> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:7168, vCores:-20>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:6144, vCores:-21>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:4096, vCores:-23>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:4096, vCores:-23>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0018_01_000002 to attempt_1445182159119_0018_m_000000_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:9 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:1
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0018/job.jar
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0018/job.xml
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0018_01_000002 taskAttempt attempt_1445182159119_0018_m_000000_0
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0018_m_000000_0
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0018: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:2048, vCores:-25> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0018_m_000000_0 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0018_m_000000_0] using containerId: [container_1445182159119_0018_01_000002 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:59190]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0018_m_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000000 Task Transitioned from SCHEDULED to RUNNING
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0018_01_000003 to attempt_1445182159119_0018_m_000001_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:8 ScheduledReds:0 AssignedMaps:2 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:0 RackLocal:2
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0018_01_000003 taskAttempt attempt_1445182159119_0018_m_000001_0
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0018_m_000001_0
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-75DGDAM1.fareast.corp.microsoft.com:58081
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0018_m_000001_0 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0018_m_000001_0] using containerId: [container_1445182159119_0018_01_000003 on NM: [MININT-75DGDAM1.fareast.corp.microsoft.com:58081]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0018_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0018_m_000001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0018_m_000001 Task Transitioned from SCHEDULED to RUNNING
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0018: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [Socket Reader #1 for port 58950] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0018 (auth:SIMPLE)
  [IPC Server handler 26 on 58950] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0018_m_000003 asked for a task
  [IPC Server handler 26 on 58950] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0018_m_000003 given task: attempt_1445182159119_0018_m_000001_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [Socket Reader #1 for port 58950] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0018 (auth:SIMPLE)
  [IPC Server handler 26 on 58950] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0018_m_000002 asked for a task
  [IPC Server handler 26 on 58950] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0018_m_000002 given task: attempt_1445182159119_0018_m_000000_0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 4 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.038753834
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.05536222
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.065459065
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 5 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.071322955
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 1 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.08565241
  [IPC Server handler 4 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.008793094
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 27 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.093142174
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.010095719
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 15 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 5 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.011723794
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 26 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 1 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.015305901
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 5 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 15 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 27 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.11594006
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.022795858
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.12050078
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 12 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.03680106
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 3 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.12929446
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 12 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.13124645
  [IPC Server handler 5 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.047528923
  [IPC Server handler 21 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.13580811
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 29 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.14036706
  [IPC Server handler 28 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.14818434
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 28 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.15795429
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [Socket Reader #1 for port 58950] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0018 (auth:SIMPLE)
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 25 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.16544476
  [IPC Server handler 10 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.07197516
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 25 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.17303155
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 0 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.088910766
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 16 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.191499
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 2 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.19211523
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.19211523
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 1 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.19211523
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 15 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.19211523
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 17 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 3 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.20517796
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 27 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.23221055
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 1 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 21 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.24588884
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 15 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 10 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.2611966
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 10 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000000_0 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 14 on 58950] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0018_m_000001_0 is : 0.26967376
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
",MachineDown,3789
503,"   mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445182159119_0019_000002
   mapreduce.v2.app.MRAppMaster: Executing with tokens:
   mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 19 cluster_timestamp: 1445182159119 } attemptId: 2 } keyId: 1694045684)
   mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.
   mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null
   mapreduce.v2.app.MRAppMaster: OutputCommitter is mapreduce.lib.output.FileOutputCommitter
   yarn.event.AsyncDispatcher: Registering class mapreduce.jobhistory.EventType for class mapreduce.jobhistory.JobHistoryEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobEventType for class mapreduce.v2.app.MRAppMaster$JobEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskEventType for class mapreduce.v2.app.MRAppMaster$TaskEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskAttemptEventType for class mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.commit.CommitterEventType for class mapreduce.v2.app.commit.CommitterEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.speculate.Speculator$EventType for class mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.rm.ContainerAllocator$EventType for class mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.launcher.ContainerLauncher$EventType for class mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled
   mapreduce.v2.app.MRAppMaster: Recovery is enabled. Will try to recover from previous life on best effort basis.
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.app.MRAppMaster: Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job_1445182159119_0019_1.jhist
 WARN  mapreduce.v2.app.MRAppMaster: Unable to parse prior job history, aborting recovery
java.io.IOException: Incompatible event log version: null
	at mapreduce.jobhistory.EventReader.<init>(EventReader.java:71)
	at mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)
	at mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1183)
	at mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1152)
	at mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1016)
	at service.AbstractService.start(AbstractService.java:193)
	at mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1500)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1496)
	at mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1429)
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.app.MRAppMaster: Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job_1445182159119_0019_1.jhist
 WARN  mapreduce.v2.app.MRAppMaster: Could not parse the old history file. Will not have old AMinfos 
java.io.IOException: Incompatible event log version: null
	at mapreduce.jobhistory.EventReader.<init>(EventReader.java:71)
	at mapreduce.v2.app.MRAppMaster.readJustAMInfos(MRAppMaster.java:1229)
	at mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1156)
	at mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1016)
	at service.AbstractService.start(AbstractService.java:193)
	at mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1500)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1496)
	at mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1429)
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobFinishEvent$Type for class mapreduce.v2.app.MRAppMaster$JobFinishEventHandler
   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started
   mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1445182159119_0019 to jobTokenSecretManager
   mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1445182159119_0019 because: not enabled; too many maps; too much input;
   mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1445182159119_0019 = 1256521728. Number of splits = 10
   mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1445182159119_0019 = 1
   mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0019Job Transitioned from NEW to INITED
   mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1445182159119_0019.
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 56172] ipc.Server: Starting Socket Reader #1 for port 56172
   yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol mapreduce.v2.api.MRClientProtocolPB to the server
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 56172] ipc.Server: IPC Server listener on 56172: starting
   mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at MSRA-SA-39.fareast.corp.microsoft.com/172.22.149.145:56172
   org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
   http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined
   http.HttpServer2: Added global filter 'safety' (class=http.HttpServer2$QuotingInputFilter)
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context static
   http.HttpServer2: adding path spec: /mapreduce/*
   http.HttpServer2: adding path spec: /ws/*
   http.HttpServer2: Jetty bound to port 56179
   org.mortbay.log: jetty-6.1.26
   org.mortbay.log: Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\2\Jetty_0_0_0_0_56179_mapreduce____.s1e00p\webapp
   org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:56179
   yarn.webapp.WebApps: Web app /mapreduce started at 56179
   yarn.webapp.WebApps: Registered webapp guice modules
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1445182159119_0019
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 56183] ipc.Server: Starting Socket Reader #1 for port 56183
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 56183] ipc.Server: IPC Server listener on 56183: starting
   mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true
   mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3
   mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33
   yarn.client.RMProxy: Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030
   mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:8192, vCores:32>
   mapreduce.v2.app.rm.RMContainerAllocator: queue: default
   mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
   yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0019Job Transitioned from INITED to SETUP
  [CommitterEvent Processor #0] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0019Job Transitioned from SETUP to RUNNING
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000001 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000002 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000003 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000004 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000005 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000006 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000007 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000008 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000009 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_r_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000001_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000003_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000005_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000006_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000007_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000008_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000009_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1>
  [Thread-50] mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:1024, vCores:1>
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1445182159119_0019, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job_1445182159119_0019_2.jhist
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:10 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:14336, vCores:-13> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:14336, vCores:-13>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 10
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000002 to attempt_1445182159119_0019_m_000000_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000003 to attempt_1445182159119_0019_m_000001_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000004 to attempt_1445182159119_0019_m_000002_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000005 to attempt_1445182159119_0019_m_000003_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000006 to attempt_1445182159119_0019_m_000004_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000007 to attempt_1445182159119_0019_m_000005_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000008 to attempt_1445182159119_0019_m_000006_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000009 to attempt_1445182159119_0019_m_000007_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000010 to attempt_1445182159119_0019_m_000008_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000011 to attempt_1445182159119_0019_m_000009_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:3072, vCores:-24>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job.jar
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job.xml
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000001_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000003_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000005_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000006_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000007_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000008_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000009_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000002 taskAttempt attempt_1445182159119_0019_m_000000_1000
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000003 taskAttempt attempt_1445182159119_0019_m_000001_1000
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000004 taskAttempt attempt_1445182159119_0019_m_000002_1000
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000005 taskAttempt attempt_1445182159119_0019_m_000003_1000
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000006 taskAttempt attempt_1445182159119_0019_m_000004_1000
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000007 taskAttempt attempt_1445182159119_0019_m_000005_1000
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000008 taskAttempt attempt_1445182159119_0019_m_000006_1000
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000009 taskAttempt attempt_1445182159119_0019_m_000007_1000
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000010 taskAttempt attempt_1445182159119_0019_m_000008_1000
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000011 taskAttempt attempt_1445182159119_0019_m_000009_1000
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000008_1000
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000001_1000
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000002_1000
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000003_1000
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000000_1000
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000006_1000
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000005_1000
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000009_1000
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000007_1000
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000004_1000
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #4] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #9] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000001_1000 : 13562
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000002_1000 : 13562
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000000_1000 : 13562
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000003_1000 : 13562
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000005_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000003_1000] using containerId: [container_1445182159119_0019_02_000005 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000008_1000 : 13562
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000007_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000003_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000001_1000] using containerId: [container_1445182159119_0019_02_000003 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:28345]
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000006_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000001_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000000_1000] using containerId: [container_1445182159119_0019_02_000002 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:28345]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000009_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000002_1000] using containerId: [container_1445182159119_0019_02_000004 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:28345]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000005_1000] using containerId: [container_1445182159119_0019_02_000007 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000005_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000008_1000] using containerId: [container_1445182159119_0019_02_000010 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000008_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000007_1000] using containerId: [container_1445182159119_0019_02_000009 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000007_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000003
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000003 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000006_1000] using containerId: [container_1445182159119_0019_02_000008 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000006_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000001 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000000 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000009_1000] using containerId: [container_1445182159119_0019_02_000011 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000004_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000009_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000002
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000002 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000005
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000005 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000008
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000008 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000007
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000007 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000006
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000006 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000009
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000009 Task Transitioned from SCHEDULED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000004_1000] using containerId: [container_1445182159119_0019_02_000006 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:10769]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000004
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000004 Task Transitioned from SCHEDULED to RUNNING
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:3072, vCores:-24> knownNMs=4
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000004 asked for a task
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000004 given task: attempt_1445182159119_0019_m_000002_1000
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000002 asked for a task
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000002 given task: attempt_1445182159119_0019_m_000000_1000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000003 asked for a task
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000003 given task: attempt_1445182159119_0019_m_000001_1000
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000005 asked for a task
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000005 given task: attempt_1445182159119_0019_m_000003_1000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000010 asked for a task
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000010 given task: attempt_1445182159119_0019_m_000008_1000
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000009 asked for a task
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000009 given task: attempt_1445182159119_0019_m_000007_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:4096, vCores:-23>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000006 asked for a task
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000006 given task: attempt_1445182159119_0019_m_000004_1000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000011 asked for a task
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000011 given task: attempt_1445182159119_0019_m_000009_1000
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000007 asked for a task
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000007 given task: attempt_1445182159119_0019_m_000005_1000
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000008 asked for a task
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000008 given task: attempt_1445182159119_0019_m_000006_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:3072, vCores:-24>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.10394478
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.10635664
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.1066108
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.295472
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.10680563
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.10685723
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.106964506
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.106881365
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.106493875
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.10681946
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.10660437
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.10635664
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.1066108
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.295472
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.10680563
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.10685723
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.106964506
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.106881365
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.106493875
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.10681946
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.10660437
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.10635664
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.1066108
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.295472
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.10680563
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.10685723
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.106964506
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.106881365
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.106493875
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.10681946
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.10660437
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.108781725
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.10635664
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.29965714
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.13710664
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.11593631
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.14160493
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.13726272
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.14980216
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.1417813
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.124968685
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.18389814
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.17777202
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.524377
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.19242907
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.1902514
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.19266446
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.19209063
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.19258286
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.19255035
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.19212553
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.19211523
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.19158794
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.5323719
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.19242907
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.19247705
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.19266446
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.19209063
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.19258286
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.19255035
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.19212553
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.19211523
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.19158794
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.5323719
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.19242907
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.19247705
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.19266446
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.19209063
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.19258286
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.19255035
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.19212553
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.21517316
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.1948611
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.5323719
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.19242907
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.20372403
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.22723113
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.23740155
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.24723715
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.24502383
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.19212553
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.27776006
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.5323719
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.26858845
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.667
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.2781602
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.27813601
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.2783809
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.27765483
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.27825075
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.27811313
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.24445581
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.27776006
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.27696857
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.667
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.2781602
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.27813601
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.2783809
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.27765483
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.27825075
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.27811313
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.27772525
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.27776006
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.27696857
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.67487156
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.2781602
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.27813601
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.2783809
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.27765483
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.27825075
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.27811313
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.27772525
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.3243128
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.30291036
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.29593822
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.76102096
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.35281968
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.36206445
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.3554983
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.360263
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.3585813
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.27772525
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.36319977
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.3624012
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.84687364
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.36388028
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.36390656
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.36404583
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.36323506
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.3638923
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.3637686
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.27772525
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.36319977
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.3624012
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 0.9459533
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.36388028
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.36390656
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.36404583
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.36323506
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.3638923
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.3637686
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.33056664
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.3624012
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.36319977
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000009_1000 is : 1.0
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000009_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000009_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000011 taskAttempt attempt_1445182159119_0019_m_000009_1000
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000009_1000
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000009_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000009_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000009 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:2048, vCores:-25>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold reached. Scheduling reduces.
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0019_m_000002
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445182159119_0019_m_000002
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from NEW to UNASSIGNED
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.36388028
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=5 release= 0 newContainers=0 finishedContainers=1 resourcelimit=<memory:2048, vCores:-25> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000011
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:9 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:10 RackLocal:0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000009_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.36390656
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.39102918
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.41400805
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.421291
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.42233998
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000012 to attempt_1445182159119_0019_r_000000_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:9 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:0 HostLocal:10 RackLocal:0
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000012 taskAttempt attempt_1445182159119_0019_r_000000_1000
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_r_000000_1000
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_r_000000_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_r_000000_1000] using containerId: [container_1445182159119_0019_02_000012 on NM: [04DN8IQ.fareast.corp.microsoft.com:64260]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_r_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_r_000000 Task Transitioned from SCHEDULED to RUNNING
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.38659573
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.36317363
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.41347364
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=1 release= 0 newContainers=1 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000013 to attempt_1445182159119_0019_m_000002_1001
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:12 ContRel:0 HostLocal:10 RackLocal:1
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000013 taskAttempt attempt_1445182159119_0019_m_000002_1001
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000002_1001
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.42679533
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.44950968
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.44980705
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000002_1001 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000002_1001] using containerId: [container_1445182159119_0019_02_000013 on NM: [04DN8IQ.fareast.corp.microsoft.com:64260]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000002
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.4486067
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.44964966
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.44950172
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.44789755
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.448704
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.36317363
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.44968578
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.44950968
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.44980705
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.4486067
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.44964966
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.44950172
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.44789755
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.36317363
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.448704
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.44968578
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.44950968
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.44980705
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.4486067
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.44950172
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.44964966
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.44789755
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.40339205
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.448704
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.44968578
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.4800264
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.5043758
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.5343203
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.53521925
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.5352825
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.46541902
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.50285363
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.44859612
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0019_m_000004
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445182159119_0019_m_000004
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from NEW to UNASSIGNED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:12 ContRel:0 HostLocal:10 RackLocal:1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.52721065
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.5352021
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.53543663
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.5343203
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.5352825
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.53521925
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.53341997
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.53425497
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.44859612
  [Thread-54] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.165.66:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [Thread-54] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073744061_3286
  [Thread-54] hdfs.DFSClient: Excluding datanode 10.86.165.66:50010
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.5352028
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.5352021
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.53543663
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.5343203
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.5352825
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.53521925
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.53341997
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.44859612
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.53425497
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.5352028
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.5352021
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.53543663
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.61072564
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.5965759
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.6126403
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_r_000012 asked for a task
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_r_000012 given task: attempt_1445182159119_0019_r_000000_1000
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000013 asked for a task
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000013 given task: attempt_1445182159119_0019_m_000002_1001
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.53341997
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.44859612
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.53425497
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.5628982
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.6209487
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.6210422
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.6199081
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.620844
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.6207798
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.5522524
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.5095292
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6017114
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000014 to attempt_1445182159119_0019_m_000004_1001
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:13 ContRel:0 HostLocal:10 RackLocal:2
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000014 taskAttempt attempt_1445182159119_0019_m_000004_1001
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000004_1001
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000004_1001 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000004_1001] using containerId: [container_1445182159119_0019_02_000014 on NM: [04DN8IQ.fareast.corp.microsoft.com:64260]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000004
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.6208445
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.6209487
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.6210422
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.6199081
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.620844
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.6207798
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.61898744
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6197233
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.5342037
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 0 maxEvents 10000
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.6208445
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.6209487
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.6210422
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.6199081
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.667
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.6452596
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.66537446
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.66537446
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.6452596
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.61898744
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.5342037
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6197233
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.6208445
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.64745617
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.66521084
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.66521084
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.667
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.667
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.667
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.64745617
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.0
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.61898744
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.5342037
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6446815
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6446815
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0019_m_000000
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445182159119_0019_m_000000
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from NEW to UNASSIGNED
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.6208445
  [Socket Reader #1 for port 56183] SecurityLogger.ipc.Server: Auth successful for job_1445182159119_0019 (auth:SIMPLE)
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445182159119_0019_m_000014 asked for a task
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445182159119_0019_m_000014 given task: attempt_1445182159119_0019_m_000004_1001
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:13 ContRel:0 HostLocal:10 RackLocal:2
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.667
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.667
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.667
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.667
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.667
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.667
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.667
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.667
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.5342037
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.667
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.016380234
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.0
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.667
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.667
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.667
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.69132215
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.6779769
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.6923886
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.667
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.59646577
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.667
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.043639295
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.667
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.0
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.6675142
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.67558074
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.73348606
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.7183496
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.7354205
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.667
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6681547
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6196791
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.059922766
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.6797831
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.7001761
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.7115852
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.0
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.757781
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.74151844
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.7599814
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.66714954
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6196791
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.6943572
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.7217471
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.7411928
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.7552234
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.7846617
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.7674506
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.7885486
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.072357535
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.6955358
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6196791
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.7262846
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.043156985
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.746995
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.7660728
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.778935
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.81903166
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.8004005
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.8223591
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10175717
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.72579026
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6196791
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.75944906
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.06676305
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.773568
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.7936655
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.8084495
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.84707767
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.8336158
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.85712224
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.7568521
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6465807
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.7921466
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.6465807
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.075558126
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.80830956
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.8276353
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.84453225
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.8675425
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.8529882
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.8780673
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.7878011
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.667
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.82477516
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.845921
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.86344665
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.88332087
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.89398754
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.87938565
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.0945463
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.90642077
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.81768197
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.667
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.8560278
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.88768053
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.9031517
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.92447376
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.90766734
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.92363554
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.9357531
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.10680563
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.8475377
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.667
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.8873018
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.9238391
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.9391674
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.9595412
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.9509443
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.9337547
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.96288574
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.880285
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.678726
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.92113507
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.10680563
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445182159119_0019_02_000015 to attempt_1445182159119_0019_m_000000_1001
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0019_02_000015 taskAttempt attempt_1445182159119_0019_m_000000_1001
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445182159119_0019_m_000000_1001
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.9549086
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.97119546
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 0.99328744
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.96900606
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.9504901
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445182159119_0019: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 0.98149294
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445182159119_0019_m_000000_1001 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445182159119_0019_m_000000_1001] using containerId: [container_1445182159119_0019_02_000015 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:59190]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445182159119_0019_m_000000
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.91209686
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.7076595
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.95401
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000006_1000 is : 1.0
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000006_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000006_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000008 taskAttempt attempt_1445182159119_0019_m_000006_1000
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000006_1000
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000006_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000006_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000006 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 2
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.10680563
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 1 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:2 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000008_1000 is : 1.0
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000008_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000008_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000010 taskAttempt attempt_1445182159119_0019_m_000008_1000
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000008_1000
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000008_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000008_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000008 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 3
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.97014797
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 0.9870312
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 2 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:12 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000008
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:11 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000006_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 0.99410284
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.97556126
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.9446994
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.74664754
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 0.98745716
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 3 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000010
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:3 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000008_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000003_1000 is : 1.0
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000003_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000003_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000005 taskAttempt attempt_1445182159119_0019_m_000003_1000
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000003_1000
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000003_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000003_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000003 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 4
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000005_1000 is : 1.0
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000005_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000005_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000007 taskAttempt attempt_1445182159119_0019_m_000005_1000
  [ContainerLauncher #9] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000005_1000
  [ContainerLauncher #9] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000005_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000005_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000005 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 5
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 3 maxEvents 10000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:1 CompletedMaps:5 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000005
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:9 AssignedReds:1 CompletedMaps:5 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000003_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1001 is : 0.10680563
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000001_1000 is : 1.0
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000001_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000001_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000003 taskAttempt attempt_1445182159119_0019_m_000001_1000
  [ContainerLauncher #4] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000001_1000
  [ContainerLauncher #4] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000001_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000001_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000001 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 6
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 0.9997146
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000004_1000 is : 1.0
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000004_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000006 taskAttempt attempt_1445182159119_0019_m_000004_1000
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000004_1000
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000004_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445182159119_0019_m_000004_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000004 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 7
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000014 taskAttempt attempt_1445182159119_0019_m_000004_1001
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000004_1001
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:9 AssignedReds:1 CompletedMaps:7 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000007
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:8 AssignedReds:1 CompletedMaps:7 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000005_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 0.9940365
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 5 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 0.97553384
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.7869564
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0019_m_000004
  [DefaultSpeculator background processing] mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000003
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000006
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000001_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:6 AssignedReds:1 CompletedMaps:7 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000004_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000007_1000 is : 1.0
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000007_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000007_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000009 taskAttempt attempt_1445182159119_0019_m_000007_1000
  [ContainerLauncher #2] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000007_1000
  [ContainerLauncher #2] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:10769
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000007_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000007_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000007 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 8
  [CommitterEvent Processor #1] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #1] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/_temporary/attempt_1445182159119_0019_m_000004_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000004_1001 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 7 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:6 AssignedReds:1 CompletedMaps:8 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [Socket Reader #1 for port 56183] ipc.Server: Socket Reader #1 for port 56183: readAndProcess from client 10.86.164.138 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 8 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000000_1000 is : 1.0
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000002 taskAttempt attempt_1445182159119_0019_m_000000_1000
  [ContainerLauncher #3] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000000_1000
  [ContainerLauncher #3] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445182159119_0019_m_000000_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000000 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 9
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000015 taskAttempt attempt_1445182159119_0019_m_000000_1001
  [ContainerLauncher #5] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000000_1001
  [ContainerLauncher #5] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:6 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000009
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:5 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000007_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 8 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.8194983
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #2] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #2] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/_temporary/attempt_1445182159119_0019_m_000000_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000000_1001 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000002
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000014
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000000_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:3 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000004_1001: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.87097865
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000015
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:2 AssignedReds:1 CompletedMaps:9 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000000_1001: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.10660437
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.9258646
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1001 is : 0.11431367
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 0.97752357
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_m_000002_1000 is : 1.0
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_m_000002_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000004 taskAttempt attempt_1445182159119_0019_m_000002_1000
  [ContainerLauncher #8] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000002_1000
  [ContainerLauncher #8] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_m_000002_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Issuing kill to other attempt attempt_1445182159119_0019_m_000002_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_m_000002 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 10
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from RUNNING to KILL_CONTAINER_CLEANUP
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000013 taskAttempt attempt_1445182159119_0019_m_000002_1001
  [ContainerLauncher #7] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_m_000002_1001
  [ContainerLauncher #7] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
  [CommitterEvent Processor #3] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
 WARN [CommitterEvent Processor #3] mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://msra-sa-41:9000/pageout/out4/_temporary/2/_temporary/attempt_1445182159119_0019_m_000002_1001
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_m_000002_1001 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:2 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [Socket Reader #1 for port 56183] ipc.Server: Socket Reader #1 for port 56183: readAndProcess from client 10.86.164.138 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]
java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at ipc.Server.channelRead(Server.java:2593)
	at ipc.Server.access$2800(Server.java:135)
	at ipc.Server$Connection.readAndProcess(Server.java:1471)
	at ipc.Server$Listener.doRead(Server.java:762)
	at ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
	at ipc.Server$Listener$Reader.run(Server.java:607)
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 9 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000004
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445182159119_0019_02_000013
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000002_1000: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445182159119_0019_m_000002_1001: Container killed by the ApplicationMaster.
Container killed on request. Exit code is 137
Container exited with a non-zero exit code 137

  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.033333335
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.10000001
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.13333334
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.13333334
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.16666667
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.20000002
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.23333333
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.23333333
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.23333333
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.23333333
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 28 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 20 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.26666668
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445182159119_0019_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.3
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6666667
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6682114
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6732092
  [IPC Server handler 27 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6779541
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.68303674
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6880477
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.6930393
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.69816583
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7030546
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7079118
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.713512
  [IPC Server handler 13 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.71924984
  [IPC Server handler 22 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.72358006
  [IPC Server handler 19 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.72893924
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7340402
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7397736
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7446486
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.74911946
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.75449336
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7598647
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.764735
  [IPC Server handler 18 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.77018344
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7754492
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7811562
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7861187
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7917964
  [IPC Server handler 5 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.7973094
  [IPC Server handler 7 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.80304694
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8089151
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8143567
  [IPC Server handler 17 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8194424
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.82508785
  [IPC Server handler 8 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.83074033
  [IPC Server handler 0 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.83621037
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8418144
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8480226
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.85317284
  [IPC Server handler 11 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8605091
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.86837167
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.87561053
  [IPC Server handler 29 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.88292134
  [IPC Server handler 10 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.88976276
  [IPC Server handler 24 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.8967041
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.90360665
  [IPC Server handler 9 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.91110235
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.91808313
  [IPC Server handler 6 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9251123
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9317502
  [IPC Server handler 26 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.93958765
  [IPC Server handler 16 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9468132
  [IPC Server handler 21 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.95411193
  [IPC Server handler 1 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9612774
  [IPC Server handler 23 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9681872
  [IPC Server handler 25 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9750513
  [IPC Server handler 14 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.9822508
  [IPC Server handler 15 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.98923755
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 0.99615645
  [IPC Server handler 12 on 56183] mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1445182159119_0019_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: attempt_1445182159119_0019_r_000000_1000 given a go for committing the task output.
  [IPC Server handler 4 on 56183] mapred.TaskAttemptListenerImpl: Commit go/no-go request from attempt_1445182159119_0019_r_000000_1000
  [IPC Server handler 4 on 56183] mapreduce.v2.app.job.impl.TaskImpl: Result of canCommit for attempt_1445182159119_0019_r_000000_1000:true
  [IPC Server handler 3 on 56183] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445182159119_0019_r_000000_1000 is : 1.0
  [IPC Server handler 2 on 56183] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445182159119_0019_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445182159119_0019_02_000012 taskAttempt attempt_1445182159119_0019_r_000000_1000
  [ContainerLauncher #6] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445182159119_0019_r_000000_1000
  [ContainerLauncher #6] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:64260
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445182159119_0019_r_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445182159119_0019_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445182159119_0019_r_000000 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 11
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0019Job Transitioned from RUNNING to COMMITTING
  [CommitterEvent Processor #4] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_COMMIT
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Calling handler for JobFinishedEvent 
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445182159119_0019Job Transitioned from COMMITTING to SUCCEEDED
  [Thread-109] mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry
  [Thread-109] mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true
  [Thread-109] mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true
  [Thread-109] mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true
  [Thread-109] mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true
  [Thread-109] mapreduce.v2.app.MRAppMaster: Calling stop for all the services
  [Thread-109] mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job_1445182159119_0019_2.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019-1445247552638-msrabi-pagerank-1445248878331-10-1-SUCCEEDED-default-1445248388426.jhist_tmp
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019-1445247552638-msrabi-pagerank-1445248878331-10-1-SUCCEEDED-default-1445248388426.jhist_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019/job_1445182159119_0019_2_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019.summary_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019.summary
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019_conf.xml_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019_conf.xml
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019-1445247552638-msrabi-pagerank-1445248878331-10-1-SUCCEEDED-default-1445248388426.jhist_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445182159119_0019-1445247552638-msrabi-pagerank-1445248878331-10-1-SUCCEEDED-default-1445248388426.jhist
  [Thread-109] mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop()
  [Thread-109] mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to 
  [Thread-109] mapreduce.v2.app.rm.RMContainerAllocator: History url is http://MSRA-SA-39.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445182159119_0019
  [Thread-109] mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.
  [Thread-109] mapreduce.v2.app.rm.RMContainerAllocator: Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:14 ContRel:0 HostLocal:10 RackLocal:3
  [Thread-109] mapreduce.v2.app.MRAppMaster: Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445182159119_0019
  [Thread-109] ipc.Server: Stopping server on 56183
  [IPC Server listener on 56183] ipc.Server: Stopping IPC Server listener on 56183
  [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
  [TaskHeartbeatHandler PingChecker] mapreduce.v2.app.TaskHeartbeatHandler: TaskHeartbeatHandler thread interrupted
",MachineDown,16501
512,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0015, Ident: (mapreduce.security.token.JobTokenIdentifier@1623b78d)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0015
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@db57326
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7862f56
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000000_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0015_m_000000_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 721ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000003_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0015_m_000003_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 595ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000004_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0015_m_000004_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 661ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000006_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0015_m_000006_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 578ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000009_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0015_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 8434ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000007_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0015_m_000007_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 572ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000008_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0015_m_000008_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 563ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000002_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0015_m_000002_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 12092ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000001_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0015_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0015_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0015&reduce=0&map=attempt_1445062781478_0015_m_000005_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0015_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0015_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0015_m_000005_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 565ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0015_m_000001_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 10963ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445062781478_0015_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0015_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0015_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/task_1445062781478_0015_r_000000
   mapred.Task: Task 'attempt_1445062781478_0015_r_000000_0' done.
",MachineDown,1038
516,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0019, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0019
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4d90b2ca
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:134217728+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48254386; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306472(69225888); length = 8907925/6553600
   mapred.MapTask: (EQUATOR) 57323122 kvi 14330776(57323104)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":58957; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57323122 kv 14330776(57323104) kvi 12127800(48511200)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57323122; bufend = 699496; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330776(57323104); kvend = 5417752(21671008); length = 8913025/6553600
   mapred.MapTask: (EQUATOR) 9768248 kvi 2442056(9768224)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 1 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9768248 kv 2442056(9768224) kvi 241544(966176)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 2 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9768248; bufend = 57981481; bufvoid = 104857600
   mapred.MapTask: kvstart = 2442056(9768224); kvend = 19738252(78953008); length = 8918205/6553600
   mapred.MapTask: (EQUATOR) 67050233 kvi 16762552(67050208)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 4 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67050233 kv 16762552(67050208) kvi 14554320(58217280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67050233; bufend = 10441503; bufvoid = 104857600
   mapred.MapTask: kvstart = 16762552(67050208); kvend = 7853256(31413024); length = 8909297/6553600
   mapred.MapTask: (EQUATOR) 19510255 kvi 4877556(19510224)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 6 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19510255 kv 4877556(19510224) kvi 2674180(10696720)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 7 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19510255; bufend = 67733940; bufvoid = 104857600
   mapred.MapTask: kvstart = 4877556(19510224); kvend = 22176360(88705440); length = 8915597/6553600
   mapred.MapTask: (EQUATOR) 76802676 kvi 19200664(76802656)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 9 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76802676 kv 19200664(76802656) kvi 17001428(68005712)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76802676; bufend = 20189504; bufvoid = 104857600
   mapred.MapTask: kvstart = 19200664(76802656); kvend = 10290256(41161024); length = 8910409/6553600
   mapred.MapTask: (EQUATOR) 29258256 kvi 7314560(29258240)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 12 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29258256 kv 7314560(29258240) kvi 5109580(20438320)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29258256; bufend = 77515779; bufvoid = 104857600
   mapred.MapTask: kvstart = 7314560(29258240); kvend = 24621820(98487280); length = 8907141/6553600
   mapred.MapTask: (EQUATOR) 86584515 kvi 21646124(86584496)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 14 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86584515 kv 21646124(86584496) kvi 19440748(77762992)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
",MachineDown,969
525,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0019, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0019
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@7cedd9bc
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:0+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48233939; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17301360(69205440); length = 8913037/6553600
   mapred.MapTask: (EQUATOR) 57302675 kvi 14325664(57302656)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57302675 kv 14325664(57302656) kvi 12126896(48507584)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57302675; bufend = 709216; bufvoid = 104857600
   mapred.MapTask: kvstart = 14325664(57302656); kvend = 5420188(21680752); length = 8905477/6553600
   mapred.MapTask: (EQUATOR) 9777968 kvi 2444488(9777952)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":58957; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9777968 kv 2444488(9777952) kvi 250856(1003424)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 1 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9777968; bufend = 58030301; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444488(9777952); kvend = 19750456(79001824); length = 8908433/6553600
   mapred.MapTask: (EQUATOR) 67099053 kvi 16774756(67099024)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 3 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67099053 kv 16774756(67099024) kvi 14578988(58315952)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67099053; bufend = 10501292; bufvoid = 104857600
   mapred.MapTask: kvstart = 16774756(67099024); kvend = 7868200(31472800); length = 8906557/6553600
   mapred.MapTask: (EQUATOR) 19570044 kvi 4892504(19570016)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 5 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19570044 kv 4892504(19570016) kvi 2699328(10797312)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 6 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19570044; bufend = 67823152; bufvoid = 104857600
   mapred.MapTask: kvstart = 4892504(19570016); kvend = 22198672(88794688); length = 8908233/6553600
   mapred.MapTask: (EQUATOR) 76891904 kvi 19222972(76891888)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 8 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76891904 kv 19222972(76891888) kvi 17028244(68112976)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76891904; bufend = 20274616; bufvoid = 104857600
   mapred.MapTask: kvstart = 19222972(76891888); kvend = 10311532(41246128); length = 8911441/6553600
   mapred.MapTask: (EQUATOR) 29343368 kvi 7335836(29343344)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 11 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29343368 kv 7335836(29343344) kvi 5140140(20560560)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29343368; bufend = 77571991; bufvoid = 104857600
   mapred.MapTask: kvstart = 7335836(29343344); kvend = 24635876(98543504); length = 8914361/6553600
   mapred.MapTask: (EQUATOR) 86640743 kvi 21660180(86640720)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 13 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86640743 kv 21660180(86640720) kvi 19461792(77847168)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:58957. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
",MachineDown,969
537,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0012, Ident: (mapreduce.security.token.JobTokenIdentifier@5c46c7b0)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0012
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3c33282e
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@1fcaf332
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0012_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0012_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 3557ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000002_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0012_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000003_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000003_1: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0012_m_000003_1 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0012_m_000002_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 49273ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0012_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0012_m_000003_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 20853ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000008_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0012_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000005_1 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0012_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0012_m_000008_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 23668ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000001_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000001_1: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0012_m_000001_1 decomp: 60515836 len: 60515840 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000007_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0012_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0012_m_000007_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 559ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0012_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000006_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0012_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0012_m_000006_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 619ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0012_m_000005_1
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 55428ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0012&reduce=0&map=attempt_1445062781478_0012_m_000004_1 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0012_m_000004_1: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445062781478_0012_m_000004_1 decomp: 60513765 len: 60513769 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0012_m_000001_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 68950ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0012_m_000000_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 126936ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0012_m_000004_1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 50788ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [Thread-93] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.164.9:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [Thread-93] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742531_1726
  [Thread-93] hdfs.DFSClient: Excluding datanode 10.86.164.9:50010
   mapred.Task: Task:attempt_1445062781478_0012_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0012_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0012_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out2/_temporary/1/task_1445062781478_0012_r_000000
   mapred.Task: Task 'attempt_1445062781478_0012_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,1082
546,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0008, Ident: (mapreduce.security.token.JobTokenIdentifier@78093f60)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0008
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@1e30bbc5
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@35c7d3d3
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0008_r_000000_1000: Got 10 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0008_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0008_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445087491445_0008_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1730ms
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 5 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 5 of 5 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000002_0,attempt_1445087491445_0008_m_000003_0,attempt_1445087491445_0008_m_000004_0,attempt_1445087491445_0008_m_000006_0,attempt_1445087491445_0008_m_000007_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445087491445_0008_m_000001_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2215ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0008&reduce=0&map=attempt_1445087491445_0008_m_000005_0,attempt_1445087491445_0008_m_000008_0,attempt_1445087491445_0008_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0008_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445087491445_0008_m_000002_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445087491445_0008_m_000005_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0008_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445087491445_0008_m_000003_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445087491445_0008_m_000008_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0008_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445087491445_0008_m_000004_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445087491445_0008_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 11722ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445087491445_0008_m_000006_0
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0008_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0008_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445087491445_0008_m_000007_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 18657ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [Thread-24] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [Thread-24] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743071_2287
  [Thread-24] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445087491445_0008_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0008_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0008_r_000000_1000' to hdfs://msra-sa-41:9000/out/out4/_temporary/2/task_1445087491445_0008_r_000000
   mapred.Task: Task 'attempt_1445087491445_0008_r_000000_1000' done.
",MachineDown,748
570,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@c5f5deb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@428d7ac3
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at hdfs.DFSInputStream.read(DFSInputStream.java:655)
	at java.io.DataInputStream.readByte(DataInputStream.java:265)
	at io.WritableUtils.readVLong(WritableUtils.java:308)
	at io.WritableUtils.readVIntInRange(WritableUtils.java:348)
	at io.Text.readString(Text.java:471)
	at io.Text.readString(Text.java:464)
	at mapred.MapTask.getSplitDetails(MapTask.java:358)
	at mapred.MapTask.runNewMapper(MapTask.java:751)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.86.165.66:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information
java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at hdfs.DFSInputStream.read(DFSInputStream.java:655)
	at java.io.DataInputStream.readByte(DataInputStream.java:265)
	at io.WritableUtils.readVLong(WritableUtils.java:308)
	at io.WritableUtils.readVIntInRange(WritableUtils.java:348)
	at io.Text.readString(Text.java:471)
	at io.Text.readString(Text.java:464)
	at mapred.MapTask.getSplitDetails(MapTask.java:358)
	at mapred.MapTask.runNewMapper(MapTask.java:751)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Successfully connected to /10.86.169.121:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073744042_3267
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48249276; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600
   mapred.MapTask: (EQUATOR) 57318028 kvi 14329500(57318000)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318028; bufend = 686843; bufvoid = 104857600
   mapred.MapTask: kvstart = 14329500(57318000); kvend = 5414592(21658368); length = 8914909/6553600
   mapred.MapTask: (EQUATOR) 9755595 kvi 2438892(9755568)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9755595 kv 2438892(9755568) kvi 240952(963808)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9755595; bufend = 58006021; bufvoid = 104857600
   mapred.MapTask: kvstart = 2438892(9755568); kvend = 19744380(78977520); length = 8908913/6553600
   mapred.MapTask: (EQUATOR) 67074757 kvi 16768684(67074736)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67074757 kv 16768684(67074736) kvi 14558340(58233360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67074757; bufend = 10447270; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768684(67074736); kvend = 7854692(31418768); length = 8913993/6553600
   mapred.MapTask: (EQUATOR) 19516006 kvi 4878996(19515984)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19516006 kv 4878996(19515984) kvi 2677056(10708224)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19516006; bufend = 67756598; bufvoid = 104857600
   mapred.MapTask: kvstart = 4878996(19515984); kvend = 22182024(88728096); length = 8911373/6553600
   mapred.MapTask: (EQUATOR) 76825334 kvi 19206328(76825312)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76825334 kv 19206328(76825312) kvi 17012764(68051056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76825334; bufend = 20217188; bufvoid = 104857598
   mapred.MapTask: kvstart = 19206328(76825312); kvend = 10297172(41188688); length = 8909157/6553600
   mapred.MapTask: (EQUATOR) 29285924 kvi 7321476(29285904)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29285924 kv 7321476(29285904) kvi 5114320(20457280)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29285924; bufend = 77503154; bufvoid = 104857600
   mapred.MapTask: kvstart = 7321476(29285904); kvend = 24618672(98474688); length = 8917205/6553600
   mapred.MapTask: (EQUATOR) 86571906 kvi 21642972(86571888)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86571906 kv 21642972(86571888) kvi 19445800(77783200)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86571906; bufend = 20324632; bufvoid = 104857600
   mapred.MapTask: kvstart = 21642972(86571888); kvend = 14513800(58055200); length = 7129173/6553600
   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288688442 bytes
   mapred.Task: Task:attempt_1445182159119_0020_m_000002_0 is done. And is in the process of committing
   mapred.Task: Task 'attempt_1445182159119_0020_m_000002_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...
   metrics2.impl.MetricsSystemImpl: MapTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: MapTask metrics system shutdown complete.
",MachineDown,634
571,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@11628d93)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@294c9ed9
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@333861df
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000006_0,attempt_1445182159119_0020_m_000005_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0020_m_000006_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0020_m_000005_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1850ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0020_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 54720ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000003_0,attempt_1445182159119_0020_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000008_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0020_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000007_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445182159119_0020_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0020_m_000007_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 1315ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0020_m_000008_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 49554ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0020_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0020_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0020_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 246671ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0020&reduce=0&map=attempt_1445182159119_0020_m_000002_0,attempt_1445182159119_0020_m_000004_0,attempt_1445182159119_0020_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0020_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0020_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0020_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0020_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0020_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 96601ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0020_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0020_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0020_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/task_1445182159119_0020_r_000000
   mapred.Task: Task 'attempt_1445182159119_0020_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,890
572,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0020, Ident: (mapreduce.security.token.JobTokenIdentifier@7521491b)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0020
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@3c28dd37
 WARN  hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at hdfs.DFSInputStream.read(DFSInputStream.java:655)
	at java.io.DataInputStream.readByte(DataInputStream.java:265)
	at io.WritableUtils.readVLong(WritableUtils.java:308)
	at io.WritableUtils.readVIntInRange(WritableUtils.java:348)
	at io.Text.readString(Text.java:471)
	at io.Text.readString(Text.java:464)
	at mapred.MapTask.getSplitDetails(MapTask.java:358)
	at mapred.MapTask.runNewMapper(MapTask.java:751)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
 WARN  hdfs.DFSClient: Failed to connect to /10.86.165.66:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information
java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at net.NetUtils.connect(NetUtils.java:530)
	at hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)
	at hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)
	at hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)
	at hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)
	at hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)
	at hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)
	at hdfs.DFSInputStream.read(DFSInputStream.java:847)
	at hdfs.DFSInputStream.read(DFSInputStream.java:655)
	at java.io.DataInputStream.readByte(DataInputStream.java:265)
	at io.WritableUtils.readVLong(WritableUtils.java:308)
	at io.WritableUtils.readVIntInRange(WritableUtils.java:348)
	at io.Text.readString(Text.java:471)
	at io.Text.readString(Text.java:464)
	at mapred.MapTask.getSplitDetails(MapTask.java:358)
	at mapred.MapTask.runNewMapper(MapTask.java:751)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
   hdfs.DFSClient: Successfully connected to /10.86.169.121:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073744042_3267
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:536870912+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48250246; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17305444(69221776); length = 8908953/6553600
   mapred.MapTask: (EQUATOR) 57318998 kvi 14329744(57318976)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57318998 kv 14329744(57318976) kvi 12130124(48520496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57318998; bufend = 707922; bufvoid = 104857599
   mapred.MapTask: kvstart = 14329744(57318976); kvend = 5419856(21679424); length = 8909889/6553600
   mapred.MapTask: (EQUATOR) 9776658 kvi 2444160(9776640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9776658 kv 2444160(9776640) kvi 247856(991424)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9776658; bufend = 57994455; bufvoid = 104857600
   mapred.MapTask: kvstart = 2444160(9776640); kvend = 19741496(78965984); length = 8917065/6553600
   mapred.MapTask: (EQUATOR) 67063207 kvi 16765796(67063184)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67063207 kv 16765796(67063184) kvi 14570840(58283360)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67063207; bufend = 10480387; bufvoid = 104857600
   mapred.MapTask: kvstart = 16765796(67063184); kvend = 7862980(31451920); length = 8902817/6553600
   mapred.MapTask: (EQUATOR) 19549139 kvi 4887280(19549120)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19549139 kv 4887280(19549120) kvi 2679652(10718608)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19549139; bufend = 67751785; bufvoid = 104857600
   mapred.MapTask: kvstart = 4887280(19549120); kvend = 22180828(88723312); length = 8920853/6553600
   mapred.MapTask: (EQUATOR) 76820537 kvi 19205128(76820512)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76820537 kv 19205128(76820512) kvi 16995388(67981552)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76820537; bufend = 20214918; bufvoid = 104857600
   mapred.MapTask: kvstart = 19205128(76820512); kvend = 10296608(41186432); length = 8908521/6553600
   mapred.MapTask: (EQUATOR) 29283670 kvi 7320912(29283648)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29283670 kv 7320912(29283648) kvi 5125060(20500240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29283670; bufend = 77538589; bufvoid = 104857600
   mapred.MapTask: kvstart = 7320912(29283648); kvend = 24627528(98510112); length = 8907785/6553600
   mapred.MapTask: (EQUATOR) 86607341 kvi 21651828(86607312)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86607341 kv 21651828(86607312) kvi 19456628(77826512)
   mapred.MapTask: Starting flush of map output
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 86607341; bufend = 19628334; bufvoid = 104857600
   mapred.MapTask: kvstart = 21651828(86607312); kvend = 14655860(58623440); length = 6995969/6553600
   mapred.MapTask: Finished spill 7
   mapred.Merger: Merging 8 sorted segments
   mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 288330442 bytes
   mapred.Task: Task:attempt_1445182159119_0020_m_000004_0 is done. And is in the process of committing
   mapred.Task: Task 'attempt_1445182159119_0020_m_000004_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...
   metrics2.impl.MetricsSystemImpl: MapTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: MapTask metrics system shutdown complete.
",MachineDown,634
582,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@5d3cb6cf)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@7cedd9bc
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:805306368+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48215795; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17296824(69187296); length = 8917573/6553600
   mapred.MapTask: (EQUATOR) 57284531 kvi 14321128(57284512)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57284531 kv 14321128(57284512) kvi 12112692(48450768)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57284531; bufend = 630553; bufvoid = 104857600
   mapred.MapTask: kvstart = 14321128(57284512); kvend = 5400516(21602064); length = 8920613/6553600
   mapred.MapTask: (EQUATOR) 9699305 kvi 2424820(9699280)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9699305 kv 2424820(9699280) kvi 222764(891056)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9699305; bufend = 57911793; bufvoid = 104857600
   mapred.MapTask: kvstart = 2424820(9699280); kvend = 19720828(78883312); length = 8918393/6553600
   mapred.MapTask: (EQUATOR) 66980545 kvi 16745132(66980528)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 66980545 kv 16745132(66980528) kvi 14546624(58186496)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 66980545; bufend = 10374147; bufvoid = 104857600
   mapred.MapTask: kvstart = 16745132(66980528); kvend = 7836420(31345680); length = 8908713/6553600
   mapred.MapTask: (EQUATOR) 19442899 kvi 4860720(19442880)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19442899 kv 4860720(19442880) kvi 2660780(10643120)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19442899; bufend = 67657921; bufvoid = 104857600
   mapred.MapTask: kvstart = 4860720(19442880); kvend = 22157356(88629424); length = 8917765/6553600
   mapred.MapTask: (EQUATOR) 76726657 kvi 19181660(76726640)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76726657 kv 19181660(76726640) kvi 16980352(67921408)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76726657; bufend = 20115617; bufvoid = 104857600
   mapred.MapTask: kvstart = 19181660(76726640); kvend = 10271780(41087120); length = 8909881/6553600
   mapred.MapTask: (EQUATOR) 29184353 kvi 7296084(29184336)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29184353 kv 7296084(29184336) kvi 5097788(20391152)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29184353; bufend = 77442473; bufvoid = 104857600
   mapred.MapTask: kvstart = 7296084(29184336); kvend = 24603496(98413984); length = 8906989/6553600
   mapred.MapTask: (EQUATOR) 86511225 kvi 21627800(86511200)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86511225 kv 21627800(86511200) kvi 19431636(77726544)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

 WARN  mapred.Task: Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task.statusUpdate(Task.java:1063)
	at mapred.MapTask.runNewMapper(MapTask.java:787)
	at mapred.MapTask.run(MapTask.java:341)
	at mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
",MachineDown,885
585,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0018, Ident: (mapreduce.security.token.JobTokenIdentifier@304cc139)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0018
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@647e1a0a
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@71de9590
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000004_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000004_1000: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000004_1000 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0018_m_000004_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1033ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000009_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000009_1000: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000009_1000 decomp: 56695786 len: 56695790 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0018_m_000009_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 608ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000006_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000006_1000: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000006_1000 decomp: 60515100 len: 60515104 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0018_m_000006_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 627ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000007_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000007_1000: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000007_1000 decomp: 60517368 len: 60517372 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0018_m_000007_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1111ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000008_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000008_1000: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000008_1000 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0018_m_000008_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 710ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000001_1001 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000001_1001: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000001_1001 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0018_m_000001_1001
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 719ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000002_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000002_1000: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000002_1000 decomp: 60514392 len: 60514396 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0018_m_000002_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 18960ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000000_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0018_m_000000_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 16391ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000003_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000003_1000: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000003_1000 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0018_m_000003_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 10162ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0018_r_000000_1000: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0018&reduce=0&map=attempt_1445182159119_0018_m_000005_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0018_m_000005_1000: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0018_m_000005_1000 decomp: 60514806 len: 60514810 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0018_m_000005_1000
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 10477ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0018_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0018_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0018_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out2/_temporary/2/task_1445182159119_0018_r_000000
   mapred.Task: Task 'attempt_1445182159119_0018_r_000000_1000' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,1030
586,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@54e0a229)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6119d61b
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:939524096+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48252774; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306072(69224288); length = 8908325/6553600
   mapred.MapTask: (EQUATOR) 57321526 kvi 14330376(57321504)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57321526 kv 14330376(57321504) kvi 12128560(48514240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57321526; bufend = 695920; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330376(57321504); kvend = 5416856(21667424); length = 8913521/6553600
   mapred.MapTask: (EQUATOR) 9764656 kvi 2441160(9764640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9764656 kv 2441160(9764640) kvi 236684(946736)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9764656; bufend = 58004947; bufvoid = 104857600
   mapred.MapTask: kvstart = 2441160(9764640); kvend = 19744112(78976448); length = 8911449/6553600
   mapred.MapTask: (EQUATOR) 67073683 kvi 16768416(67073664)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67073683 kv 16768416(67073664) kvi 14561752(58247008)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67073683; bufend = 10426966; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768416(67073664); kvend = 7849620(31398480); length = 8918797/6553600
   mapred.MapTask: (EQUATOR) 19495718 kvi 4873924(19495696)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19495718 kv 4873924(19495696) kvi 2677448(10709792)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19495718; bufend = 67755457; bufvoid = 104857600
   mapred.MapTask: kvstart = 4873924(19495696); kvend = 22181748(88726992); length = 8906577/6553600
   mapred.MapTask: (EQUATOR) 76824209 kvi 19206048(76824192)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76824209 kv 19206048(76824192) kvi 16996444(67985776)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76824209; bufend = 20191510; bufvoid = 104857600
   mapred.MapTask: kvstart = 19206048(76824192); kvend = 10290756(41163024); length = 8915293/6553600
   mapred.MapTask: (EQUATOR) 29260262 kvi 7315060(29260240)
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29260262 kv 7315060(29260240) kvi 5114312(20457248)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29260262; bufend = 77519736; bufvoid = 104857600
   mapred.MapTask: kvstart = 7315060(29260240); kvend = 24622816(98491264); length = 8906645/6553600
   mapred.MapTask: (EQUATOR) 86588488 kvi 21647116(86588464)
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86588488 kv 21647116(86588464) kvi 19453336(77813344)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MININT-FNANLI5/10.86.169.121""; destination host is: ""04dn8iq.fareast.corp.microsoft.com"":49470; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 0 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 1 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 2 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 3 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 4 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 5 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 6 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 7 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 8 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 9 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 10 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 11 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 12 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 13 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 14 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 15 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 16 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 17 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 18 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 19 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 20 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 21 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 22 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 23 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 24 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 25 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 26 time(s); maxRetries=45
   ipc.Client: Retrying connect to server: 04dn8iq.fareast.corp.microsoft.com/10.86.164.9:49470. Already tried 27 time(s); maxRetries=45
",MachineDown,798
591,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0013, Ident: (mapreduce.security.token.JobTokenIdentifier@54e0a229)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0013
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@20c50279
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4da4c44b
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0013&reduce=0&map=attempt_1445062781478_0013_m_000009_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000009_1000: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000009_1000 decomp: 56695786 len: 56695790 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0013_m_000009_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 9748ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0013&reduce=0&map=attempt_1445062781478_0013_m_000004_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000004_1000: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000004_1000 decomp: 60513765 len: 60513769 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 2 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 2 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0013_m_000004_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 12355ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 6 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0013&reduce=0&map=attempt_1445062781478_0013_m_000006_1000,attempt_1445062781478_0013_m_000007_1000,attempt_1445062781478_0013_m_000001_1000,attempt_1445062781478_0013_m_000002_1000,attempt_1445062781478_0013_m_000000_1000,attempt_1445062781478_0013_m_000003_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000006_1000: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000006_1000 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0013_m_000006_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000007_1000: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000007_1000 decomp: 60517368 len: 60517372 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445062781478_0013_m_000007_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000001_1000: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000001_1000 decomp: 60515836 len: 60515840 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0013_m_000001_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000002_1000: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000002_1000 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0013_m_000002_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000000_1000: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000000_1000 decomp: 60515385 len: 60515389 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0013_r_000000_1000: Got 1 new map-outputs
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0013&reduce=0&map=attempt_1445062781478_0013_m_000008_1000 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000008_1000: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445062781478_0013_m_000008_1000 decomp: 60516677 len: 60516681 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0013_m_000000_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000003_1000: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000003_1000 decomp: 60515787 len: 60515791 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0013_m_000008_1000
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 15159ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0013_m_000003_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 60331ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0013&reduce=0&map=attempt_1445062781478_0013_m_000005_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0013_m_000005_1000: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0013_m_000005_1000 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0013_m_000005_1000
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 8266ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445062781478_0013_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445062781478_0013_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445062781478_0013_r_000000_1000' to hdfs://msra-sa-41:9000/pageout/out3/_temporary/2/task_1445062781478_0013_r_000000
   mapred.Task: Task 'attempt_1445062781478_0013_r_000000_1000' done.
",MachineDown,839
619,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@7253580c)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@49a29f92
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@7138e57a
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0003_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4184ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000010_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0003_m_000010_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4803ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000000_0: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000000_0 decomp: 227948846 len: 227948850 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0003_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5708ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000003_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000003_0: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000003_0 decomp: 216980068 len: 216980072 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0003_m_000003_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5430ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000004_0,attempt_1445087491445_0003_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000004_0: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000004_0 decomp: 216992138 len: 216992142 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0003_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000002_0: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000002_0 decomp: 216986711 len: 216986715 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0003_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 11577ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000008_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000008_1: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000008_1 decomp: 216989049 len: 216989053 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0003_m_000008_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 4575ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000007_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0003_m_000007_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8521ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000005_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000005_1: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000005_1 decomp: 216996859 len: 216996863 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0003_m_000005_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 5996ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000006_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000006_1: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000006_1 decomp: 217023144 len: 217023148 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0003_m_000006_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8023ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000011_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000011_1: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0003_m_000011_1 decomp: 216988481 len: 216988485 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000009_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000009_0: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0003_m_000009_0 decomp: 216983391 len: 216983395 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0003_m_000011_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 9783ms
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0003_m_000009_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 37843ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0003&reduce=0&map=attempt_1445087491445_0003_m_000012_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0003_m_000012_0: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445087491445_0003_m_000012_0 decomp: 216991205 len: 216991209 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0003_m_000012_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 42832ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742895_2094
  [DataStreamer for file /out/out4/_temporary/1/_temporary/attempt_1445087491445_0003_r_000000_0/part-r-00000] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445087491445_0003_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0003_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0003_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445087491445_0003_r_000000
   mapred.Task: Task 'attempt_1445087491445_0003_r_000000_0' done.
",MachineDown,1282
620,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445182159119_0017, Ident: (mapreduce.security.token.JobTokenIdentifier@56b87a95)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445182159119_0017
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@64abfe29
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@64f8145f
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0017_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0017_r_000000_0: Got 7 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000004_0,attempt_1445182159119_0017_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0017_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445182159119_0017_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 767ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445182159119_0017_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445182159119_0017_m_000004_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445182159119_0017_m_000002_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1242ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000000_0,attempt_1445182159119_0017_m_000003_0,attempt_1445182159119_0017_m_000001_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445182159119_0017_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 539ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445182159119_0017_m_000000_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445182159119_0017_m_000003_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0017_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445182159119_0017_m_000001_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1126ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000006_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000006_0: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000006_0 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445182159119_0017_m_000006_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 359ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445182159119_0017_r_000000_0: Got 2 new map-outputs
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000005_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000005_0: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000005_0 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445182159119_0017_m_000005_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 877ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445182159119_0017&reduce=0&map=attempt_1445182159119_0017_m_000007_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445182159119_0017_m_000007_0: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445182159119_0017_m_000007_0 decomp: 60517368 len: 60517372 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445182159119_0017_m_000007_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 792ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445182159119_0017_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445182159119_0017_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445182159119_0017_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out3/_temporary/1/task_1445182159119_0017_r_000000
   mapred.Task: Task 'attempt_1445182159119_0017_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,865
630,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0010, Ident: (mapreduce.security.token.JobTokenIdentifier@3acd9e86)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0010
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@40c918ef
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@22ce3b8c
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0010_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0010_m_000003_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0010_m_000000_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0010&reduce=0&map=attempt_1445087491445_0010_m_000009_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0010_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0010_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0010&reduce=0&map=attempt_1445087491445_0010_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0010_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0010_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445087491445_0010_m_000009_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 5760ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0010&reduce=0&map=attempt_1445087491445_0010_m_000008_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0010_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0010_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445087491445_0010_m_000008_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 4264ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0010&reduce=0&map=attempt_1445087491445_0010_m_000005_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0010_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0010_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445087491445_0010_m_000005_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2494ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0010&reduce=0&map=attempt_1445087491445_0010_m_000004_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0010_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0010_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0010_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445087491445_0010_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 81348ms
",MachineDown,552
648,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@30db7df3)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@152f92dc
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@1b394c94
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0004_r_000000_1 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0004_m_000003_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0004_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0004_r_000000_1: Got 7 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 4 to fetcher#5
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000004_0,attempt_1445094324383_0004_m_000007_0,attempt_1445094324383_0004_m_000002_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0004_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000001_0,attempt_1445094324383_0004_m_000005_0,attempt_1445094324383_0004_m_000009_0,attempt_1445094324383_0004_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0004_r_000000_1: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0004_r_000000_1: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000000_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445094324383_0004_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0004_r_000000_1: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0004_m_000000_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 3876ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0004_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0004_m_000004_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0004_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0004_m_000007_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000002_1: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445094324383_0004_m_000002_1 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0004_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0004_m_000009_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0004_m_000002_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 520671ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0004_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 616622ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000006_1,attempt_1445094324383_0004_m_000003_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000006_1: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000006_1 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0004_m_000006_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0004_m_000003_1: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0004_m_000003_1 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0004_m_000003_1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 209231ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445094324383_0004_r_000000_1 is done. And is in the process of committing
   mapred.Task: Task attempt_1445094324383_0004_r_000000_1 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445094324383_0004_r_000000_1' to hdfs://msra-sa-41:9000/out/out2/_temporary/1/task_1445094324383_0004_r_000000
   mapred.Task: Task 'attempt_1445094324383_0004_r_000000_1' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,800
669,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000010_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0004_m_000010_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 2938ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0004_m_000001_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1842ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000011_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0004_m_000011_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 33594ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000003_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000008_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000012_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000000_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000000_1: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000000_1 decomp: 227948846 len: 227948850 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0004_m_000000_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1786ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000002_0: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000002_0 decomp: 216986711 len: 216986715 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000007_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0004_m_000007_1
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0004_m_000002_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 31694ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 51067ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 2 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 2 of 2 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000004_0,attempt_1445087491445_0004_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000004_0: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000004_0 decomp: 216992138 len: 216992142 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0004_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000009_0: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000009_0 decomp: 216983391 len: 216983395 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000005_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000005_0: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0004_m_000005_0 decomp: 216996859 len: 216996863 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000006_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000006_1: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000006_1 decomp: 217023144 len: 217023148 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0004_m_000006_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1639ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0004_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 78797ms
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0004_m_000005_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 37927ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000008_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000008_1: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0004_m_000008_1 decomp: 216989049 len: 216989053 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0004_m_000008_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1902ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000012_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0004_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0004_m_000012_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1929ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000003_2 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000003_2: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0004_m_000003_2 decomp: 216980068 len: 216980072 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0004_m_000003_2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1942ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
",MachineDown,1285
672,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 5 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0003_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0003_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1903ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 4 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000008_0,attempt_1445094324383_0003_m_000003_0,attempt_1445094324383_0003_m_000001_0,attempt_1445094324383_0003_m_000005_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0003_m_000008_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0003_m_000003_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0003_m_000001_0
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0003_m_000005_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 8893ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0003_m_000006_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0003_m_000004_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000000_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000000_1: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000000_1 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0003_m_000000_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2327ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000007_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000007_1: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0003_m_000007_1 decomp: 216976206 len: 216976210 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0003_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0003_m_000007_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2840ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0003_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 66143ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000004_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0003_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-FNANLI5.fareast.corp.microsoft.com:13562 to fetcher#4
  [fetcher#4] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0003&reduce=0&map=attempt_1445094324383_0003_m_000006_0 sent hash and received reply
  [fetcher#4] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0003_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#4] mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1445094324383_0003_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#4] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0003_m_000006_0
  [fetcher#4] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 31275ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0003_m_000004_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 41618ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445094324383_0003_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445094324383_0003_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445094324383_0003_r_000000_0' to hdfs://msra-sa-41:9000/out/out3/_temporary/1/task_1445094324383_0003_r_000000
   mapred.Task: Task 'attempt_1445094324383_0003_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,929
685,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0002, Ident: (mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0002
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@6ad3381f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@4fad9bb2
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000003_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0002_m_000006_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0002_m_000006_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_1000: Got 12 new map-outputs
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000001_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000000_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000000_0: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000000_0 decomp: 227948846 len: 227948850 to DISK
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445087491445_0002_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0002_m_000001_0
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0002_m_000000_0
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 2895ms
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 3 to fetcher#3
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#3
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 2902ms
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 7 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 7 of 7 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#3] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000007_1,attempt_1445087491445_0002_m_000010_0,attempt_1445087491445_0002_m_000012_1 sent hash and received reply
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445087491445_0002_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000002_0,attempt_1445087491445_0002_m_000003_1,attempt_1445087491445_0002_m_000004_0,attempt_1445087491445_0002_m_000005_0,attempt_1445087491445_0002_m_000008_0,attempt_1445087491445_0002_m_000009_0,attempt_1445087491445_0002_m_000011_0 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000002_0: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000002_0 decomp: 216986711 len: 216986715 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0002_m_000007_1
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445087491445_0002_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0002_m_000002_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000003_1: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000003_1 decomp: 216980068 len: 216980072 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0002_m_000010_0
  [fetcher#3] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#3] mapreduce.task.reduce.Fetcher: fetcher#3 about to shuffle output of map attempt_1445087491445_0002_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0002_m_000003_1
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000004_0: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000004_0 decomp: 216992138 len: 216992142 to DISK
  [fetcher#3] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0002_m_000012_1
  [fetcher#3] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#3 in 6591ms
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0002_m_000004_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000005_0: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000005_0 decomp: 216996859 len: 216996863 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0002_m_000005_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000008_0: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000008_0 decomp: 216989049 len: 216989053 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0002_m_000008_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000009_0: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000009_0 decomp: 216983391 len: 216983395 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0002_m_000009_0
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0002_m_000011_0
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 17923ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0002_r_000000_1000: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000006_1000 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0002_m_000006_1000: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (32663142)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445087491445_0002_m_000006_1000 decomp: 217023144 len: 217023148 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0002_m_000006_1000
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 1979ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445087491445_0002_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0002_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0002_r_000000_1000' to hdfs://msra-sa-41:9000/out/out5/_temporary/2/task_1445087491445_0002_r_000000
   mapred.Task: Task 'attempt_1445087491445_0002_r_000000_1000' done.
",MachineDown,962
692,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0003, Ident: (mapreduce.security.token.JobTokenIdentifier@2a5f8fa2)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0003
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@261041cc
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@3bd8eac0
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 4 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000002_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000002_0: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000002_0 decomp: 60514392 len: 60514396 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445076437777_0003_m_000002_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1318ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 3 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 3 of 3 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000003_0,attempt_1445076437777_0003_m_000001_0,attempt_1445076437777_0003_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000003_0: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000003_0 decomp: 60515787 len: 60515791 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445076437777_0003_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445076437777_0003_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445076437777_0003_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3361ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000009_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000006_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000006_1: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445076437777_0003_m_000006_1 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445076437777_0003_m_000006_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 855ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000007_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000007_1: Shuffling to disk since 60517368 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445076437777_0003_m_000007_1 decomp: 60517368 len: 60517372 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60517372 bytes from map-output for attempt_1445076437777_0003_m_000007_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1420ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000005_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445076437777_0003_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445076437777_0003_m_000005_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 2012ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445076437777_0003_m_000009_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 108850ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000004_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000004_0: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000004_0 decomp: 60513765 len: 60513769 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445076437777_0003_m_000004_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 11180ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445076437777_0003_r_000000_0: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MININT-75DGDAM1.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MININT-75DGDAM1.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445076437777_0003&reduce=0&map=attempt_1445076437777_0003_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445076437777_0003_m_000008_0: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445076437777_0003_m_000008_0 decomp: 60516677 len: 60516681 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445076437777_0003_m_000008_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MININT-75DGDAM1.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 16337ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 601334942 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 601334842 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
  [Thread-96] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as 10.86.169.121:50010
	at hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)
	at hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1362)
	at hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:589)
  [Thread-96] hdfs.DFSClient: Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742656_1851
  [Thread-96] hdfs.DFSClient: Excluding datanode 10.86.169.121:50010
   mapred.Task: Task:attempt_1445076437777_0003_r_000000_0 is done. And is in the process of committing
   mapred.Task: Task attempt_1445076437777_0003_r_000000_0 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445076437777_0003_r_000000_0' to hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/task_1445076437777_0003_r_000000
   mapred.Task: Task 'attempt_1445076437777_0003_r_000000_0' done.
   metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
",MachineDown,976
708,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445062781478_0018, Ident: (mapreduce.security.token.JobTokenIdentifier@7253580c)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445062781478_0018
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2eedd32f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b904247
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 2 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000000_0: Shuffling to disk since 60515385 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445062781478_0018_m_000000_0 decomp: 60515385 len: 60515389 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 60515389 bytes from map-output for attempt_1445062781478_0018_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 542ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000001_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000001_0: Shuffling to disk since 60515836 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000001_0 decomp: 60515836 len: 60515840 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515840 bytes from map-output for attempt_1445062781478_0018_m_000001_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 402ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000006_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000006_1: Shuffling to disk since 60515100 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000006_1 decomp: 60515100 len: 60515104 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515104 bytes from map-output for attempt_1445062781478_0018_m_000006_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1004ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000003_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000003_1: Shuffling to disk since 60515787 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000003_1 decomp: 60515787 len: 60515791 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60515791 bytes from map-output for attempt_1445062781478_0018_m_000003_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 550ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000009_0: Shuffling to disk since 56695786 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000009_0 decomp: 56695786 len: 56695790 to DISK
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#2
  [fetcher#2] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000004_1 sent hash and received reply
  [fetcher#2] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000004_1: Shuffling to disk since 60513765 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#2] mapreduce.task.reduce.Fetcher: fetcher#2 about to shuffle output of map attempt_1445062781478_0018_m_000004_1 decomp: 60513765 len: 60513769 to DISK
  [fetcher#2] mapreduce.task.reduce.OnDiskMapOutput: Read 60513769 bytes from map-output for attempt_1445062781478_0018_m_000004_1
  [fetcher#2] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#2 in 635ms
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 56695790 bytes from map-output for attempt_1445062781478_0018_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 9171ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000008_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000008_1: Shuffling to disk since 60516677 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000008_1 decomp: 60516677 len: 60516681 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60516681 bytes from map-output for attempt_1445062781478_0018_m_000008_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 666ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000002_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000002_1: Shuffling to disk since 60514392 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000002_1 decomp: 60514392 len: 60514396 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514396 bytes from map-output for attempt_1445062781478_0018_m_000002_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 844ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445062781478_0018_r_000000_0: Got 1 new map-outputs
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445062781478_0018&reduce=0&map=attempt_1445062781478_0018_m_000005_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445062781478_0018_m_000005_1: Shuffling to disk since 60514806 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445062781478_0018_m_000005_1 decomp: 60514806 len: 60514810 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 60514810 bytes from map-output for attempt_1445062781478_0018_m_000005_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 834ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: Exception in getting events
java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":53419; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.getMapCompletionEvents(Unknown Source)
	at mapreduce.task.reduce.EventFetcher.getMapCompletionEvents(EventFetcher.java:120)
	at mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:66)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""MSRA-SA-41/10.190.173.170""; destination host is: ""minint-75dgdam1.fareast.corp.microsoft.com"":53419; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 0 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 1 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 2 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-75dgdam1.fareast.corp.microsoft.com/10.86.165.66:53419. Already tried 34 time(s); maxRetries=45
",MachineDown,1512
710,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0004, Ident: (mapreduce.security.token.JobTokenIdentifier@7f219df4)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0004
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@4112054e
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@309134f7
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1001 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000002_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000003_0'
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000004_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000005_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000008_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445087491445_0004_m_000009_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000012_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000002_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000004_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000009_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445087491445_0004_m_000005_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1001: Got 12 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000000_1 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000003_2 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000000_1: Shuffling to disk since 227948846 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000003_2: Shuffling to disk since 216980068 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000000_1 decomp: 227948846 len: 227948850 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000003_2 decomp: 216980068 len: 216980072 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 227948850 bytes from map-output for attempt_1445087491445_0004_m_000000_1
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 3139ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 6 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 6 of 6 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000001_0,attempt_1445087491445_0004_m_000006_1,attempt_1445087491445_0004_m_000007_1,attempt_1445087491445_0004_m_000008_1,attempt_1445087491445_0004_m_000010_0,attempt_1445087491445_0004_m_000011_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000001_0: Shuffling to disk since 217000992 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000001_0 decomp: 217000992 len: 217000996 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216980072 bytes from map-output for attempt_1445087491445_0004_m_000003_2
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 3493ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 4 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 4 of 4 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000012_1,attempt_1445087491445_0004_m_000004_1000,attempt_1445087491445_0004_m_000009_1000,attempt_1445087491445_0004_m_000002_1000 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000012_1: Shuffling to disk since 216991205 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000012_1 decomp: 216991205 len: 216991209 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217000996 bytes from map-output for attempt_1445087491445_0004_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000006_1: Shuffling to disk since 217023144 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000006_1 decomp: 217023144 len: 217023148 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216991209 bytes from map-output for attempt_1445087491445_0004_m_000012_1
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000004_1000: Shuffling to disk since 216992138 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000004_1000 decomp: 216992138 len: 216992142 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217023148 bytes from map-output for attempt_1445087491445_0004_m_000006_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000007_1: Shuffling to disk since 216987422 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000007_1 decomp: 216987422 len: 216987426 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216992142 bytes from map-output for attempt_1445087491445_0004_m_000004_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000009_1000: Shuffling to disk since 216983391 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000009_1000 decomp: 216983391 len: 216983395 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216987426 bytes from map-output for attempt_1445087491445_0004_m_000007_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000008_1: Shuffling to disk since 216989049 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000008_1 decomp: 216989049 len: 216989053 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216983395 bytes from map-output for attempt_1445087491445_0004_m_000009_1000
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000002_1000: Shuffling to disk since 216986711 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0004_m_000002_1000 decomp: 216986711 len: 216986715 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216989053 bytes from map-output for attempt_1445087491445_0004_m_000008_1
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000010_0: Shuffling to disk since 216998520 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000010_0 decomp: 216998520 len: 216998524 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 216986715 bytes from map-output for attempt_1445087491445_0004_m_000002_1000
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 16138ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216998524 bytes from map-output for attempt_1445087491445_0004_m_000010_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000011_0: Shuffling to disk since 216988481 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000011_0 decomp: 216988481 len: 216988485 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988485 bytes from map-output for attempt_1445087491445_0004_m_000011_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 21130ms
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0004_r_000000_1001: Got 1 new map-outputs
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0004&reduce=0&map=attempt_1445087491445_0004_m_000005_1000 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0004_m_000005_1000: Shuffling to disk since 216996859 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0004_m_000005_1000 decomp: 216996859 len: 216996863 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216996863 bytes from map-output for attempt_1445087491445_0004_m_000005_1000
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8327ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 13 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 13 files, 2831866878 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 13 sorted segments
   mapred.Merger: Merging 4 intermediate segments out of a total of 13
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2831866760 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445087491445_0004_r_000000_1001 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0004_r_000000_1001 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0004_r_000000_1001' to hdfs://msra-sa-41:9000/out/out1/_temporary/2/task_1445087491445_0004_r_000000
   mapred.Task: Task 'attempt_1445087491445_0004_r_000000_1001' done.
",MachineDown,1074
718,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445087491445_0006, Ident: (mapreduce.security.token.JobTokenIdentifier@7253580c)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445087491445_0006
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@64f8145f
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@337d44e4
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_1000 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445087491445_0006_r_000000_1000: Got 10 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000000_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000001_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0006_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445087491445_0006_m_000001_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5786ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000009_0 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000009_0: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445087491445_0006_m_000009_0 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445087491445_0006_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 7149ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 7 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 7 of 7 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445087491445_0006&reduce=0&map=attempt_1445087491445_0006_m_000002_0,attempt_1445087491445_0006_m_000003_0,attempt_1445087491445_0006_m_000004_0,attempt_1445087491445_0006_m_000005_0,attempt_1445087491445_0006_m_000006_0,attempt_1445087491445_0006_m_000007_0,attempt_1445087491445_0006_m_000008_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445087491445_0006_m_000009_0
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 4801ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445087491445_0006_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445087491445_0006_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445087491445_0006_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445087491445_0006_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000006_0: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000006_0 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445087491445_0006_m_000006_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000007_0: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000007_0 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445087491445_0006_m_000007_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445087491445_0006_m_000008_0: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445087491445_0006_m_000008_0 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445087491445_0006_m_000008_0
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 56602ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445087491445_0006_r_000000_1000 is done. And is in the process of committing
   mapred.Task: Task attempt_1445087491445_0006_r_000000_1000 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445087491445_0006_r_000000_1000' to hdfs://msra-sa-41:9000/out/out2/_temporary/2/task_1445087491445_0006_r_000000
   mapred.Task: Task 'attempt_1445087491445_0006_r_000000_1000' done.
",MachineDown,720
728,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MapTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445076437777_0001, Ident: (mapreduce.security.token.JobTokenIdentifier@c5f5deb)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0001
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@2aa78480
   mapred.MapTask: Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:939524096+134217728
   mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
   mapred.MapTask: mapreduce.task.io.sort.mb: 100
   mapred.MapTask: soft limit at 83886080
   mapred.MapTask: bufstart = 0; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396; length = 6553600
   mapred.MapTask: Map output collector class = mapred.MapTask$MapOutputBuffer
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 0; bufend = 48252774; bufvoid = 104857600
   mapred.MapTask: kvstart = 26214396(104857584); kvend = 17306072(69224288); length = 8908325/6553600
   mapred.MapTask: (EQUATOR) 57321526 kvi 14330376(57321504)
  [SpillThread] mapred.MapTask: Finished spill 0
   mapred.MapTask: (RESET) equator 57321526 kv 14330376(57321504) kvi 12128560(48514240)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 57321526; bufend = 695920; bufvoid = 104857600
   mapred.MapTask: kvstart = 14330376(57321504); kvend = 5416856(21667424); length = 8913521/6553600
   mapred.MapTask: (EQUATOR) 9764656 kvi 2441160(9764640)
  [SpillThread] mapred.MapTask: Finished spill 1
   mapred.MapTask: (RESET) equator 9764656 kv 2441160(9764640) kvi 236684(946736)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 9764656; bufend = 58004947; bufvoid = 104857600
   mapred.MapTask: kvstart = 2441160(9764640); kvend = 19744112(78976448); length = 8911449/6553600
   mapred.MapTask: (EQUATOR) 67073683 kvi 16768416(67073664)
  [SpillThread] mapred.MapTask: Finished spill 2
   mapred.MapTask: (RESET) equator 67073683 kv 16768416(67073664) kvi 14561752(58247008)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 67073683; bufend = 10426966; bufvoid = 104857600
   mapred.MapTask: kvstart = 16768416(67073664); kvend = 7849620(31398480); length = 8918797/6553600
   mapred.MapTask: (EQUATOR) 19495718 kvi 4873924(19495696)
  [SpillThread] mapred.MapTask: Finished spill 3
   mapred.MapTask: (RESET) equator 19495718 kv 4873924(19495696) kvi 2677448(10709792)
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 19495718; bufend = 67755457; bufvoid = 104857600
   mapred.MapTask: kvstart = 4873924(19495696); kvend = 22181748(88726992); length = 8906577/6553600
   mapred.MapTask: (EQUATOR) 76824209 kvi 19206048(76824192)
  [SpillThread] mapred.MapTask: Finished spill 4
   mapred.MapTask: (RESET) equator 76824209 kv 19206048(76824192) kvi 16996444(67985776)
  [communication thread] mapred.Task: Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: ""04DN8IQ/10.86.164.9""; destination host is: ""minint-fnanli5.fareast.corp.microsoft.com"":52839; 
	at net.NetUtils.wrapException(NetUtils.java:772)
	at ipc.Client.call(Client.java:1472)
	at ipc.Client.call(Client.java:1399)
	at ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:244)
	at com.sun.proxy.$Proxy9.statusUpdate(Unknown Source)
	at mapred.Task$TaskReporter.run(Task.java:737)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: An existing connection was forcibly closed by the remote host
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at net.SocketInputStream.read(SocketInputStream.java:161)
	at net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at ipc.Client$Connection.run(Client.java:966)

   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 76824209; bufend = 20191510; bufvoid = 104857600
   mapred.MapTask: kvstart = 19206048(76824192); kvend = 10290756(41163024); length = 8915293/6553600
   mapred.MapTask: (EQUATOR) 29260262 kvi 7315060(29260240)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 5
   mapred.MapTask: (RESET) equator 29260262 kv 7315060(29260240) kvi 5114312(20457248)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 1 time(s); maxRetries=45
   mapred.MapTask: Spilling map output
   mapred.MapTask: bufstart = 29260262; bufend = 77519736; bufvoid = 104857600
   mapred.MapTask: kvstart = 7315060(29260240); kvend = 24622816(98491264); length = 8906645/6553600
   mapred.MapTask: (EQUATOR) 86588488 kvi 21647116(86588464)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 2 time(s); maxRetries=45
  [SpillThread] mapred.MapTask: Finished spill 6
   mapred.MapTask: (RESET) equator 86588488 kv 21647116(86588464) kvi 19453336(77813344)
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 3 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 4 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 5 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 6 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 7 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 8 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 9 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 10 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 11 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 12 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 13 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 14 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 15 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 16 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 17 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 18 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 19 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 20 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 21 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 22 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 23 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 24 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 25 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 26 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 27 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 28 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 29 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 30 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 31 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 32 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 33 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 34 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 35 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 36 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 37 time(s); maxRetries=45
  [communication thread] ipc.Client: Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 38 time(s); maxRetries=45
",MachineDown,997
741,"   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
   mapred.YarnChild: Executing with tokens:
   mapred.YarnChild: Kind: mapreduce.job, Service: job_1445094324383_0005, Ident: (mapreduce.security.token.JobTokenIdentifier@1ebe6739)
   mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
   mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445094324383_0005
   conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
   yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
   mapred.Task:  Using ResourceCalculatorProcessTree : yarn.util.WindowsBasedProcessTree@56f8d74c
   mapred.ReduceTask: Using ShuffleConsumerPlugin: mapreduce.task.reduce.Shuffle@5b895852
   mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_2 Thread started: EventFetcher for fetching Map Completion Events
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 1 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_1'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000008_1'
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445094324383_0005_m_000007_0'
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445094324383_0005_m_000008_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of OBSOLETE map-task: 'attempt_1445094324383_0005_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000007_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000008_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.ShuffleSchedulerImpl: Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_0'
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: attempt_1445094324383_0005_r_000000_2: Got 10 new map-outputs
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000000_0 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000009_1 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000009_1: Shuffling to disk since 172334804 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0005_m_000009_1 decomp: 172334804 len: 172334808 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216988127 bytes from map-output for attempt_1445094324383_0005_m_000000_0
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 1358ms
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 7 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 7 of 7 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#5
  [fetcher#5] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000005_0,attempt_1445094324383_0005_m_000003_0,attempt_1445094324383_0005_m_000001_0,attempt_1445094324383_0005_m_000004_0,attempt_1445094324383_0005_m_000002_0,attempt_1445094324383_0005_m_000007_2,attempt_1445094324383_0005_m_000006_3 sent hash and received reply
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000005_0: Shuffling to disk since 216990140 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000005_0 decomp: 216990140 len: 216990144 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 172334808 bytes from map-output for attempt_1445094324383_0005_m_000009_1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1764ms
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning MSRA-SA-39.fareast.corp.microsoft.com:13562 with 1 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to MSRA-SA-39.fareast.corp.microsoft.com:13562 to fetcher#1
  [fetcher#1] mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000008_3 sent hash and received reply
  [fetcher#1] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000008_3: Shuffling to disk since 217015228 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#1] mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1445094324383_0005_m_000008_3 decomp: 217015228 len: 217015232 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216990144 bytes from map-output for attempt_1445094324383_0005_m_000005_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000003_0: Shuffling to disk since 216972750 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000003_0 decomp: 216972750 len: 216972754 to DISK
  [fetcher#1] mapreduce.task.reduce.OnDiskMapOutput: Read 217015232 bytes from map-output for attempt_1445094324383_0005_m_000008_3
  [fetcher#1] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 1851ms
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216972754 bytes from map-output for attempt_1445094324383_0005_m_000003_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000001_0: Shuffling to disk since 217009502 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000001_0 decomp: 217009502 len: 217009506 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217009506 bytes from map-output for attempt_1445094324383_0005_m_000001_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000004_0: Shuffling to disk since 216999709 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000004_0 decomp: 216999709 len: 216999713 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216999713 bytes from map-output for attempt_1445094324383_0005_m_000004_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000002_0: Shuffling to disk since 216991624 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000002_0 decomp: 216991624 len: 216991628 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216991628 bytes from map-output for attempt_1445094324383_0005_m_000002_0
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000007_2: Shuffling to disk since 216976206 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000007_2 decomp: 216976206 len: 216976210 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 216976210 bytes from map-output for attempt_1445094324383_0005_m_000007_2
  [fetcher#5] mapreduce.task.reduce.MergeManagerImpl: attempt_1445094324383_0005_m_000006_3: Shuffling to disk since 217011663 is greater than maxSingleShuffleLimit (35232152)
  [fetcher#5] mapreduce.task.reduce.Fetcher: fetcher#5 about to shuffle output of map attempt_1445094324383_0005_m_000006_3 decomp: 217011663 len: 217011667 to DISK
  [fetcher#5] mapreduce.task.reduce.OnDiskMapOutput: Read 217011667 bytes from map-output for attempt_1445094324383_0005_m_000006_3
  [EventFetcher for fetching Map Completion Events] mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning
  [fetcher#5] mapreduce.task.reduce.ShuffleSchedulerImpl: MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 7876ms
   mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs
   mapreduce.task.reduce.MergeManagerImpl: Merging 10 files, 2125289789 bytes from disk
   mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
   mapred.Merger: Merging 10 sorted segments
   mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 2125289689 bytes
   conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   mapred.Task: Task:attempt_1445094324383_0005_r_000000_2 is done. And is in the process of committing
   mapred.Task: Task attempt_1445094324383_0005_r_000000_2 is allowed to commit now
   mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_1445094324383_0005_r_000000_2' to hdfs://msra-sa-41:9000/out/out5/_temporary/1/task_1445094324383_0005_r_000000
   mapred.Task: Task 'attempt_1445094324383_0005_r_000000_2' done.
",MachineDown,832
757,"   mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445087491445_0006_000002
   mapreduce.v2.app.MRAppMaster: Executing with tokens:
   mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 6 cluster_timestamp: 1445087491445 } attemptId: 2 } keyId: -1547346236)
   mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.
   mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null
   mapreduce.v2.app.MRAppMaster: OutputCommitter is mapreduce.lib.output.FileOutputCommitter
   yarn.event.AsyncDispatcher: Registering class mapreduce.jobhistory.EventType for class mapreduce.jobhistory.JobHistoryEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobEventType for class mapreduce.v2.app.MRAppMaster$JobEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskEventType for class mapreduce.v2.app.MRAppMaster$TaskEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.TaskAttemptEventType for class mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.commit.CommitterEventType for class mapreduce.v2.app.commit.CommitterEventHandler
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.speculate.Speculator$EventType for class mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.rm.ContainerAllocator$EventType for class mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.launcher.ContainerLauncher$EventType for class mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled
   mapreduce.v2.app.MRAppMaster: Recovery is enabled. Will try to recover from previous life on best effort basis.
   mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
   mapreduce.v2.app.MRAppMaster: Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job_1445087491445_0006_1.jhist
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000009
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000000
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000004
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000003
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000002
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000001
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000008
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000007
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000006
   mapreduce.v2.app.MRAppMaster: Read from history task task_1445087491445_0006_m_000005
   mapreduce.v2.app.MRAppMaster: Read completed tasks from history 10
   yarn.event.AsyncDispatcher: Registering class mapreduce.v2.app.job.event.JobFinishEvent$Type for class mapreduce.v2.app.MRAppMaster$JobFinishEventHandler
   metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
   metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
   metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started
   mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1445087491445_0006 to jobTokenSecretManager
   mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1445087491445_0006 because: not enabled; too many maps; too much input;
   mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1445087491445_0006 = 1313861632. Number of splits = 10
   mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1445087491445_0006 = 1
   mapreduce.v2.app.job.impl.JobImpl: job_1445087491445_0006Job Transitioned from NEW to INITED
   mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1445087491445_0006.
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 24904] ipc.Server: Starting Socket Reader #1 for port 24904
   yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol mapreduce.v2.api.MRClientProtocolPB to the server
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 24904] ipc.Server: IPC Server listener on 24904: starting
   mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at MSRA-SA-39.fareast.corp.microsoft.com/172.22.149.145:24904
   org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
   http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined
   http.HttpServer2: Added global filter 'safety' (class=http.HttpServer2$QuotingInputFilter)
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
   http.HttpServer2: Added filter AM_PROXY_FILTER (class=yarn.server.webproxy.amfilter.AmIpFilter) to context static
   http.HttpServer2: adding path spec: /mapreduce/*
   http.HttpServer2: adding path spec: /ws/*
   http.HttpServer2: Jetty bound to port 24911
   org.mortbay.log: jetty-6.1.26
   org.mortbay.log: Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\2\Jetty_0_0_0_0_24911_mapreduce____.viriwu\webapp
   org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:24911
   yarn.webapp.WebApps: Web app /mapreduce started at 24911
   yarn.webapp.WebApps: Registered webapp guice modules
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1445087491445_0006
   ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
  [Socket Reader #1 for port 24914] ipc.Server: Starting Socket Reader #1 for port 24914
  [IPC Server Responder] ipc.Server: IPC Server Responder: starting
  [IPC Server listener on 24914] ipc.Server: IPC Server listener on 24914: starting
   mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true
   mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3
   mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33
   yarn.client.RMProxy: Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030
   mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:8192, vCores:32>
   mapreduce.v2.app.rm.RMContainerAllocator: queue: default
   mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
   yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445087491445_0006Job Transitioned from INITED to SETUP
  [CommitterEvent Processor #0] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445087491445_0006Job Transitioned from SETUP to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000000 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000000_0] using containerId: [container_1445087491445_0006_01_000003 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000000_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1445087491445_0006, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job_1445087491445_0006_2.jhist
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000000_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000000 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000001 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000001_0] using containerId: [container_1445087491445_0006_01_000002 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000001_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000001_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000001 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000002 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000002_0] using containerId: [container_1445087491445_0006_01_000004 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000002_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000002_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000002 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000003 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000003_0] using containerId: [container_1445087491445_0006_01_000005 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000003_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000003_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000003 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000004 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000004_0] using containerId: [container_1445087491445_0006_01_000006 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000004_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000004_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000004 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000005 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000005_0] using containerId: [container_1445087491445_0006_01_000007 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000005_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000005_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000005 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000006 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000006_0] using containerId: [container_1445087491445_0006_01_000008 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000006_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000006_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000006 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000007 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000007_0] using containerId: [container_1445087491445_0006_01_000009 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000007_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000007_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000007 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000008 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000008_0] using containerId: [container_1445087491445_0006_01_000010 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000008_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000008_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000008 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Recovering task task_1445087491445_0006_m_000009 from prior app attempt, status was SUCCEEDED
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_m_000009_0] using containerId: [container_1445087491445_0006_01_000011 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_m_000009_0 TaskAttempt Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_m_000009_0
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_m_000009 Task Transitioned from NEW to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_r_000000 Task Transitioned from NEW to SCHEDULED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 2
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 3
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 4
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 5
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 6
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 7
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 8
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 9
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 10
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED
  [Thread-51] mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:1024, vCores:1>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:18432, vCores:-2>
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold reached. Scheduling reduces.
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445087491445_0006: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:18432, vCores:-2> knownNMs=3
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445087491445_0006_02_000002 to attempt_1445087491445_0006_r_000000_1000
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0
  [AsyncDispatcher event handler] yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job.jar
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job.xml
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445087491445_0006_02_000002 taskAttempt attempt_1445087491445_0006_r_000000_1000
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445087491445_0006_r_000000_1000
  [ContainerLauncher #0] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:30535
  [ContainerLauncher #0] mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445087491445_0006_r_000000_1000 : 13562
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445087491445_0006_r_000000_1000] using containerId: [container_1445087491445_0006_02_000002 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from ASSIGNED to RUNNING
  [AsyncDispatcher event handler] mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445087491445_0006_r_000000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_r_000000 Task Transitioned from SCHEDULED to RUNNING
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445087491445_0006: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:17408, vCores:-3> knownNMs=3
  [Socket Reader #1 for port 24914] SecurityLogger.ipc.Server: Auth successful for job_1445087491445_0006 (auth:SIMPLE)
  [IPC Server handler 3 on 24914] mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445087491445_0006_r_000002 asked for a task
  [IPC Server handler 3 on 24914] mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445087491445_0006_r_000002 given task: attempt_1445087491445_0006_r_000000_1000
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 0 maxEvents 10000
  [IPC Server handler 16 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 16 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.033333335
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.06666667
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.10000001
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.13333334
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.13333334
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.13333334
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 9 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.16666667
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 7 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.16666667
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.16666667
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.20000002
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 12 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.20000002
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.23333333
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.23333333
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.23333333
  [IPC Server handler 22 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.26666668
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.26666668
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 6 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.26666668
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.3
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.3
  [IPC Server handler 19 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 24914] mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1445087491445_0006_r_000000_1000. startIndex 10 maxEvents 10000
  [IPC Server handler 19 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.3
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.3
  [IPC Server handler 1 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.3
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.66823685
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.67233235
  [IPC Server handler 1 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.67679924
  [IPC Server handler 1 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.6813049
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.6852852
  [IPC Server handler 17 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.6872937
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.68980455
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.69383335
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.69835025
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.70129484
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.70394546
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7078279
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.71232575
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7152325
  [IPC Server handler 26 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.71842885
  [IPC Server handler 26 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7226823
  [IPC Server handler 26 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.72719526
  [IPC Server handler 26 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.73171526
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7361948
  [IPC Server handler 3 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.74045134
  [IPC Server handler 10 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7434802
  [IPC Server handler 7 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7467067
  [IPC Server handler 13 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7511169
  [IPC Server handler 8 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7556449
  [IPC Server handler 6 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7601615
  [IPC Server handler 0 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.76461244
  [IPC Server handler 15 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7670945
  [IPC Server handler 16 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.769084
  [IPC Server handler 22 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.77191097
  [IPC Server handler 2 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.77614814
  [IPC Server handler 12 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7806607
  [IPC Server handler 18 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.78517413
  [IPC Server handler 18 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7896831
  [IPC Server handler 18 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7942026
  [IPC Server handler 2 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.7987197
  [IPC Server handler 1 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.80262893
  [IPC Server handler 19 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8059623
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.80899835
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8132914
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.81784284
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.82238376
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.82664263
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8308073
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8333676
  [IPC Server handler 24 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.83689684
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.84137964
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.84593403
  [IPC Server handler 9 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8504034
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.85239947
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.85481805
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8583641
  [IPC Server handler 17 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8628692
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.86744463
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8719784
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8764548
  [IPC Server handler 20 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8807161
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.8841282
  [IPC Server handler 25 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.88749576
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.89198923
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.89653313
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.90106803
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.90541744
  [IPC Server handler 9 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9078121
  [IPC Server handler 5 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9120325
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.91656995
  [IPC Server handler 9 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.921121
  [IPC Server handler 21 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9255755
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.92850983
  [IPC Server handler 14 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9317012
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9349985
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.93702596
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9396632
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9436671
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.94823813
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.95122594
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.95456314
  [IPC Server handler 23 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9590293
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9635743
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9681356
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9727003
  [IPC Server handler 9 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9772455
  [IPC Server handler 4 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9817877
  [IPC Server handler 28 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9863492
  [IPC Server handler 29 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9908993
  [IPC Server handler 13 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 0.9954866
  [IPC Server handler 27 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 1.0
  [IPC Server handler 11 on 24914] mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1445087491445_0006_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: attempt_1445087491445_0006_r_000000_1000 given a go for committing the task output.
  [IPC Server handler 10 on 24914] mapred.TaskAttemptListenerImpl: Commit go/no-go request from attempt_1445087491445_0006_r_000000_1000
  [IPC Server handler 10 on 24914] mapreduce.v2.app.job.impl.TaskImpl: Result of canCommit for attempt_1445087491445_0006_r_000000_1000:true
  [IPC Server handler 8 on 24914] mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445087491445_0006_r_000000_1000 is : 1.0
  [IPC Server handler 17 on 24914] mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445087491445_0006_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445087491445_0006_02_000002 taskAttempt attempt_1445087491445_0006_r_000000_1000
  [ContainerLauncher #1] mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445087491445_0006_r_000000_1000
  [ContainerLauncher #1] yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:30535
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445087491445_0006_r_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_r_000000_1000
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.TaskImpl: task_1445087491445_0006_r_000000 Task Transitioned from RUNNING to SUCCEEDED
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 11
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445087491445_0006Job Transitioned from RUNNING to COMMITTING
  [CommitterEvent Processor #1] mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_COMMIT
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: Calling handler for JobFinishedEvent 
  [AsyncDispatcher event handler] mapreduce.v2.app.job.impl.JobImpl: job_1445087491445_0006Job Transitioned from COMMITTING to SUCCEEDED
  [Thread-78] mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry
  [Thread-78] mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true
  [Thread-78] mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true
  [Thread-78] mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true
  [Thread-78] mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true
  [Thread-78] mapreduce.v2.app.MRAppMaster: Calling stop for all the services
  [Thread-78] mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0
  [RMCommunicator Allocator] mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job_1445087491445_0006_2.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006-1445091992888-msrabi-word+count-1445093780514-10-1-SUCCEEDED-default-1445092006078.jhist_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006-1445091992888-msrabi-word+count-1445093780514-10-1-SUCCEEDED-default-1445092006078.jhist_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006/job_1445087491445_0006_2_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006_conf.xml_tmp
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006.summary_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006.summary
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006_conf.xml_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006_conf.xml
  [eventHandlingThread] mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006-1445091992888-msrabi-word+count-1445093780514-10-1-SUCCEEDED-default-1445092006078.jhist_tmp to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging
							/history/done_intermediate/msrabi/job_1445087491445_0006-1445091992888-msrabi-word+count-1445093780514-10-1-SUCCEEDED-default-1445092006078.jhist
  [Thread-78] mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop()
  [Thread-78] mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to 
  [Thread-78] mapreduce.v2.app.rm.RMContainerAllocator: History url is http://MSRA-SA-39.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445087491445_0006
  [Thread-78] mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.
  [Thread-78] mapreduce.v2.app.rm.RMContainerAllocator: Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0
  [Thread-78] mapreduce.v2.app.MRAppMaster: Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0006
  [Thread-78] ipc.Server: Stopping server on 24914
  [IPC Server listener on 24914] ipc.Server: Stopping IPC Server listener on 24914
  [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
  [TaskHeartbeatHandler PingChecker] mapreduce.v2.app.TaskHeartbeatHandler: TaskHeartbeatHandler thread interrupted
",MachineDown,4678
